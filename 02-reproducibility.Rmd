# Reproducible research {#reproducibility}

Before we continue, it's worth highlighting the similarity between the iterative decision making cycle I've outlined in figure \@ref(fig:decisions3) and _the scientific method_, i.e.:

- _Observation > Hypothesis > Experiment > Analyze > Interpret > Report > (Repeat)_

```{r scimethod, echo=FALSE, fig.cap = "_The Scientific Method_ overlain on iterative decision making.", fig.width=3, fig.align = 'center'}
knitr::include_graphics("img/scimethod.png")
```

<br>

It's also worth noting that _replication is one of the fundamental tenets of science_. In other words, published research should be robust enough and the methods described in enough detail that anyone else should be able to repeat the study (using the publication only) and find similar results. Sadly, this is rarely the case!!!

<br>

## Replication and the Reproducibility Spectrum

> _"**Replication** is the ultimate standard by which scientific claims are judged."_ [@Peng2011]

If the results of a study or experiment cannot by replicated by an independent set of investigators then whatever scientific claims were made should be treated with caution! At best, it suggests that evidence for the claim is weak or mixed, or specific to particular ecosystems or other circumstances and cannot be generalized. At worst, there was error (or even dishonesty) in the original study and the claims were plainly false.

Unfortunately, some studies may not be entirely replicable purely due to the nature of the data or phenomenon (e.g. rare phenomena, long term records, loss of species or ecosystems, or very expensive once-off science projects like space missions). In these cases the "gold standard" of full replication (from new data collection to results) cannot be achieved, and we have to settle for a lower rung on the reproducibility spectrum (Figure \@ref(fig:peng)).

<br>

```{r peng, echo=FALSE, fig.cap = "The Reproducibility Spectrum [@Peng2011].", fig.width=3, fig.align = 'center'}
knitr::include_graphics("img/peng_reproducibility.jpg")
```

<br>

**Reproducibility** falls short of full **replication** because it focuses on reproducing the same result _from the same data set_, rather than analyzing independently collected data. While this may seem trivial, you'd be surprised at how few studies are even reproducible, let alone replicable.

<br>

```{r reprocrisis, echo=FALSE, fig.cap = "'Is there a reproducibility* crisis?' Results from a survey of >1500 top scientists [@Baker2016; @Penny2016]. *Note that they did not discern between reproducibility and replicability, and that the terms are often used interchangeably.", fig.width=6, fig.align = 'center', warning = F, message = F}
# load library
library(tidyverse)

# Get Baker data.
cridata <- read_delim("/home/jasper/GIT/BIO3019S_Ecoforecasting/data/Reproducibilitysurveyrawdata20160523.txt", delim = "\t")

#20 (is there a crisis)
names(cridata)[20] <- "Crisis"

cridata <- cridata %>% group_by(Crisis) %>% summarize(count = n()) %>%na.omit()

cridata$Crisis <- c("7% \n I don't know", "52% \n Significant crisis", "Slight crisis \n 38%", "No crisis \n 3%")

cridata$Crisis <- factor(cridata$Crisis, levels = c("52% \n Significant crisis", "No crisis \n 3%", "7% \n I don't know", "Slight crisis \n 38%"))

# Compute percentages
cridata$fraction <- cridata$count / sum(cridata$count)

# Compute the cumulative percentages (top of each rectangle)
cridata$ymax <- cumsum(cridata$fraction)

# Compute the bottom of each rectangle
cridata$ymin <- c(0, head(cridata$ymax, n=-1))

# Compute label position
cridata$labelPosition <- (cridata$ymax + cridata$ymin) / 2

# # Compute a good label
# data$label <- paste0(data$category, "\n value: ", data$count)
# 
# Make the plot
ggplot(cridata, aes(ymax=ymax, ymin=ymin, xmax=4, xmin=3, fill=Crisis)) +
  geom_rect() +
  geom_text(x=4.6, aes(y=labelPosition, label=Crisis, color=Crisis)) + # x here controls label position (inner / outer)
  scale_fill_brewer(palette="Set1", direction = 1) +
  scale_color_brewer(palette="Set1", direction = 1) +
  coord_polar(theta="y") +
  xlim(c(2, 4.5)) +
  theme_void() +
  theme(legend.position = "none")
```

<br>

We clearly have a problem...

Working reproducibly is not just a requirement for using quantitative approaches in iterative decision-making, it is central to scientific progress!!!

While full replication is a huge challenge (and sometimes impossible) to achieve, it is something all scientists should be working towards.

<br>

## Why work reproducibly?

```{r miracle, echo=FALSE, fig.cap = "Let's start being more specific about our miracles... Cartoon &copy; Sidney Harris. Used with permission [ScienceCartoonsPlus.com](www.ScienceCartoonsPlus.com)", fig.width=6, fig.align = 'center', out.width="150%"}
knitr::include_graphics("img/miracle.jpg")
```

<br>

***In addition to basic scientific rigour, working reproducibly is hugely valuable, because:***

(Adapted from "Five selfish reasons to work reproducibly" [@Markowetz2015])

1. _**It helps us avoid mistakes and/or track down errors in analyses**_
- This is what highlighted the importance of working reproducibly for me. In 2017 I published the first evidence of observed climate change impacts on biodiversity in the Fynbos Biome [@Slingsby2017]. The analyses were quite complicated, and when working on the revisions I found an error in my R code. Fortunately, it didn't change the results qualitatively, but it made me realize how easy it is to make a mistake and potentially put the wrong message out there! This encouraged me to make all data and R code from the paper available, so that anyone is free to check my data and analyses and let me (and/or the world) know if they find any errors.
2. _**It makes it easier to write papers**_
- e.g. Dynamic documents like RMarkdown or Jupyter Notebooks update automatically when you change your analyses, so you don't have to copy/paste or save/insert all tables and figures (or worry about whether you included the latest versions.
3. _**It helps the review process**_
- Often issues picked at by reviewers are matters of clarity/confusion. Sharing your data and analyses allows them to see exactly what you did, not just what you said you did, allowing them to identify the problem and make constructive suggestions.
- It's also handy to be able to respond to a reviewer's comment with something like _"That's a great suggestion, but not really in line with the objectives of the study. We have chosen not to include the suggested analysis, but do provide all data and code so that interested readers can explore this for themselves."_ (Feel free to copy and paste - [CCO 1.0](https://creativecommons.org/publicdomain/zero/1.0/))
4. _**It enables continuity of the research**_
- When people leave a project (e.g. students/postdocs), or you forget what you did _X_ days/weeks/months/years ago, it can be a serious setback for a project and make it difficult for you or a new student to pick up where things left off. If the data and workflow are well curated and documented this problem is avoided. _Trust me, this is a very common problem!!!_ I have many papers that I (or my students) never published and may never go back to, because I know it'll take me a few days or weeks to understand the datasets and analyses again...
- This is obviously incredibly important for long-term projects!!!
5. _**It helps to build your reputation**_
- Working reproducibly makes it clear you're an honest, open, careful and transparent researcher, and should errors be found in your work you're unlikely to be accused of dishonesty (e.g. see my paper example under point 1 - although no one has told me of any errors yet...).
- When others reuse your data, code, etc you're likely to get credit for it - either just informally, or formally through citations or acknowledgements (depending on the licensing conditions you specify - see "Preserve" in the Data Life Cycle).

And some less selfish reasons (and relevant for ecoforecasting):

6. _**It allows you (or others) to rapidly build on previous findings and analyses**_
7. _**It allows easy comparison of new analytical approaches to older ones**_
8. _**It makes it easy to repeat the same analyses when new data are collected or added**_

And one more selfish reason (but don't tell anyone I said this): _**Should you decide to leave biology, reproducible research skills are highly sought after in other careers like data science etc...**_

<br>

## Scientific Workflows

```{r datapipeline, echo=FALSE, fig.cap = "'Data Pipeline' from [xkcd.com/2054](https://xkcd.com/2054), used under a [CC-BY-NC 2.5 license](https://creativecommons.org/licenses/by-nc/2.5/).", fig.width=3, fig.align = 'center'}
knitr::include_graphics("img/xkcd_data_pipeline_2x.png")
```

<br>

Working reproducibly requires careful planning and documentation of each step in your scientific workflow from _planning_ your data collection to _sharing_ your results.

This entails a number of overlapping/intertwined components, namely:

- Data management
- Coding and code management (data analysis)
- Computing and software

For the rest of this section we'll work through these components and some of the tools that help you achieve this.

<br>

## Data Management

### Why do you need to manage your data?

Data management is often the last thing on a scientists mind when doing a new study - "I have a cool idea, and I'm going to test it!". You don't want to "waste" time planning how you're going to manage your data and implementing that plan... Unfortunately, this never ends well and really is a realm where "haste makes waste".

<br>

```{r datadecay, echo=FALSE, fig.cap = "The 'Data Decay Curve' [@Michener1997]", fig.width=3, fig.align = 'center', out.width="75%"}
knitr::include_graphics("img/datadecaycurve.jpg")
```

<br>

- Bad data management leads to data loss... (The "data decay curve" [@Michener1997])
- Your future self will hate you if you lose it before you're finished with it - Eek!!! This is less likely in the world of Dropbox, Google Drive, iCloud etc, but I know people who had to repeat their PhD's because they lost their data or it was on a laptop that was stolen...
- Data has value beyond your current project: 
  - to yourself for reuse in future projects, collaborations, etc (i.e. publications and citations), 
  - for others for follow-up studies, or combining multiple datasets for meta-analyses or synthesis etc
  - for science on general (especially long-term ecology in a time of global change)
- We've covered this before, but it is also key for transparency and accountability.
- Data collection is expensive, and is often paid for with taxpayers' money. You owe it to them to make sure that science gets the most out of that data in the long term.
- Lastly, good planning and data management can help iron out issues like intellectual property, permissions for ethics, collection permits, etc, in addition to outlining expectations for who will be authors on the paper(s), responsible for managing different aspects of the data etc up front. If you don't establish these permissions and ground rules early they can result in data loss, not being able to publish the study, damage careers, and/or damage relationships in collaborations (including student-supervisor), etc. 

To avoid data (and relationship) decay, and to reap the benefits of good data management, it is important to consider the full *Data Life Cycle*.

<br>

### The Data Life Cycle

```{r datalifecycle, echo=FALSE, fig.cap = "The Data Life Cycle, adapted from https://www.dataone.org/", fig.width=6, fig.align = 'center'}
# load library
#library(ggplot2)

# Create test data.
data <- data.frame(
  category=c("1.Plan", "2.Collect", "3.Assure", "4.Describe", "5.Preserve", "6.Discover", "7.Integrate", "8.Analyze"),
  count=rep(12.5, 8)
)
 
# Compute percentages
data$fraction <- data$count / sum(data$count)

# Compute the cumulative percentages (top of each rectangle)
data$ymax <- cumsum(data$fraction)

# Compute the bottom of each rectangle
data$ymin <- c(0, head(data$ymax, n=-1))

# Compute label position
data$labelPosition <- (data$ymax + data$ymin) / 2

# Compute a good label
data$label <- paste0(data$category, "\n value: ", data$count)

# Make the plot
ggplot(data, aes(ymax=ymax, ymin=ymin, xmax=4, xmin=3, fill=category)) +
  geom_rect() +
  geom_text(x=3.5, aes(y=labelPosition, label=category, color=category)) + # x here controls label position (inner / outer)
  scale_fill_brewer(palette="Blues") +
  scale_color_brewer(palette="Blues", direction = -1) +
  coord_polar(theta="y") +
  xlim(c(2, 4.5)) +
  theme_void() +
  theme(legend.position = "none")
```

<br>

Note that there are quite a few different versions of the data life cycle out there. This is the most comprehensive one I know of, and covers all the steps relevant to a range of different kinds of research projects. A full description of this data life cycle and related ecoinformatics issues can be found in [@Michener2012]. 

Not all projects need to do all steps, nor will they necessarily follow the order here, but it is worth being aware of and considering all steps. For example, often the first thing you do when you have a new hypothesis is search around for any existing data that could be used to test it without having to spend money and time collecting new data (i.e. skip to step 6). In this case I would argue that you should still do step 1 (Plan), and you'd want to do some checking to assure the quality of the data (step 3), but you can certainly skip steps 2, 4 and 5. A meta-analysis or synthesis paper would probably do the same. On the other hand, if you're collecting new data you would do steps 1 to 5 and possibly skip 6 and 7, although in my experience few studies do not reuse existing data (e.g. weather or various GIS data to put your new samples in context).

<br>

#### Plan 
Good data management begins with planning. In this step you essentially outline the plan for every step of the cycle in as much detail as possible. Usually this is done by constructing a document or Data Management Plan (DMP). 

While developing DMPs can seem tedious, they are essential for the reasons I gave above, and because most funders and universities now require them.

Fortunately, there are a number of online data management planning tools that make it easy by providing templates and prompts to ensure that you cover all the bases, like the Digital Curation Centre's [DMPOnline](https://dmponline.dcc.ac.uk/) and [UCT's DMP Tool](https://dmp.lib.uct.ac.za/).

<br>

```{r uctdmp, echo=FALSE, fig.cap = "Screenshot of [UCT's Data Management Planning Tool's Data Management Checklist.](http://www.digitalservices.lib.uct.ac.za/dls/systems/uct-dmp)", fig.width=3, fig.align = 'center', out.width="90%"}
knitr::include_graphics("img/UCTDMP.png")
```

<br>

A key thing to bear in mind is that a DMP is a living document and should be regularly revised during the life of a project, especially when big changes happen - e.g. new team members, new funding, new direction, change of institution, etc.

I typically develop one overarching DMP for an umbrella project (e.g. a particular grant), but then add specifics for subprojects (e.g. separate student projects etc).

<br>

#### Collect and Assure

There are many, many different kinds of data that can be collected in a vast number of ways!

```{r datatypes, echo=FALSE, fig.cap = "[Springer Nature Infographic](https://doi.org/10.6084/m9.figshare.5883193.v1) illustrating the vast range of research data types.", fig.width=3, fig.align = 'center', out.width="90%"}
knitr::include_graphics("img/datatypes.png")
```

<br>

While "Collect" and "Assure" are different steps in the life cycle, I advocate that it is foolish to collect data without doing quality assurance and quality control (QA/QC) as you go, irrespective of how you are collecting the data.

For example:

- automated logging instruments (weather stations, cameras, acoustic recorders) need to be checked that they're logging properly, are calibrated/focused, are reporting sensible values, etc
- if you're filling in data sheets, you need to check that all fields have been completed, that there are no obvious errors and that any numbers or other values look realistic. In fact, if you're using handwritten data sheets it's best to capture them as soon as possible (i.e. that evening), because that helps you spot errors and omissions, you have a better chance of deciphering bad handwriting or cryptic notes, and you can plot any values to see if there are suspicious outliers (e.g. because someone wrote down a measurement in centimetres when they were meant to use metres).

When transcribing or capturing data into a spreadsheet or database it is often best to **use data validation** tricks like drop-down menus, conditional formatting, restricted value ranges etc to avoid spelling mistakes and highlight data entries that are outside the expected range of the data field. It may seem like a lot of effort to set this up, but *it'll save you a lot of time and pain in the long run!!!*

Increasingly, I've started moving towards capturing data directly into a spreadsheet on a tablet, or better yet, into an App on my phone. There are a number of "no code" app builders these days like [AppSheet](https://www.appsheet.com) that inserts data directly into Google Sheets and saves photos to your Google Drive.

<br>

```{r appsheet, echo=FALSE, fig.cap = "An example data collection app I built in [AppSheet](https://www.appsheet.com) that allows you to log GPS coordinates, take photos, record various fields, etc.", fig.width=3, fig.align = 'center', out.width="90%"}
knitr::include_graphics("img/veldwatch.png")
```

<br>

#### Describe

There are few things worse than having a spreadsheet of what you think is the data you need, but you don't know what the column names mean, how variables were measured, what units they're reported in, etc... - *Especially when you were the one who collected and captured the data!!!* 

This descriptive data about the data is called ***metadata*** and is essential for making the data reusable, but is also useful for many other purposes like making the data findable (e.g. using keyword searches). In fact, metadata makes up the majority of what are called the [**FAIR data principles**](https://www.go-fair.org/fair-principles/) [@Wilkinson2016], which largely focus on this and the next few steps of the Data Life Cycle. I'm not going to dwell on them other than to say that they are a key component of making your work reproducible, and that like reproducibility, practicing FAIR data principles is a spectrum.

<br>

```{r fairdata, echo=FALSE, fig.cap = "The FAIR data principles [ErrantScience.com](https://errantscience.com/).", fig.width=3, fig.align = 'center', out.width="75%"}
knitr::include_graphics("img/fairdata.jpg")
```

<br>

**Some key kinds of metadata:**

- the study context
  - why the data were generated
  - who funded, created, collected, assured, managed and owns the data (not always the same person)
    - contact details for the above
  - when and where the data were collected
  - where the data are stored
- the data format
  - what is the file format
  - what softwares were used (and what version)
- the data content
  - what was measured
  - how it was measured
  - what units was it reported in
  - what QA/QC has been applied
  - is it raw data or a derived data product (e.g. spatially interpolated climate layers)
  - if derived, how it was analyzed

<br>

**Metadata standards and interoperability**

Many data user communities have developed particular metadata standards or schemas in an attempt to enable the best possible description and interoperability of a data type for their needs.

They are typically human and machine-readable data, so that the metadata records can also be read by machines, facilitating storing and querying multiple datasets in a common database (or across databases).

Using common metadata schemas has many advantages in that they make data sharing easier, they allow you to search and integrate data across datasets, and they simplify metadata capture (i.e. having a list of required fields makes it easier to not forget any).

There are [many standards](https://www.dcc.ac.uk/guidance/standards/metadata/list), but perhaps the most common ones you'll encounter in biological sciences (other than geospatial metadata standards) are [DarwinCore](https://www.tdwg.org/standards/dwc/) and [Ecological Metadata Language (EML)](https://knb.ecoinformatics.org/tools/eml).

<br>

#### Preserve

There are two major components to preserving your data:

1. ***Back your data up now!!! (and repeat regularly)***

Losing your data can be incredibly inconvenient!!! A good friend of mine lost all of his PhD data twice. It took him 7 years to complete the degree...

Beyond inconvenience, losing data can be incredibly expensive! Doing 4 extra years to get your PhD is expensive at a personal level, but if the data are part of a big project it can rapidly add up to millions - like [How Toy Story 2 Almost Got Deleted](https://www.youtube.com/watch?v=8dhp_20j0Ys).

>***PRO TIP:*** Storing data on the cloud is not enough! You could easily delete that single version of all you data! You may also lose access whe you change institution etc. E.g. What happens to your UCT MS OneDrive and Google Drive content when you graduate and ICTS close your email account?

<br>

2. ***Long-term preservation and publication***

This involves the deposition of your data (and metadata!) in a **data repository** where it can be managed and curated over the long term. 

This is increasingly a requirement of funders and publishers (i.e. journals). Many journals allow you (or require you) to submit and publish your data with them as supplementary material. Unfortunately, many journals differ in how they curate the data and whether they are available open access. I prefer to publish my data in an online open access repository where you can get a permanent **Digital Object Identifier (DOI)** that you can link to from your paper.

Another consideration, if you are keen for people to reuse your data (which if you are not you will fail this course by default) is where people are most likely to look for your data (i.e. making your data "Findable/Discoverable". There are many "bespoke" data repositories for different kinds of data, e.g.

Global databases:

- [GenBank](https://www.ncbi.nlm.nih.gov/genbank/) - for molecular data
- [TRY](https://www.try-db.org/TryWeb/Home.php) - for plant traits
- [Dryad](https://datadryad.org) - for generalist biological and environmental research

South African databases:

- [SANBI](https://www.sanbi.org/resources/infobases/) - for most kinds of South African biodiversity data
- [SAEON](https://catalogue.saeon.ac.za/) - for South African environmental data (e.g. hydrology, meteorology, etc) and biodiversity data that don't fit SANBI's databases

If none of these suit your data, there are also "generalist" data repositories that accept almost any kind of data, like:

- [FigShare](https://figshare.com/)
- [Zenodo](https://zenodo.org/)
- UCT's [ZivaHub](https://zivahub.uct.ac.za/) (which is built on and searchable through FigShare)

<br>

I haven't discussed **physical samples** at all. These are obviously a huge (if not bigger) challenge too, although there are some obvious homes for common biological data, like herbaria for plant collections and museums for animal specimens.

<br>

#### Discover

This is perhaps the main point of the Data Life Cycle and FAIR data principles - to make data findable so that it can be reused. 

The biggest challenge to discovering data is that **so many datasets are not online** and are in the "filing cabinet in a bath in the basement under a leaking pipe" as in Figure \@ref(fig:fairdata). If you ***preserve and publish*** them in an online data repository, this overcomes the biggest hurdle.

The next biggest challenge is **that there is so much online that finding what you need can be quite challenging** (like looking for a needle in a haystack...). This is where choosing the right portal can be important. It is also what metadata standards are aimed at - allowing interoperable searches for specific data types across multiple repositories.

A final consideration is whether you have **permission to use the data**. You can often find out about the existence of a dataset, either online or in a paper, but the data aren't made freely available. This is where **licensing** comes into play. Most data repositories require you to publish the data under a license. There are many options depending on the kind of data and what restrictions you want to put on its use. I'm not going to go into the gory details, but [Creative Commons](https://creativecommons.org/licenses/) have created an extensible system of generic licenses that are easy to interpret and cover most situations. I say *extensible* because the licenses are made up of a string of components that can be layered over each other. For example:

- CCO - means it is *Open* - i.e. there are no restrictions on use and it is in the public domain
- CC BY - means *by attribution* - you can use the data for any purpose, but only if you indicate attribution of the data to the source or owner of the data
- CC BY-SA - means *by attribution* + *share alike* - i.e. you have to indicate attribution and share your derived product under the same license
- CC BY-ND - means *by attribution* + *no derivatives* - i.e. you have to indicate attribution, but cannot use it to make a derived product. This is often used for images - allowing you to show the image, but not to alter it.
- CC BY-NC - means *by attribution* + *non-commercial* - you have to indicate attribution, but cannot use it for commercial purposes (i.e. you can't sell derived products)
- CC BY-NC-SA - *by attribution* + *non-commercial* + *share alike*
- CC BY-NC-ND - *by attribution* + *non-commercial* + *no derivatives*

<br>

#### Integrate

#### Analyze

<br>

## Coding and code management

<br>

- Why write code
- Some rules of coding
- Version control

<br>

## Computing and software

<br>

- Why use open source software
- Containers

<br>

# Reproducible research {#reproducibility}

## The Reproducibility Crisis

> _"**Replication** is the ultimate standard by which scientific claims are judged."_ [@Peng2011]

_Replication is one of the fundamental tenets of science_ and if the results of a study or experiment cannot by replicated by an independent set of investigators then whatever scientific claims were made should be treated with caution! At best, it suggests that evidence for the claim is weak or mixed, or specific to particular ecosystems or other circumstances and cannot be generalized. At worst, there was error (or even dishonesty) in the original study and the claims were plainly false.

In other words, published research should be robust enough and the methods described in enough detail that anyone else should be able to repeat the study (using the publication only) and find similar results. Sadly, this is rarely the case!!!

```{r reprocrisis, echo=FALSE, fig.cap = "'Is there a reproducibility* crisis?' Results from a survey of >1500 top scientists [@Baker2016; @Penny2016]. *Note that they did not discern between reproducibility and replicability, and that the terms are often used interchangeably.", fig.width=6, fig.align = 'center', warning = F, message = F, out.width="125%"}
# load library
library(tidyverse)

# Get Baker data.
cridata <- read_delim("/home/jasper/GIT/BIO3019S_Ecoforecasting/data/Reproducibilitysurveyrawdata20160523.txt", delim = "\t")

#20 (is there a crisis)
names(cridata)[20] <- "Crisis"

cridata <- cridata %>% group_by(Crisis) %>% summarize(count = n()) %>%na.omit()

cridata$Crisis <- c("7% \n I don't know", "52% \n Significant crisis", "Slight crisis \n 38%", "No crisis \n 3%")

cridata$Crisis <- factor(cridata$Crisis, levels = c("52% \n Significant crisis", "No crisis \n 3%", "7% \n I don't know", "Slight crisis \n 38%"))

# Compute percentages
cridata$fraction <- cridata$count / sum(cridata$count)

# Compute the cumulative percentages (top of each rectangle)
cridata$ymax <- cumsum(cridata$fraction)

# Compute the bottom of each rectangle
cridata$ymin <- c(0, head(cridata$ymax, n=-1))

# Compute label position
cridata$labelPosition <- (cridata$ymax + cridata$ymin) / 2

# # Compute a good label
# data$label <- paste0(data$category, "\n value: ", data$count)
# 
# Make the plot
ggplot(cridata, aes(ymax=ymax, ymin=ymin, xmax=4, xmin=3, fill=Crisis)) +
  geom_rect() +
  geom_text(x=4.6, aes(y=labelPosition, label=Crisis, color=Crisis)) + # x here controls label position (inner / outer)
  scale_fill_brewer(palette="Set1", direction = 1) +
  scale_color_brewer(palette="Set1", direction = 1) +
  coord_polar(theta="y") +
  xlim(c(2, 4.5)) +
  theme_void() +
  theme(legend.position = "none")
```

<br>

We have a problem...

Since we're failing the [gentleman's agreement](https://en.wikipedia.org/wiki/Gentlemen%27s_agreement)^[I know many may find the use of this term offensive. In fact, I have used it here because that offense highlights my point. "Gentlemen's agreements" have been used for nefarious purposes since the dawn of time have no place in science.] that we'll describe our methods in enough detail that anyone else should be able to repeat the study (using the publication only) and find similar results, modern scientists are trying to formalize the process in the form of ***Reproducible Research***. Reproducible research makes use of modern software tools to share data, code and other resources required to allow others to reproduce the same result as the original study, thus making all analyses open and transparent.

As you will learn in this module, working reproducibly is not just a requirement for using quantitative approaches in iterative decision-making, it is central to scientific progress!!!

While full replication is a huge challenge (and sometimes impossible) to achieve, it is something all scientists should be working towards.

<br>

## Replication and the Reproducibility Spectrum

Understandably, some studies may not be entirely replicable purely due to the nature of the data or phenomenon (e.g. rare phenomena, long term records, loss of species or ecosystems, or very expensive once-off science projects like space missions). In these cases the "gold standard" of full replication (from new data collection to results) cannot be achieved, and we have to settle for a lower rung on the reproducibility spectrum (Figure \@ref(fig:peng)).

<br>

```{r peng, echo=FALSE, fig.cap = "The Reproducibility Spectrum [@Peng2011].", fig.width=3, fig.align = 'center'}
knitr::include_graphics("img/peng_reproducibility.jpg")
```

<br>

**Reproducibility** falls short of full **replication** because it focuses on reproducing the same result _from the same data set_, rather than analyzing independently collected data. While this may seem trivial, you'd be surprised at how few studies are even reproducible, let alone replicable.

<br>

## Why work reproducibly?

```{r miracle, echo=FALSE, fig.cap = "Let's start being more specific about our miracles... Cartoon &copy; Sidney Harris. Used with permission [ScienceCartoonsPlus.com](www.ScienceCartoonsPlus.com)", fig.width=6, fig.align = 'center', out.width="150%"}
knitr::include_graphics("img/miracle.jpg")
```

<br>

***In addition to basic scientific rigour, working reproducibly is hugely valuable, because:***

(Adapted from "Five selfish reasons to work reproducibly" [@Markowetz2015])

1. _**It helps us avoid mistakes and/or track down errors in analyses**_
- This is what highlighted the importance of working reproducibly for me. In 2017 I published the first evidence of observed climate change impacts on biodiversity in the Fynbos Biome [@Slingsby2017]. The analyses were quite complicated, and when working on the revisions I found an error in my R code. Fortunately, it didn't change the results qualitatively, but it made me realize how easy it is to make a mistake and potentially put the wrong message out there! This encouraged me to make all data and R code from the paper available, so that anyone is free to check my data and analyses and let me (and/or the world) know if they find any errors.
2. _**It makes it easier to write papers**_
- e.g. Dynamic documents like RMarkdown or Jupyter Notebooks update automatically when you change your analyses, so you don't have to copy/paste or save/insert all tables and figures (or worry about whether you included the latest versions.
3. _**It helps the review process**_
- Often issues picked at by reviewers are matters of clarity/confusion. Sharing your data and analyses allows them to see exactly what you did, not just what you said you did, allowing them to identify the problem and make constructive suggestions.
- It's also handy to be able to respond to a reviewer's comment with something like _"That's a great suggestion, but not really in line with the objectives of the study. We have chosen not to include the suggested analysis, but do provide all data and code so that interested readers can explore this for themselves."_ (Feel free to copy and paste - [CCO 1.0](https://creativecommons.org/publicdomain/zero/1.0/))
4. _**It enables continuity of the research**_
- When people leave a project (e.g. students/postdocs), or you forget what you did _X_ days/weeks/months/years ago, it can be a serious setback for a project and make it difficult for you or a new student to pick up where things left off. If the data and workflow are well curated and documented this problem is avoided. _Trust me, this is a very common problem!!!_ I have many papers that I (or my students) never published and may never go back to, because I know it'll take me a few days or weeks to understand the datasets and analyses again...
- This is obviously incredibly important for long-term projects!!!
5. _**It helps to build your reputation**_
- Working reproducibly makes it clear you're an honest, open, careful and transparent researcher, and should errors be found in your work you're unlikely to be accused of dishonesty (e.g. see my paper example under point 1 - although no one has told me of any errors yet...).
- When others reuse your data, code, etc you're likely to get credit for it - either just informally, or formally through citations or acknowledgements (depending on the licensing conditions you specify - see "Preserve" in the Data Life Cycle).

And some less selfish reasons (and relevant for ecoforecasting):

6. _**It allows you (or others) to rapidly build on previous findings and analyses**_
7. _**It allows easy comparison of new analytical approaches to older ones**_
8. _**It makes it easy to repeat the same analyses when new data are collected or added**_

And one more selfish reason (but don't tell anyone I said this): _**Should you decide to leave biology, reproducible research skills are highly sought after in other careers like data science etc...**_

<br>

## Scientific Workflows

```{r datapipeline, echo=FALSE, fig.cap = "'Data Pipeline' from [xkcd.com/2054](https://xkcd.com/2054), used under a [CC-BY-NC 2.5 license](https://creativecommons.org/licenses/by-nc/2.5/).", fig.width=3, fig.align = 'center'}
knitr::include_graphics("img/xkcd_data_pipeline_2x.png")
```

<br>

Working reproducibly requires careful planning and documentation of each step in your scientific workflow from _planning_ your data collection to _sharing_ your results.

This entails a number of overlapping/intertwined components, namely:

- Coding and code management (data analysis)
- Computing and software
- Data management

For the rest of this section we'll work through these components and some of the tools that help you achieve this. **Data management** is big enough to warrant a separate chapter.

<br>


## Coding and code management

<br>

- Why write code
- Some rules of coding
- Version control

<br>

## Computing and software

<br>

- Why use open source software
- Containers

<br>

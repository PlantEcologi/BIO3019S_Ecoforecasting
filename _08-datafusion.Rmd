# Data Fusion and Assimilation {#datafusion}

This lecture builds on section \@ref(bayesian), "Going Bayesian", with a focus on how a Bayesian framework allows us to:

1. ***Fuse*** multiple sources of data that vary in their spatial or temporal scale, uncertainty, gaps, etc into our models, and
2. ***Assimilate*** the previous forecast and new observations so that we can update our forecasts for the next iteration

Along the way I'll also discuss **propogating and quantifying uncertainty** in a Bayesian model using MCMC, because it's important for the narrative, but we'll go into uncertainty in more detail in section \@ref(uncertainty).

<br>

## A quick reminder of Bayes Theorem and the benefits it provides

$$
\underbrace{p(\theta|D)}_\text{posterior} \; \propto \; \underbrace{p(D|\theta)}_\text{likelihood} \;\; \underbrace{p(\theta)}_\text{prior} \;
$$
Where the ***posterior*** is a probability distribution representing the credibility of the parameter values $\theta$, taking the data, $D$, into account.

The ***likelihood***, $p(D|\theta)$ represents the probability that the data could be generated by the model with parameter values $\theta$. The likelihood of the parameters are maximized by choosing the parameters that maximize the probability of the data.

The ***prior*** probability distribution is the marginal probability of the parameters, $p(\theta)$, and represents the credibility of the parameter values, $\theta$, without the data, $D$. Since ***it must not*** be inferred from the data, it is specified using our **prior belief** of what the parameters should be, before interrogating the data. Prior information can be sourced from previous analyses (of other data!), literature, meta-analyses, expert opinion or ecological theory. We need to be very careful about how we specify our priors so as not to bias the analysis!

<br>

### The benefits of Bayes

1. They are **focused on estimating what properties are** (i.e. the actual values of particular parameters) and not null hypothesis testing
2. They are highly **flexible**, allowing one to build relatively **complex models with varied data sources** (and/or of varying quality), especially Hierarchical Bayesian models
  - This is important for ***Data Fusion*** that we'll discuss today
3. They can easily treat all terms as probability distributions, making it **easier to quantify, propagate, partition and represent uncertainties throughout the analysis probabilistically**. 
  - Which we'll touch on today, but is the focus of section \@ref(uncertainty) (our next lecture).
4. They provide an **iterative** probabilistic framework that makes it easier to update predictions as new data become available, mirroring the scientific method and completing the forecasting cycle.
  - This closing of the forecast loop is termed ***Data Assimilation*** and we'll discuss it towards the end of the lecture.

<br>

## Data fusion

Here we'll revisit the Fynbos postfire recovery model we played with in the practical to highlight:

- including varied data sources
- using priors to borrow strength across your data (by using Hierarchical Bayes and hyperpriors)
- gap filling

To illustrate all this we should quickly revisit how our model structure has changed as we've moved from Least Squares to Maximum Likelihood Estimation to "single-level" Bayes to Hierarchical Bayes.

<br>

### Model layers

***Least Squares***

The method of **Least Squares** makes no explicit distinction between the **process model** (that models the drivers determining the pattern observed) and the **data model** (that models the observation error or data observation process, i.e. the factors that may cause mismatch between the process model and the data). This is because we require homogeneity of variance in order to minimize the sums of squares, so the data model can only ever be a **normal distribution**.

Looking back at the practical, what this means is that for the Nonlinear Least Squares all we fitted was the process model, which for our simpler negative exponential model not including the seasonality term:

\begin{gather}
  \text{NDVI}_{i,t}=\alpha_i+\gamma_i\Big(1-e^{-\frac{age_{i,t}}{\lambda_i}}\Big)
\end{gather}

<br>

***Maximum Likelihood***

MLE does make a distinction between the **data model** and the **process model**, like so:

```{r mlelayers, echo=F, out.width='50%', fig.align='center', fig.cap='Maximum Likelihood makes a distinction between the process and data models.'}
knitr::include_graphics("img/MLElayers.png")
```

<br>

I didn't make the distinction in the equations presented in the practical, but we did in the functions we wrote, where we included the **data model**:

\begin{gather}
NDVI_{i,t}\sim\mathcal{N}(\mu_{i,t},\frac{1}{\sqrt{\tau}}) \\
\end{gather}

which is a Gaussian (normal) distribution, just like the NLS analysis, and was included as part of the the likelihood function, and separated from the function for the **process model**:

\begin{gather}  
\mu_{i,t}=\alpha_i+\gamma_i\Big(1-e^{-\frac{age_{i,t}}{\lambda_i}}\Big)\\
\end{gather}

Where $\mu$ is now the mean of the process model, not accounting for the residual error described by the data model - similar to the open circles in Figure \@ref(fig:leastsquares2).

<br>

***Single-level Bayes***

When we use Bayesian models we now have the **priors**, which are essentially models describing our prior expectation for each of the parameters in the process model, like so: 

```{r bayeslayers, echo=F, out.width='50%', fig.align='center', fig.cap='Bayesian models include an additional layer; parameter models that describe our prior expectation for the parameters in the process.'}
knitr::include_graphics("img/bayeslayers.png")
```

<br>

When implementing Bayesian models, we just specify the priors as **parameter models**, e.g.:

\begin{gather}
\alpha_i\sim\mathcal{N}(\mu_{\alpha},\frac{1}{\sqrt{\tau_{\alpha}}})\\
\gamma_i\sim\mathcal{N}(\mu_{\gamma},\frac{1}{\sqrt{\tau_{\gamma}}})\\
\lambda_i\sim\mathcal{N}(\mu_{\lambda},\frac{1}{\sqrt{\tau_{\lambda}}})\\
\end{gather}

Where (in this case) we believe they are all sampled from normal distributions with means $\mu$ and standard deviations $\tau$.

Note that you need priors (i.e. parameter models) for all parameters.

<br>

***Hierarchical Bayes***

- The value of priors and HB for Data Fusion with post-fire recovery as an example? 


<br>

\begin{gather}  
\mu_{i,t}=\alpha_i+\gamma_i\Big(1-e^{-\frac{age_{i,t}}{\lambda_i}}\Big)+
      A_i\times sin\Big(2\pi\times age_{i,t}+\Big[\phi+\frac{\pi}{6}(m_{i,t}-1)\Big]\Big) \\
\end{gather}

<br>

## Propogating and quantifying uncertainty using MCMC

***Move to next lecture!!!***

Moncrieff et al example 

<br>

## Completing the forecast cycle

- Revisiting the forecast cycle… (forecast vs analysis step) - Ch13.1
  - Forecast step is just propagating the uncertainty in our variable(s) of interest into the future…
  - Analysis step is combining our prior knowledge with the new data
    - This is better than just using the new data, because 
      - it uses our prior information and understanding
      - there is likely error (noise) in the data
      - somewhat similar to borrowing strength across your data in space in the postfire example, but in time?
      - if the forecast is uncertain and the data precise then the data will prevail
      - if the forecast is precise and the data uncertain, then we retain the legacy of previous observations
  - There are a number of methods for achieving these steps, but they come with trade-offs between their assumptions and computational efficiency, with MCMC being the most flexible (i.e. least assumptions), but computationally taxing because it is a numerical solution. More efficient analytical methods (e.g. Kalman filter) have many limitations and assumptions (e.g. linear, normal only).

# Data Fusion and Assimilation {#datafusion}

This lecture builds on section \@ref(bayesian), "Going Bayesian", focusing on how a Bayes Theorem allows us to:

1. ***Fuse*** multiple sources of data that vary in their spatial or temporal scale, uncertainty, gaps, etc into our models, and
2. ***Assimilate*** the previous forecast and new observations so that we can update our forecasts for the next iteration

<br>

## A quick reminder of Bayes Theorem and the benefits it provides

<br>

$$
\underbrace{p(\theta|D)}_\text{posterior} \; \propto \; \underbrace{p(D|\theta)}_\text{likelihood} \;\; \underbrace{p(\theta)}_\text{prior} \;
$$
<br>

Where the ***posterior*** is a probability distribution representing the credibility of the parameter values $\theta$, taking the data, $D$, into account.

The ***likelihood***, $p(D|\theta)$ represents the probability that the data could be generated by the model with parameter values $\theta$. This term looks for the best estimate of the parameters using Maximum Likelihood Estimation, where the likelihood of the parameters are maximized by choosing the parameters that maximize the probability of the data.

The ***prior*** probability distribution is the marginal probability of the parameters, $p(\theta)$, and represents the credibility of the parameter values, $\theta$, without the data, $D$. Since ***it must not*** be inferred from the data, it is specified using our **prior belief** of what the parameters should be, before interrogating the data. Prior information can be sourced from previous analyses (of other data!), literature, meta-analyses, expert opinion or ecological theory. We need to be very careful about how we specify our priors so as not to bias the analysis!

<br>

### The benefits of Bayes

1. They are **focused on estimating what properties are** (i.e. the actual values of particular parameters) and not null hypothesis testing
2. They are highly **flexible**, allowing one to build relatively **complex models with varied data sources** (and/or of varying quality), especially Hierarchical Bayesian models
- This is important for ***Data Fusion*** that we'll discuss today

3. They can easily treat all terms as probability distributions, making it **easier to quantify, propagate, partition and represent uncertainties throughout the analysis probabilistically**. 
- Which is the focus of section \@ref(uncertainty) (our next lecture).

4. They provide an **iterative probabilistic framework** that makes it easier to update predictions as new data become available, mirroring the scientific method and completing the forecasting cycle.
- This closing of the forecast loop is termed ***Data Assimilation*** and we'll discuss it towards the end of the lecture.

<br>

## Data fusion

Data can enter (or be fused with) a model in a variety of ways. Here we'll discuss these and then give an example by revisiting the Fynbos postfire recovery model we played with in the practical.

The **opportunities for data fusion are linked to model structure**, so we'll revisit how some aspects of model structure change as we move from Least Squares to Maximum Likelihood Estimation to "single-level" Bayes to Hierarchical Bayes and the data fusion opportunities provided by each.

<br>

### Model layers

***Least Squares***

The method of **Least Squares** makes no explicit distinction between the **process model** (that models the drivers determining the pattern observed) and the **data model** (that models the observation error or data observation process, i.e. the factors that may cause mismatch between the process model and the data). This is because we require homogeneity of variance in order to minimize the sums of squares, so the data model can only ever be a **normal distribution**.

Looking back at the practical, what this means is that for the Nonlinear Least Squares all we fitted was the process model, which for our simpler negative exponential model not including the seasonality term:

\begin{gather}
  \text{NDVI}_{i,t}=\alpha_i+\gamma_i\Big(1-e^{-\frac{age_{i,t}}{\lambda_i}}\Big)
\end{gather}

**The process model was the only opportunity for inputting data**, and here we input time series for NDVI and date (or postfire age since our time series started at the time of a fire). The only way to add more data sources is to make your model more complex (e.g. we added the month of the fire when we added the seasonality term to our model).

<br>

***Maximum Likelihood***

MLE does make a distinction between the **data model** and the **process model**, like so:

<br>

```{r mlelayers, echo=F, out.width='40%', fig.align='center', fig.cap='Maximum Likelihood makes a distinction between the process and data models.'}
knitr::include_graphics("img/MLElayers.png")
```

<br>

I didn't make the distinction in the equations presented in the practical, but we did in the functions, where we included the **data model**:

\begin{gather}
NDVI_{i,t}\sim\mathcal{N}(\mu_{i,t},\frac{1}{\sqrt{\tau}}) \\
\end{gather}

which is the likelihood function. In this case we specified a Gaussian (normal) distribution around the mean, $\mu$, which is described by the process model. We then had a separate function for the **process model**:

\begin{gather}  
\mu_{i,t}=\alpha_i+\gamma_i\Big(1-e^{-\frac{age_{i,t}}{\lambda_i}}\Big)\\
\end{gather}

Where $\mu$ is now the mean of the process model, not accounting for the residual error described by the data model - similar to the open circles in Figure \@ref(fig:leastsquares2).

What splitting the data and process models means, is that **we are now feeding out dependent variable (NDVI) to the data model, and our independent variable (time or postfire age) to the process model**.

The beauty of having a separate data model is that you are no longer restricted to the normal distribution and have flexibility to specify probability distributions that suit the data observation process, e.g. Binomial coin flips, Poisson counts of individuals, Exponential waiting times, etc. **You can even specify custom data models with their own covariates**, which is useful if you have information on things like instrument drift and calibration, etc - i.e. **another opportunity for data fusion**.

<br>

> **Side note:** You'd be interested to know that specifying a normal distribution in the likelihood (i.e. the data model) means that our MLE analysis is identical to the NLS analysis. In fact, you can do the maths to show that **MLE with a normal likelihood is exactly the same as Least Squares**. Any differences between the parameter estimates the two methods provided in the practical were purely because we had to solve the equations numerically (i.e. estimate) rather than analytically (i.e. diret calculation). Where MLE gets useful is when you start doing interesting things with the data model.

<br>

***Single-level Bayes***

When we use Bayesian models we now have the **priors**, which are essentially models describing our prior expectation for each of the parameters in the process model, like so: 

<br>

```{r bayeslayers, echo=F, out.width='40%', fig.align='center', fig.cap='Bayesian models include an additional layer; parameter models that describe our prior expectation for the parameters in the process.'}
knitr::include_graphics("img/bayeslayers.png")
```

<br>

When implementing Bayesian models, we just specify the priors as **parameter models**, e.g.:

\begin{gather}
\alpha_i\sim\mathcal{N}(\mu_{\alpha},\frac{1}{\sqrt{\tau_{\alpha}}})\\
\gamma_i\sim\mathcal{N}(\mu_{\gamma},\frac{1}{\sqrt{\tau_{\gamma}}})\\
\lambda_i\sim\mathcal{N}(\mu_{\lambda},\frac{1}{\sqrt{\tau_{\lambda}}})\\
\end{gather}

Where (in this case) we're saying we believe the three parameters from our simpler postfire recovery model are all sampled from normal distributions with independent means and variances.

Note that **you need priors (i.e. parameter models) for all parameters**. They don't all have to be independent though, which can be useful, for example if you have multiple separate sets drawn from the same population, etc.

While you can't specify new data in the priors in single-level Bayes (because that is then a Hierarchical model, coming next) it does still provide new opportunities for data fusion, because **the conditional nature of Bayes Theorem allows you to chain multiple likelihoods (i.e. data models) together**. I'm not going to go into the details of this, but one important consideration is that there need to be links between all terms - e.g. two datasets ($D_1$ and $D_2$) that share the same parameters ($\theta$):

$$
\underbrace{p(D_1|\theta)}_\text{likelihood 1} \;\;
\underbrace{p(D_2|\theta)}_\text{likelihood 2} \;\;\underbrace{p(\theta)}_\text{prior} \;
$$
or one dataset ($D_1$) is conditional on another ($D_2$), that is conditional on the parameters ($\theta$):

$$
\underbrace{p(D_1|D_2)}_\text{likelihood 1} \;\;
\underbrace{p(D_2|\theta)}_\text{likelihood 2} \;\;\underbrace{p(\theta)}_\text{prior} \;
$$
etc.

<br>

***Hierarchical Bayes***

Hierarchical Bayes is a form of multilevel modelling that provides **incredible flexibility** for model specification. Of course, greater flexibility brings more opportunities to tie yourself into knots, but such is life.

Most of this flexibility comes from **specifying the priors as models with inputs and specifying relationships among the priors, or from one prior to multiple layers in the model**. We don't have time to explore all the options (because they're almost endless). In the example I give below, we used a set of **environmental covariates** (soil, climate, topography, etc.) to explain the variation in the priors and constrain the parameters of the postfire recovery curve.

In other words, **Hierarchical Bayesian models allow you to fuse data through the parameter model**. Of course, when you start treating your priors as parameters like this, you have to specify priors on your priors (**hyperpriors**) to your parameter model.

<br>

```{r hbayeslayers, echo=F, out.width='40%', fig.align='center', fig.cap='Hierarchical Bayesian models allow considerable flexibility through the inclusion of hyperparameters that can drive the priors.'}
knitr::include_graphics("img/hbayeslayers.png")
```

<br>

### Hierarchical Bayes and postfire recovery example

Until now we have only been considering a single postfire recovery curve in isolation, but one can fit every MODIS pixel in the Fynbos Biome in one model, like @Wilson2015 did, (here including the seasonality term in the process model):

```{r hbpostfire, echo=F, out.width='90%', fig.align='center', fig.cap='Schematic of the Hierarchical Bayesian model of postfire NDVI recovery developed by @Wilson2015.'}
knitr::include_graphics("img/hbpostfire.png")
```

<br>

The beauty of this model is that it **simultaneously estimates** the posterior distributions of **the parameters** (by maximizing their likelihood given the observed NDVI data) while also estimating their relationship with a set of **environmental covariates**. This was done by **fusing multiple data sources** at different levels of the model.

- to the **data model** we've passed the $NDVI$ time-series for each pixel
  - note that we could fuse more data here as covariates or additional likelihood functions if we thought that additional complexity was useful
- to the **process model** we've passed the fire history for each pixel (as a time-series of vegetation $age$ since fire) and the month ($m$) in which each fire occurred (needed for the seasonality term)
- to the **parameter model** we've passed static layers of each environmental covariate

<br>

**The advantages** of including the regression of the parameters on environmental covariates are many:

- It allows you to **explore the dependence** of recovery trajectories on environmental covariates
- This allows one to **project the expected parameters** (and recovery trajectories) under altered environments (e.g. future climate scenarios - as @Wilson2015 did)
- It allows you to **predict the expected postfire recovery trajectory** for any site with known environmental covariates
- It puts an additional constraint on the parameter estimates for each pixel (i.e. "borrowing strength" across the analysis), **down-weighting the effects of anomalous or noisy NDVI data**
- The structure of the model allows you to easily **estimate any missing data** (i.e. *inverse modelling*) all within one model run

<br>

> **A brief note on data fusion, model complexity and uncertainty:** Increasing model complexity can provide many opportunities for fusing new data into your model, but you should be aware that this comes with trade-offs. Firstly, all data are uncertain (even if they don't come with with uncertainty estimates), so **adding new data sets or types to your model includes adding new sources of uncertainty**. For example, a major failing of the postfire recovery model is that we did not include estimates of uncertainty in the covariates. This falsely reduces the uncertainty, resulting in overconfidence in our parameter estimates...
Secondly, more terms and interactions in the model creates greater opportunity for **strange feedbacks and trade-offs in the model's mechanics**, especially where there is non-identifiability (where multiple parameters can influence the outcome, but there's not enough information in the model for it to partition their influence). This can bias or produce unrealistic estimates etc... Last, this is all over and above the usual dangers of overfitting.

<br>

**Utility of this model for decision makers**

While the model **does not make explicit near-term forecasts** in its current form, it can give us an estimate of the **expected NDVI signal** for any location for a given vegetation age since fire and time of year, which **can be very useful!**

@Slingsby2020 used the model to develop a proof of concept for a near-real time satellite change detection system for the Fynbos Biome.

<br>

```{r emma, echo=F, out.width='70%', fig.align='center', fig.cap='Overview of the near-real time satellite change detection workflow from @Slingsby2020.'}
knitr::include_graphics("img/emmaprocessdiagram.png")
```

<br>

The work flow comprises four major steps (Figure \@ref(fig:emma)): 

1. Fit the **model** to get parameter estimates that describe the postfire recovery trajectory for all pixels 
2. **Evaluate** the deviation of the observed NDVI from model expectation using departure metrics to identify areas that are not 'behaving' as expected. 
3. **Diagnose** the change agents driving deviations through a combination of: 
  - interpreting the model predictions and observed NDVI signal 
  - exploring high resolution imagery like Sentinel or PlanetLabs
  - field visits
4. Finally, as new information is learned or false deviations are detected, the model is iteratively **updated** (manually at this stage) to improve its predictive accuracy.

<br>

Preliminary results from the Cape Peninsula are very promising for detecting fire scars, vegetation clearing, vegetation mortality (e.g. drought impacts), and alien plant invasions.

<br>

```{r emmachange, echo=F, out.width='90%', fig.align='center', fig.cap="Examples of changes detected by @Slingsby2020. Postfire recovery curves (a, d, g, j, l, n) show the model predictions (dark grey $=$ 50% Confidence Interval, light grey $=$ 95% Confidence Interval) and observed MODIS NDVI signal (blue line) for different impacts. Satellite images and ground photos allow for diagnosis of detected anomalies, including fire at Karbonkelberg (a-c), gradual clearing of alien vegetation at Miller's Point (d-f), clearing of indigenous vegetation for a housing development near Silvermine (g-i, labeled 1), high mortality in a dense stand of *Leucadendron coniferum* near Silvermine (h-k, labelled 2), a flush of alien Australian Port Jackson Willow (*Acacia saligna*) recruitment from seed at Silvermine triggered by fire (l, m), and retarded postfire vegetation recovery and high mortality of the large fire-resistant shrub *Mimetes fimbriifolius* due to drought near the Cape of Good Hope (n, o). Satellite imagery &copy; 2017 Planet Labs Inc."}
knitr::include_graphics("img/emmachange.jpg")
```

<br>

So managers can get an overview of change in the landscape every time new MODIS data are collected (daily, or 16-day averages).

<br>

```{r emmaexceedmap, echo=F, out.width='90%', fig.align='center', fig.cap='Overview map highlighting some of the major changes detected by @Slingsby2020.'}
knitr::include_graphics("img/emmaexceedmap.png")
```

<br>

We currently have a grant from NASA and are working with CapeNature, SANParks, SANBI, SAEON and others to develop into a fully operational system - currently called [EMMA (Ecosystem Monitoring for Management Application)](https://www.emma.eco/). We're also exploring methods to turn it into a *bona fide* near-term iterative ecological forecasting system, which brings us to...

<br>

## Completing the forecast cycle (data assimilation)

```{r ecoforecastingloop3, echo=FALSE, fig.cap = "The iterative ecological forecasting cycle in the context of the scientific method, demonstrating how we stand to learn from making iterative forecasts. From [lecture on data assimilation by Michael Dietze](https://www.dropbox.com/s/pqjozune75m7wl0/09_DataAssimilation.pptx?dl=0). (Please excuse that the colours of the distributions have changed from above...).", fig.width=3, fig.align = 'center', out.width="70%"}
knitr::include_graphics("img/ecoforecastingloop.png")
```

<br>

You'll recall from the introductory lecture on models and decision making (section \@ref(models)) that we outlined iterative ecological forecasting and the scientific method as closely aligned cycles or loops. You'll also recall that I made the point in the last lecture (section \@ref(bayesian)) that **Bayes Theorem provides an iterative probabilistic framework** that makes it easier to update predictions as new data become available, mirroring the scientific method and completing the forecasting cycle. Here we'll unpack this in a bit more detail.

### Operational data assimilation

Most modelling workflows are set up to **(a) fit the available data** and estimate parameters etc (we'll call this ***analysis***), which they then often use to **(b) make predictions** (which if made forward through time are typically ***forecasts***). They usually stop there. Few workflows are set up to make forecasts repeatedly or iteratively, updating predictions as new observations are made.

When making iterative forecasts, one **could just refit the model** and entire dataset with the new observations added, but there are a few reasons this may not be ideal:

1. If it's a complex model and/or a large dataset this could be very **computationally taxing** (e.g. @Slingsby2020 ran the Hierarchical postfire model for the Cape Peninsula (~4000 MODIS pixels) and it took 4 days on a computing cluster with 64 cores and 1TB of RAM...).
2. Refitting the model **doesn't make the most of learning** from new observations and the forecast cycle...

The alternative is to **assimilate the data sequentially**, through forecast cycles, imputing observations a bit at a time as they're observed. This approach has several advantages:

1. They can handle larger datasets, because **you don't have to assimilate all data in one go**.
2. If you start the model far in the past and update towards present day, you have **the opportunity to validate your predictions**, telling you how well the model does, and whether it improves with each iteration (i.e. is it learning), which gives you a good feel for how well it should do at forecasting into the future. Think of it as letting your model get a run-up like a sportsperson before jumping/bowling/throwing.

Assimilating data sequentially is known as the ***sequential or operational data assimilation problem*** and occurs through two steps (the main components of the forecast cycle), ***the forecast step***, where we project our estimates of the state forward in time, and ***the analysis step***, where we update our estimate of the state based on new observations.

<br>

```{r forecaststeps, echo=F, message=F, fig.cap = "The two main components of the forecast cycle are **the forecast step**(stippled lines), where we project from the initial state at time 0 (t0) to the next time step (t0+1), and **the analysis step**, where we use the forecast and new observations to get an updated estimate of the current state at t0+1.", fig.align = 'center', out.width="75%"}
ddat <- data.frame(x = c(rnorm(5000, 0, 1), rnorm(5000, 5, 2), rnorm(5000, -1, 1), rnorm(5000, 1, 1)), 
                   Label = factor(c(rep("1. Initial state", 5000), rep("2. Forecast", 5000), rep("3. New observations", 5000), rep("4. Updated state", 5000)), levels = c("1. Initial state", "2. Forecast", "3. New observations", "4. Updated state")), 
                   Facet = c(rep("t0", 5000), rep("t0+1", 15000))) 

ggplot(ddat, aes(x, fill = Label, colour = Label)) +
  geom_density(alpha = 0.1, bw = 1) +
  geom_segment(aes(x = 4, y = 0, xend = 10, yend = 0.4), col = "gray50", size = 0.5, alpha = .2, linetype = 3, data = subset(ddat, Facet == "t0")) +
    geom_segment(aes(x = 0, y = 0, xend = 5, yend = 0.4), col = "gray50", size = 0.5, alpha = .2, linetype = 3, data = subset(ddat, Facet == "t0")) +
    geom_segment(aes(x = -4, y = 0, xend = 0, yend = 0.4), col = "gray50", size = 0.5, alpha = .2, linetype = 3, data = subset(ddat, Facet == "t0")) +
  ylab("Density") +
  coord_flip() +
  facet_wrap(vars(Facet))

```

<br>

### The Forecast Step

While it gets a bit like [the chicken and the egg dilemma](https://en.wikipedia.org/wiki/Chicken_or_the_egg), the first step has to be analysis, because you have to fit your model before you can make your first forecast (although you could argue that your first analysis requires priors, which could be viewed as your first forecast...).

Either way, the forecast step is probably easier to explain first.

The goals of the forecast step are to:

1. To **predict** what we think may happen (or what the focal variable(s) will be) at the next time step
2. Indicate the **uncertainty** in our forecast (based on the uncertainty that we have propagated through our model from various sources (data, priors, etc))

In short, we want to ***propagate uncertainty in our variable(s) of interest forward through time*** (and sometimes through space, depending on the goals).

There are **a number of methods** for propagating uncertainty into a forecast, mostly based on the same methods one would use to propagate the uncertainty through a model. Explaining the different methods is beyond the scope of this module, but suffice to say that (as with almost everything in statistics) there's **a trade-off** between the methods whereby the most efficient (the Kalman filter in this case) also come with the most stringent assumptions (linear models and homogeneity of variance only), while the most flexible (Markov chain Monte Carlo (MCMC) in this case) are the most computationally taxing. In short, if your model isn't too taxing, or you have cheap access to a very large computer and time to kill, MCMC is probably easiest and best...

<br>

### The Analysis Step

This step involves using Bayes Theorem to combine our prior knowledge (our forecast) with our new observations (at t0+1 in Figure \@ref(fig:forecaststeps)) to generate an updated state for the next forecast (i.e. to t0+2). 

<br>

```{r dataassimilation, echo=FALSE, fig.cap = "The forecast cycle chaining together applications of Bayes Theorem at each timestep (t0, t1, ...). The forecast from one timestep becomes the prior for the next. Note that the forecast is directly sampled as a posterior distribution when using MCMC, but can also be propagated using other methods.", fig.width=3, fig.align = 'center', out.width="100%"}
knitr::include_graphics("img/dataassimilation.png")
```

<br>

This is better than just using the new data as your updated state, because:

- it **uses our prior information** and understanding
- it allows our model to **learn and (hopefully) improve** with each iteration
- there is likely **error (noise) in the new data**, so it can't necessarily be trusted more than our prior understanding

Fortunately, **Bayes Theorem deals with the third point very nicely**, because if the forecast (prior) is uncertain and the new data precise then the data will prevail, whereas if the forecast is precise and the new data uncertain, then the posterior will retain the legacy of previous observations (Figure \@ref(fig:analysissteps)).

<br>

```{r analysissteps, echo=F, message=F, fig.cap = "The result of (A) high forecast uncertainty (the prior) and low observation error (data), and (B) low forecast uncertainty and high observation error on the posterior probability from the analysis step.", fig.align = 'center', out.width="75%"}
ddat <- data.frame(x = c(rnorm(5000, 0, 1), rnorm(5000, 0, 3), rnorm(5000, 1, 0.75), rnorm(5000, 4, 0.75), rnorm(5000, 5, 3), rnorm(5000, 5, 1)), 
                   Label = c(rep("Data", 10000), rep("Posterior", 10000), rep("Prior", 10000)), 
                   Facet = rep(c(rep("A", 5000), rep("B", 5000)), 3)) 
ggplot(ddat, aes(x, fill = Label, colour = Label)) +
  geom_density(alpha = 0.1, bw = 1) +
  ylab("Density") +
  coord_flip() +
  facet_wrap(vars(Facet))
```

<br>

Lastly, just a note that I've mostly dealt with single forecasts and haven't talked about how to deal with ***ensemble forecasts***. In short, there are methods to deal with them, but we don't have time to cover them. The methods, and how you apply them, depend on what kind of ensemble you are dealing with. Usually, ensembles can be divided into **three kinds**, but you can have mixes of all three:

1. Where you use the **same model**, but vary the inputs to explore **different scenarios**.
2. Where you have a set of **nested models** of increasing complexity (e.g. like our postfire models with and without the seasonality term).
3. A mix of models with **completely different model structures** aimed at forecasting the same thing.

<br>


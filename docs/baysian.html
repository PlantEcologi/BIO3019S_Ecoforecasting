<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>7 Going Bayesian | A Minimal Introduction to Ecological Forecasting and Reproducible Research</title>
  <meta name="description" content="This is a minimal introduction to Ecological Forecasting and Reproducible Research for the 3rd year undergraduate Biological Sciences BIO3019S class at the University of Cape Town." />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="7 Going Bayesian | A Minimal Introduction to Ecological Forecasting and Reproducible Research" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a minimal introduction to Ecological Forecasting and Reproducible Research for the 3rd year undergraduate Biological Sciences BIO3019S class at the University of Cape Town." />
  <meta name="github-repo" content="jslingsby/BIO3019S_Ecoforecasting" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7 Going Bayesian | A Minimal Introduction to Ecological Forecasting and Reproducible Research" />
  
  <meta name="twitter:description" content="This is a minimal introduction to Ecological Forecasting and Reproducible Research for the 3rd year undergraduate Biological Sciences BIO3019S class at the University of Cape Town." />
  

<meta name="author" content="Jasper Slingsby" />


<meta name="date" content="2021-10-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="practical.html"/>
<link rel="next" href="references.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Introduction to Ecological Forecasting and Reproducible Research</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Overview</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#general"><i class="fa fa-check"></i><b>1.1</b> General</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#lectures-and-practicals"><i class="fa fa-check"></i><b>1.2</b> Lectures and practicals</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#preparation"><i class="fa fa-check"></i><b>1.3</b> Preparation</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#a-bit-about-me"><i class="fa fa-check"></i><b>1.4</b> A bit about me</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#acknowledgements-and-further-reading"><i class="fa fa-check"></i><b>1.5</b> Acknowledgements and further reading:</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="pracprep.html"><a href="pracprep.html"><i class="fa fa-check"></i><b>2</b> Preparation for the practical</a></li>
<li class="chapter" data-level="3" data-path="models.html"><a href="models.html"><i class="fa fa-check"></i><b>3</b> Models and decision making</a>
<ul>
<li class="chapter" data-level="3.1" data-path="models.html"><a href="models.html#the-basics-of-making-a-decision"><i class="fa fa-check"></i><b>3.1</b> The basics of making a decision</a></li>
<li class="chapter" data-level="3.2" data-path="models.html"><a href="models.html#getting-quantitative"><i class="fa fa-check"></i><b>3.2</b> Getting quantitative</a></li>
<li class="chapter" data-level="3.3" data-path="models.html"><a href="models.html#iterative-decision-making"><i class="fa fa-check"></i><b>3.3</b> Iterative decision-making</a></li>
<li class="chapter" data-level="3.4" data-path="models.html"><a href="models.html#iterative-decision-making-and-the-scientific-method"><i class="fa fa-check"></i><b>3.4</b> Iterative decision-making and the scientific method</a></li>
<li class="chapter" data-level="3.5" data-path="models.html"><a href="models.html#the-importance-of-prediction-in-ecology"><i class="fa fa-check"></i><b>3.5</b> The importance of prediction in ecology</a></li>
<li class="chapter" data-level="3.6" data-path="models.html"><a href="models.html#iterative-near-term-ecological-forecasting"><i class="fa fa-check"></i><b>3.6</b> Iterative near-term ecological forecasting</a></li>
<li class="chapter" data-level="3.7" data-path="models.html"><a href="models.html#iterative-ecological-forecasting-in-context"><i class="fa fa-check"></i><b>3.7</b> Iterative ecological forecasting in context</a></li>
<li class="chapter" data-level="3.8" data-path="models.html"><a href="models.html#reproducible-research"><i class="fa fa-check"></i><b>3.8</b> Reproducible research</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="forecasts.html"><a href="forecasts.html"><i class="fa fa-check"></i><b>4</b> Making forecasts</a>
<ul>
<li class="chapter" data-level="4.1" data-path="forecasts.html"><a href="forecasts.html#proteaceae-as-a-model-organisms"><i class="fa fa-check"></i><b>4.1</b> Proteaceae as a model organisms</a></li>
<li class="chapter" data-level="4.2" data-path="forecasts.html"><a href="forecasts.html#proteaceae-as-management-indicators"><i class="fa fa-check"></i><b>4.2</b> Proteaceae as management indicators</a></li>
<li class="chapter" data-level="4.3" data-path="forecasts.html"><a href="forecasts.html#potential-issues-with-the-rules-of-thumb"><i class="fa fa-check"></i><b>4.3</b> Potential issues with the rules of thumb…</a></li>
<li class="chapter" data-level="4.4" data-path="forecasts.html"><a href="forecasts.html#assessing-population-viability"><i class="fa fa-check"></i><b>4.4</b> Assessing population viability</a></li>
<li class="chapter" data-level="4.5" data-path="forecasts.html"><a href="forecasts.html#climate-and-fire-driven-changes-in-demographic-rates-and-distribution"><i class="fa fa-check"></i><b>4.5</b> Climate and fire-driven changes in demographic rates and distribution</a></li>
<li class="chapter" data-level="4.6" data-path="forecasts.html"><a href="forecasts.html#near-term-iterative-ecological-forecasts"><i class="fa fa-check"></i><b>4.6</b> Near-term iterative ecological forecasts?</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="reproducibility.html"><a href="reproducibility.html"><i class="fa fa-check"></i><b>5</b> Reproducible research</a>
<ul>
<li class="chapter" data-level="5.1" data-path="reproducibility.html"><a href="reproducibility.html#the-reproducibility-crisis"><i class="fa fa-check"></i><b>5.1</b> The Reproducibility Crisis</a></li>
<li class="chapter" data-level="5.2" data-path="reproducibility.html"><a href="reproducibility.html#replication-and-the-reproducibility-spectrum"><i class="fa fa-check"></i><b>5.2</b> Replication and the Reproducibility Spectrum</a></li>
<li class="chapter" data-level="5.3" data-path="reproducibility.html"><a href="reproducibility.html#why-work-reproducibly"><i class="fa fa-check"></i><b>5.3</b> Why work reproducibly?</a></li>
<li class="chapter" data-level="5.4" data-path="reproducibility.html"><a href="reproducibility.html#barriers-to-working-reproducibly"><i class="fa fa-check"></i><b>5.4</b> Barriers to working reproducibly</a></li>
<li class="chapter" data-level="5.5" data-path="reproducibility.html"><a href="reproducibility.html#reproducible-scientific-workflows"><i class="fa fa-check"></i><b>5.5</b> Reproducible Scientific Workflows</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="reproducibility.html"><a href="reproducibility.html#file-and-folder-management"><i class="fa fa-check"></i><b>5.5.1</b> File and folder management</a></li>
<li class="chapter" data-level="5.5.2" data-path="reproducibility.html"><a href="reproducibility.html#coding-and-code-management"><i class="fa fa-check"></i><b>5.5.2</b> Coding and code management</a></li>
<li class="chapter" data-level="5.5.3" data-path="reproducibility.html"><a href="reproducibility.html#computing-environment-and-software"><i class="fa fa-check"></i><b>5.5.3</b> Computing environment and software</a></li>
<li class="chapter" data-level="5.5.4" data-path="reproducibility.html"><a href="reproducibility.html#sharing-of-the-data-code-publication-etc"><i class="fa fa-check"></i><b>5.5.4</b> Sharing of the data, code, publication etc</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="practical.html"><a href="practical.html"><i class="fa fa-check"></i><b>6</b> Practical: Pair coding with GitHub</a>
<ul>
<li class="chapter" data-level="6.1" data-path="practical.html"><a href="practical.html#objectives"><i class="fa fa-check"></i><b>6.1</b> Objectives</a></li>
<li class="chapter" data-level="6.2" data-path="practical.html"><a href="practical.html#postfire-regeneration"><i class="fa fa-check"></i><b>6.2</b> Postfire regeneration</a></li>
<li class="chapter" data-level="6.3" data-path="practical.html"><a href="practical.html#modular-design"><i class="fa fa-check"></i><b>6.3</b> Modular Design</a></li>
<li class="chapter" data-level="6.4" data-path="practical.html"><a href="practical.html#task-1-create-clone-repository"><i class="fa fa-check"></i><b>6.4</b> Task 1: Create &amp; Clone Repository</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="practical.html"><a href="practical.html#owner"><i class="fa fa-check"></i><b>6.4.1</b> OWNER</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="practical.html"><a href="practical.html#task-2-add-the-first-function-download.ndvi"><i class="fa fa-check"></i><b>6.5</b> Task 2: Add the first function: download.NDVI</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="practical.html"><a href="practical.html#owner-1"><i class="fa fa-check"></i><b>6.5.1</b> OWNER</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="practical.html"><a href="practical.html#task-3-collaborator-adds-plot.ndvi"><i class="fa fa-check"></i><b>6.6</b> Task 3: Collaborator adds plot.NDVI</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="practical.html"><a href="practical.html#collaborator"><i class="fa fa-check"></i><b>6.6.1</b> COLLABORATOR</a></li>
<li class="chapter" data-level="6.6.2" data-path="practical.html"><a href="practical.html#owner-2"><i class="fa fa-check"></i><b>6.6.2</b> OWNER</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="practical.html"><a href="practical.html#task-4-owner-adds-functions-for-model-fitting-using-mle"><i class="fa fa-check"></i><b>6.7</b> Task 4: Owner adds functions for model fitting using MLE</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="practical.html"><a href="practical.html#owner-3"><i class="fa fa-check"></i><b>6.7.1</b> OWNER</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="practical.html"><a href="practical.html#task-5-collaborator-adds-the-master-script-owner-answers-the-questions"><i class="fa fa-check"></i><b>6.8</b> Task 5: Collaborator adds the master script, Owner answers the questions</a>
<ul>
<li class="chapter" data-level="6.8.1" data-path="practical.html"><a href="practical.html#collaborator-1"><i class="fa fa-check"></i><b>6.8.1</b> COLLABORATOR</a></li>
<li class="chapter" data-level="6.8.2" data-path="practical.html"><a href="practical.html#owner-4"><i class="fa fa-check"></i><b>6.8.2</b> OWNER</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="baysian.html"><a href="baysian.html"><i class="fa fa-check"></i><b>7</b> Going Bayesian</a>
<ul>
<li class="chapter" data-level="7.1" data-path="baysian.html"><a href="baysian.html#learning-objectives"><i class="fa fa-check"></i><b>7.1</b> Learning objectives</a></li>
<li class="chapter" data-level="7.2" data-path="baysian.html"><a href="baysian.html#least-squares"><i class="fa fa-check"></i><b>7.2</b> Least Squares</a></li>
<li class="chapter" data-level="7.3" data-path="baysian.html"><a href="baysian.html#data-versus-process-models"><i class="fa fa-check"></i><b>7.3</b> Data versus process models</a></li>
<li class="chapter" data-level="7.4" data-path="baysian.html"><a href="baysian.html#maximum-likelihood"><i class="fa fa-check"></i><b>7.4</b> Maximum likelihood</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="baysian.html"><a href="baysian.html#joint-conditional-and-marginal-probabilities"><i class="fa fa-check"></i><b>7.4.1</b> Joint, conditional and marginal probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="baysian.html"><a href="baysian.html#bayes-theorem"><i class="fa fa-check"></i><b>7.5</b> Bayes’ Theorem</a></li>
<li class="chapter" data-level="7.6" data-path="baysian.html"><a href="baysian.html#bayes-theorum"><i class="fa fa-check"></i><b>7.6</b> Bayes’ theorum</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="baysian.html"><a href="baysian.html#applied-to-parameters-and-data"><i class="fa fa-check"></i><b>7.6.1</b> Applied to parameters and data</a></li>
<li class="chapter" data-level="7.6.2" data-path="baysian.html"><a href="baysian.html#worked-example"><i class="fa fa-check"></i><b>7.6.2</b> Worked example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/jslingsby/BIO3019S_Ecoforecasting" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Minimal Introduction to Ecological Forecasting and Reproducible Research</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="baysian" class="section level1" number="7">
<h1><span class="header-section-number">7</span> Going Bayesian</h1>
<p>While ecological forecasting and decision support can be done with traditional statistics (often termed “frequentist statistics”) it is generally much easier to do in a Bayesian statistical framework.</p>
<p><strong>Bayesian approaches have several advantages:</strong></p>
<blockquote>
<p><em>NOTE: there are frequentist approaches for doing much of this, but they are typically cumbersome “add-ons” that require many additional assumptions. Once you’re using Bayes you can achieve all this without much extra work.</em></p>
</blockquote>
<ul>
<li>They are typically <strong>focused on estimating what properties are</strong> (i.e. the actual value of a particular parameter) and not just establishing what they are not (i.e. testing for significant difference (null hypothesis testing), as is usually the focus in frequentist statistics)</li>
<li>They can easily treat all terms as probability distributions, making it <strong>easier to quantify, propagate and partition uncertainties throughout the analysis probabilistically</strong> (more on this later)</li>
<li>They are highly <strong>flexible</strong>, allowing one to build relatively <strong>complex models with varied data sources</strong> (and/or of varying quality), especially Hierarchical Bayesian models</li>
<li>They are inherently <strong>iterative</strong>, making it easier to update predictions as new data become available</li>
</ul>
<p>In this section I aim to provide a brief and soft introduction to Bayesian statistics and and illustrate it using the postfire recovery example we worked on in the pair-coding practical.</p>
<p>Before I can introduce Bayes, there are a few basic building blocks we need to establish first. The main one is the concept of likelihood and the estimation of maximum likelihood, since this is a major component of Bayes’ Theorem.</p>
<blockquote>
<p><em>NOTE: This really is a minimalist introduction that only provides the tidbits I need you to know to follow the rest of the module. This note is a proviso to make it clear that I am withholding important details, before anyone accuses me of lies of ommission.</em></p>
</blockquote>
<div id="learning-objectives" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> Learning objectives</h2>
<ul>
<li>How Least Squares works, and why this is a major limitation
<ul>
<li>especially with relation to the “data model”</li>
</ul></li>
<li>How Maximum Likelihood works, and how it gets around the limitations of Least Squares by allowing for more flexible data models</li>
</ul>
<p>(- dealing with the second assumption of Least Squares? identically independently distributed (the iid assumption)?)</p>
<ul>
<li>The difference between likelihood and probability, and why it’s easier to deal with uncertainty if it is expressed in the context of probability</li>
<li>How converting likelihood to probability gives you Bayes Theorem</li>
<li>The “unexpected” advantages of Bayes…
<ul>
<li>Priors</li>
<li>Hierarchical models</li>
</ul></li>
<li>data vs process vs parameter models</li>
<li></li>
</ul>
<p><br></p>
</div>
<div id="least-squares" class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> Least Squares</h2>
<p>But before I introduce Maximum Likelihood, let’s start with Least Squares, which is probably what you are most familiar with.</p>
<p>Traditional parametric statistics like regression analysis and analysis of variance (ANOVA) rely on Ordinary Least Squares (OLS). There are a few other flavours of least squares that allow a bit more flexibility (e.g. nonlinear (NLS) that we used in the practical in section <a href="practical.html#practical">6</a>, partial least squares, etc), but I’m not going to go into these distinctions.</p>
<p>In general, least squares approaches fit models by <strong>minimizing the sums of the squared residuals</strong>. Let’s explore this by looking at an example of a linear regression.</p>
<div class="figure"><span id="fig:leastsquares1"></span>
<img src="BIO3019S-Ecoforecasting_files/figure-html/leastsquares1-1.png" alt="A hypothetical example showing a linear model fit." width="672" />
<p class="caption">
Figure 7.1: A hypothetical example showing a linear model fit.
</p>
</div>
<p><br></p>
<p>Here the model (blue line) is drawn through the cloud of points so as to minimize the sum of the squared vertical (y-axis) differences (i.e. residuals) between each point and the regression line.</p>
<p>Let’s redraw this highlighting the residuals:</p>
<div class="figure"><span id="fig:leastsquares2"></span>
<img src="BIO3019S-Ecoforecasting_files/figure-html/leastsquares2-1.png" alt="The linear model highlighting the residuals." width="672" />
<p class="caption">
Figure 7.2: The linear model highlighting the residuals.
</p>
</div>
<p><br></p>
<p>So the grey lines linking each observed data point to the regression line are the residuals. Note that they are vertical and not orthogonal to the regression line, because they represent the variance in Y (<em>Reward</em> in this case) that is not explained by X (<em>Effort</em> in this case). The open black circles are the Y-values that our linear model predicts for a given X-value. There is no scatter in the predicted values, because the scatter is residual variance that the model cannot account for and predict.</p>
<p>Now let’s have a look at a histogram of the residuals:</p>
<p><br></p>
<div class="figure"><span id="fig:leastsquares3"></span>
<img src="BIO3019S-Ecoforecasting_files/figure-html/leastsquares3-1.png" alt="A histogram of the residuals from the linear model above." width="672" />
<p class="caption">
Figure 7.3: A histogram of the residuals from the linear model above.
</p>
</div>
<p><br></p>
<p>As you can see in this case, the residuals approximate a <strong><em>normal distribution</em></strong> (or should, since I generated them from a normal distribution…). You’ll recall that this is <strong>one of the assumptions when using linear models or ANOVA (often termed “homoscedasticity” or “homogeneity of variance”)</strong> - i.e. it is an assumption of the Least Squares method. Least squares cannot handle residuals that are not normally distributed. If the residuals were not normally distributed, then either we are fitting the wrong model (e.g. we should consider a non-linear rather than a linear model), or our assumptions are violated and we should not be using this technique!</p>
<p>The reason Least Squares requires this assumption is that for minimizing the sums of squares to work, <strong>a unit change in the residuals should be scale invariant</strong>. In other words, the difference between 1 and 2 needs to be the same as the difference between 101 and 102. This is only the case when the residuals are normally distributed (versus log-scale for example). If the scale is variable, then <strong>minimizing the sums of squares does not work</strong>. People often try to get around the assumption of homogeneity of variance by <strong>tranforming</strong> their data (e.g. log or arcsine transform) to try to get them to an invariant scale.</p>
<blockquote>
<p>So where does <strong>significance testing</strong> come into it? Well significance testing is the ability to reject (or to fail to reject) the null hypothesis. In the case of a linear model, <strong>the null hypothesis is that the slope is zero (i.e. there is no effect of X on Y)</strong> and sometimes includes that the intercept should be zero too (although this is not usually required). I don’t have the time to go through the full explanation of how the null hypothesis is tested in this lecture, but it is useful to highlight that the linear model is only considered useful when you can reject the null hypothesis (usually at an alpha of P &lt; 0.05).</p>
</blockquote>
<p><br></p>
</div>
<div id="data-versus-process-models" class="section level2" number="7.3">
<h2><span class="header-section-number">7.3</span> Data versus process models</h2>
<p>It’s useful at this stage to make a distinction between data models and process models.</p>
<ul>
<li>The <em>process model</em> is the bit you’ll be used to, where we describe how the model creates a prediction for a particular set of inputs or covariates (i.e. the linear model in the case above).</li>
<li>The <em>data model</em> describes the residuals (i.e. the mismatch between the process model and the data). It is also often called the data observation process.</li>
</ul>
<p>Because of the reliance on minimizing the sums of squares, <strong>the data model in a Least Squares analysis can only ever be a normal distribution</strong> (i.e. homogeneity of variance). This is suboptimal because:</p>
<ol style="list-style-type: lower-alpha">
<li>the data model can take many other forms</li>
</ol>
<ul>
<li>e.g. Binomial coin flips, Poisson counts of individuals, Exponential waiting times</li>
<li>this is where Maximum Likelihood comes into it’s own</li>
</ul>
<ol start="2" style="list-style-type: lower-alpha">
<li>there are times when one would like to include additional information in the data model</li>
</ol>
<ul>
<li>e.g. sampling variability (e.g. different observers or instruments), measurement errors, instrument calibration, proxy data, unequal variances, missing data</li>
<li>this is where Bayesian models come into their own</li>
</ul>
<p><br></p>
</div>
<div id="maximum-likelihood" class="section level2" number="7.4">
<h2><span class="header-section-number">7.4</span> Maximum likelihood</h2>
<p><strong><em>Maximum likelihood is a method for estimating model parameters</em></strong></p>
<p><em>The likelihood principle states a that parameter value is more likely than another if it is the one for which the data are more probable.</em></p>
<p>Maximum Likelihood Estimation (MLE) applies this principle by optimizing the parameter values such that they maximize the likelihood that the process described by the given model produced the data that were observed.</p>
<p>If this seems like a tongue-twister full of double-negatives that’s because it is…</p>
<p>Viewed differently, when using MLE we assume that we have the correct model and apply MLE to choose the parameters so as to maximize the <em>conditional probability</em> of the data given those parameter estimates. The notation for this conditional probability is <span class="math inline">\(P(Data|Parameters)\)</span>.</p>
<p>This process leaves us knowing the likelihood of the best set of parameters and the conditional probability of the data given those parameters <span class="math inline">\(P(Data|Parameters)\)</span>. The problem is that in the context of forecasting (and many other modelling approaches) <strong>what we really want is to be able to express the uncertainty in the parameter estimates as probabilities</strong>. In other words, we are interested in the conditional probability of the parameters given the data <span class="math inline">\(P(Parameters|Data)\)</span>. To get there, we need to apply a little probability theory, which provides a somewhat surprising and very useful byproduct.</p>
<p>First, let’s brush up on our probability theory…</p>
<div id="joint-conditional-and-marginal-probabilities" class="section level3" number="7.4.1">
<h3><span class="header-section-number">7.4.1</span> Joint, conditional and marginal probabilities</h3>
<p>Here we’ll look at the interactions between two random variables and illustrate it with the Venn diagram below.</p>
<div class="figure" style="text-align: center"><span id="fig:vennprobability"></span>
<img src="img/vennprobability.png" alt="Venn diagram illustrating ten events (points) across two sets $x$ and $y$." width="75%" />
<p class="caption">
Figure 7.4: Venn diagram illustrating ten events (points) across two sets <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>.
</p>
</div>
<p><br></p>
<p>First, we can define the <strong><em>joint probability</em></strong>, <span class="math inline">\(P(x,y)\)</span>, which is the probability of both <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> occurring simultaneously, which in the case of Figure <a href="baysian.html#fig:vennprobability">7.4</a> is the probability of occurring within the overlap of the circles = 3/10.</p>
<p>Second, based on the joint probability we can define two <strong><em>conditional probabilities</em></strong>, the probability of <span class="math inline">\(x\)</span> given <span class="math inline">\(y\)</span>, <span class="math inline">\(P(x|y)\)</span>, and the probability of <span class="math inline">\(y\)</span> given <span class="math inline">\(x\)</span>, <span class="math inline">\(P(y|x)\)</span>. In the context of the Venn diagram these would be the probability of being in set <span class="math inline">\(x\)</span> given that we are only considering the points in set <span class="math inline">\(y\)</span> (= 3/6) and the probability of being in set <span class="math inline">\(y\)</span> given that we are only considering the points in set <span class="math inline">\(x\)</span> (= 3/7).</p>
<p>Last, we can define two <strong><em>marginal probabilities</em></strong> <span class="math inline">\(P(x)\)</span> = 7/10 and <span class="math inline">\(P(y)\)</span> = 6/10.</p>
<p>From this, we can see that <strong>the joint probabilities <span class="math inline">\(P(x,y)\)</span> and <span class="math inline">\(P(y,x)\)</span> are identical</strong> (= 0.3).</p>
<p>We can also show that <strong>the joint probabilities are the product of the conditional and marginal probabilities</strong>:</p>
<p><span class="math inline">\(P(x,y)\)</span> = <span class="math inline">\(P(x|y)\)</span> * <span class="math inline">\(P(y)\)</span> = 3/6 * 6/10 = 0.3</p>
<p>And by the same token:</p>
<p><span class="math inline">\(P(y,x)\)</span> = <span class="math inline">\(P(y|x)\)</span> * <span class="math inline">\(P(y)\)</span> = 3/7 * 7/10 = 0.3</p>
<p>Why I’m telling you all this is because this means that we can derive what we want to know, <span class="math inline">\(P(Parameters|Data)\)</span>, as a function of what our maximum likelihood estimation has given us, <span class="math inline">\(P(Data|Parameters)\)</span>…</p>
</div>
</div>
<div id="bayes-theorem" class="section level2" number="7.5">
<h2><span class="header-section-number">7.5</span> Bayes’ Theorem</h2>
<!--- ### Maximum Likelihood Estimation vs Null Hypothesis Testing

In null hypothesis testing you usually want to know the probability of the null hypothesis conditional on the data. Formally, this is called the *conditional probability* of the hypothesis given the data, and can be written in model form as $P(Hypothesis|Data)$.

The somewhat bizarre thing in null hypothesis testing is that the aim is usually to falsify or reject the null hypothesis. So when you're fitting a linear model to data (which you would presumably only do if you believed there may be some effect of the independent (X) variable on the dependent (Y) and variable), your null hypothesis is actually that there is no effect of X on Y (the inverse of your belief).


The maximum likelihood estimator selects the parameter value which gives the observed data the largest possible probability (or probability density, in the continuous case).


### Probability vs Likelihood

While _probability_ and _likelihood_ may seem like similar concepts, the distinction between them is fundamentally important in statistics. 

**Probability relates to possible results (i.e. the data) given a hypothesis.** 

- e.g. if you have an unbias coin (the hypothesis), the probability of a coin toss landing heads up (the result) is 0.5.
- This can also be termed the *conditional probability* of the data given the model, or written in model form $P(Model|Data)$.

**Likelihood relates to the hypotheses, given the results.**

- e.g. say we've performed 1000 tosses and the coin landed heads up 700 times (the data), it is less _likely_ that the hypothesis of an unbias coin is true relative to a hypothesis that the coin favours heads.
- This is the conditional probability of the data given the model, or written in model form $P(Data|Model)$.

This may seem like a subtle and potentially arbitrary distinction, but likelihood comes into it's own when comparing multiple hypotheses - either specified as different parameters for a given model, or as different models altogether. --->
<p><br></p>
<p><br></p>
<p><strong>Here’s an example</strong></p>
<p>We’ll build on the fynbos postfire vegetation recovery example we used in the practical, where we looked at time series of a satellite measure of vegetation greenness or health - the normalized difference vegetation index (NDVI) from the MODIS satellite mission.</p>
<p>Before, we were fitting the model using <strong><em>least squares</em></strong>, which optimizes the parameters of the model to minimize the sum of the squares of the residuals (the residuals being the difference between each observed and fitted value provided by a model).</p>
<p>Maximum likelihood</p>
<p>Add Postfire example</p>
<ul>
<li>demonstrate p-value by fitting lm to NDVI</li>
<li>demonstrate fitting with MLE</li>
<li>fit negative exponential and compare likelihoods</li>
</ul>
<div class="figure"><span id="fig:postfireBayes"></span>
<img src="BIO3019S-Ecoforecasting_files/figure-html/postfireBayes-1.png" alt="Time-series of 16-day normalized difference vegetation index (NDVI) from the MODIS satellite mission following fire in a seasonal wetland in the Silvermine section of Table Mountain National Park." width="672" />
<p class="caption">
Figure 7.5: Time-series of 16-day normalized difference vegetation index (NDVI) from the MODIS satellite mission following fire in a seasonal wetland in the Silvermine section of Table Mountain National Park.
</p>
</div>
<p>If we want to know the rate of vegetation recovery, we could fit a linear model to this and examine the slope, like so:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="baysian.html#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Here I fit a linear model to a dependent variable &quot;NDVI&quot; as a function </span></span>
<span id="cb1-2"><a href="baysian.html#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># of the independent variable &quot;calendar_date&quot;, both of which are columns</span></span>
<span id="cb1-3"><a href="baysian.html#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># in the dataframe &quot;dat&quot;</span></span>
<span id="cb1-4"><a href="baysian.html#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="baysian.html#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit model</span></span>
<span id="cb1-6"><a href="baysian.html#cb1-6" aria-hidden="true" tabindex="-1"></a>fit_linear <span class="ot">&lt;-</span> <span class="fu">lm</span>(NDVI <span class="sc">~</span> calendar_date, <span class="at">data =</span> dat)</span>
<span id="cb1-7"><a href="baysian.html#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="baysian.html#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Summarize and print output</span></span>
<span id="cb1-9"><a href="baysian.html#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit_linear)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = NDVI ~ calendar_date, data = dat)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.215858 -0.051878 -0.002463  0.052636  0.237306 
## 
## Coefficients:
##                 Estimate Std. Error t value             Pr(&gt;|t|)    
## (Intercept)   -1.5102413  0.1921736  -7.859     0.00000000000106 ***
## calendar_date  0.0001147  0.0000109  10.522 &lt; 0.0000000000000002 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.08099 on 136 degrees of freedom
## Multiple R-squared:  0.4488, Adjusted R-squared:  0.4447 
## F-statistic: 110.7 on 1 and 136 DF,  p-value: &lt; 0.00000000000000022</code></pre>
<p>From the p-value you can see that the result is highly significant - i.e. there is very little probability that the slope and intercept are zero - but does that mean we’ve done this right?</p>
<p>Let’s try adding the linear model to our plot:</p>
<div class="figure"><span id="fig:unnamed-chunk-2"></span>
<img src="BIO3019S-Ecoforecasting_files/figure-html/unnamed-chunk-2-1.png" alt="NDVI timeseries overlain with a linear regression." width="672" />
<p class="caption">
Figure 7.6: NDVI timeseries overlain with a linear regression.
</p>
</div>
<p><br></p>
<p>Do you think a linear model is appropriate here?</p>
<p>Let’s have a look at the residuals:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="baysian.html#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>)) <span class="co"># Make a panel plot 2 by 2</span></span>
<span id="cb3-2"><a href="baysian.html#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit_linear)</span></code></pre></div>
<p><img src="BIO3019S-Ecoforecasting_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="baysian.html#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>)) <span class="co"># Reset graphical parameters to single plot</span></span></code></pre></div>
<p><br></p>
<p>It’s actually not too bad, but there’s still lots of interesting stuff going on there. I’m not sold that a linear model is the best we can do. Let’s try something else:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="baysian.html#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Here I fit a negative exponential model to a dependent variable &quot;NDVI&quot; </span></span>
<span id="cb5-2"><a href="baysian.html#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co"># as a function of the independent variable &quot;age&quot; (years since the fire), </span></span>
<span id="cb5-3"><a href="baysian.html#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># both of which are columns in the dataframe &quot;dat&quot;</span></span>
<span id="cb5-4"><a href="baysian.html#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="baysian.html#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert &quot;calendar_date&quot; to postfire age in days since fire</span></span>
<span id="cb5-6"><a href="baysian.html#cb5-6" aria-hidden="true" tabindex="-1"></a>dat<span class="sc">$</span>age <span class="ot">&lt;-</span> (<span class="fu">as.numeric</span>(dat<span class="sc">$</span>calendar_date) <span class="sc">-</span> <span class="fu">min</span>(<span class="fu">as.numeric</span>(dat<span class="sc">$</span>calendar_date), <span class="at">na.rm =</span> T))<span class="sc">/</span><span class="fl">365.25</span></span>
<span id="cb5-7"><a href="baysian.html#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="baysian.html#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Guesstimate list of initial parameter estimates to feed to the model fitting function</span></span>
<span id="cb5-9"><a href="baysian.html#cb5-9" aria-hidden="true" tabindex="-1"></a>start <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">alpha =</span> <span class="fl">0.2</span>, <span class="at">lambda =</span> <span class="fl">0.5</span>, <span class="at">gamma =</span> <span class="fl">0.4</span>)</span>
<span id="cb5-10"><a href="baysian.html#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="baysian.html#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model</span></span>
<span id="cb5-12"><a href="baysian.html#cb5-12" aria-hidden="true" tabindex="-1"></a>fit_negexp <span class="ot">&lt;-</span> <span class="fu">nls</span>(NDVI <span class="sc">~</span> alpha <span class="sc">+</span> gamma <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> <span class="fu">exp</span>(<span class="sc">-</span> age<span class="sc">/</span>lambda)), <span class="at">data =</span> dat, </span>
<span id="cb5-13"><a href="baysian.html#cb5-13" aria-hidden="true" tabindex="-1"></a>              <span class="at">start =</span> start, <span class="at">trace =</span> F, <span class="at">control =</span> <span class="fu">nls.control</span>(<span class="at">maxiter =</span> <span class="dv">500</span>))</span>
<span id="cb5-14"><a href="baysian.html#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="baysian.html#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the data points </span></span>
<span id="cb5-16"><a href="baysian.html#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(dat<span class="sc">$</span>age, dat<span class="sc">$</span>NDVI)</span>
<span id="cb5-17"><a href="baysian.html#cb5-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-18"><a href="baysian.html#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Overlay the fitted model on the plot</span></span>
<span id="cb5-19"><a href="baysian.html#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(dat<span class="sc">$</span>age, <span class="fu">predict</span>(fit_negexp, <span class="fu">list</span>(<span class="at">x =</span> dat<span class="sc">$</span>age)), <span class="at">col =</span> <span class="st">&#39;skyblue&#39;</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><img src="BIO3019S-Ecoforecasting_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p><br></p>
<p>Here we’ve fit the negative exponential function:</p>
<p><span class="math display">\[\begin{gather}
  \text{NDVI}_{i,t}=\alpha_i+\gamma_i\Big(1-e^{-\frac{age_{i,t}}{\lambda_i}}\Big)
\end{gather}\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(\alpha\)</span> is the NDVI at time 0 (i.e. directly after the fire)</li>
<li><span class="math inline">\(\gamma\)</span> is the maximum average <em>increase</em> in NDVI
<ul>
<li>i.e. the maximum NDVI reached by the blue curve is <span class="math inline">\(\alpha + \gamma\)</span></li>
</ul></li>
<li><span class="math inline">\(\lambda\)</span> is the rate of increase in NDVI</li>
</ul>
<p>Since <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\gamma\)</span> are reasonably intuitive to read off the graph we can easily eyeball whether the model estimates for those parameters are any good:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="baysian.html#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit_negexp)</span></code></pre></div>
<pre><code>## 
## Formula: NDVI ~ alpha + gamma * (1 - exp(-age/lambda))
## 
## Parameters:
##        Estimate Std. Error t value             Pr(&gt;|t|)    
## alpha   0.25107    0.02887   8.695   0.0000000000000104 ***
## lambda  1.17687    0.21396   5.500   0.0000001835574518 ***
## gamma   0.32371    0.02723  11.887 &lt; 0.0000000000000002 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.07302 on 135 degrees of freedom
## 
## Number of iterations to convergence: 12 
## Achieved convergence tolerance: 0.000003903</code></pre>
<p>Looks pretty good.</p>
<p>The plot also suggests than a negative exponential is a lot better than a linear fit, but there’s still a lot of noise! Let’s look at the residuals of this model:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="baysian.html#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the residuals of the model</span></span>
<span id="cb8-2"><a href="baysian.html#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit_negexp)</span></code></pre></div>
<p><img src="BIO3019S-Ecoforecasting_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>There’s still some weird stuff going on… especially an interesting oscillating pattern in the fitted values in the range 0.2 to 0.5. It may be present in values &gt;0.5, but there are too many to tell.</p>
<p>This is where thinking about the data becomes important! We know this is the trajectory of vegetation “greenness” following a fire. What factors other than fire may affect greenness at any particular point in time? Below I add a term to the model to account for variation we should expect in most temperate ecosystems.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="baysian.html#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Here I fit a the same negative exponential model (plus a mystery additional term) </span></span>
<span id="cb9-2"><a href="baysian.html#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="co"># to a dependent variable &quot;NDVI&quot; as a function of the independent variable </span></span>
<span id="cb9-3"><a href="baysian.html#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co"># &quot;age&quot; (years since the fire), both of which are in the dataframe &quot;dat&quot;</span></span>
<span id="cb9-4"><a href="baysian.html#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="baysian.html#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Guesstimate list of initial parameter estimates to feed to the model fitting function</span></span>
<span id="cb9-6"><a href="baysian.html#cb9-6" aria-hidden="true" tabindex="-1"></a>start <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">alpha =</span> <span class="fl">0.2</span>, <span class="at">lambda =</span> <span class="fl">0.5</span>, <span class="at">gamma =</span> <span class="fl">0.4</span>, <span class="at">A =</span> <span class="fl">0.6</span>, <span class="at">phi =</span> <span class="dv">0</span>)</span>
<span id="cb9-7"><a href="baysian.html#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="baysian.html#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model</span></span>
<span id="cb9-9"><a href="baysian.html#cb9-9" aria-hidden="true" tabindex="-1"></a>fit_negexpS <span class="ot">&lt;-</span> <span class="fu">nls</span>(NDVI <span class="sc">~</span> alpha <span class="sc">+</span> gamma <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> <span class="fu">exp</span>(<span class="sc">-</span> age<span class="sc">/</span>lambda)) <span class="sc">+</span></span>
<span id="cb9-10"><a href="baysian.html#cb9-10" aria-hidden="true" tabindex="-1"></a>                  A<span class="sc">*</span><span class="fu">sin</span>(<span class="dv">2</span><span class="sc">*</span>pi<span class="sc">*</span>age <span class="sc">+</span> (phi <span class="sc">+</span> pi<span class="sc">/</span><span class="dv">6</span><span class="sc">*</span>(<span class="dv">3</span> <span class="sc">-</span> <span class="dv">1</span>))), <span class="at">data =</span> dat,   </span>
<span id="cb9-11"><a href="baysian.html#cb9-11" aria-hidden="true" tabindex="-1"></a>               <span class="co">#Note that the 3 is the month of fire (March)</span></span>
<span id="cb9-12"><a href="baysian.html#cb9-12" aria-hidden="true" tabindex="-1"></a>              <span class="at">start =</span> start, <span class="at">trace =</span> F, <span class="at">control =</span> <span class="fu">nls.control</span>(<span class="at">maxiter =</span> <span class="dv">500</span>))</span>
<span id="cb9-13"><a href="baysian.html#cb9-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-14"><a href="baysian.html#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the data points </span></span>
<span id="cb9-15"><a href="baysian.html#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(dat<span class="sc">$</span>age, dat<span class="sc">$</span>NDVI)</span>
<span id="cb9-16"><a href="baysian.html#cb9-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-17"><a href="baysian.html#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Overlay the fitted model on the plot</span></span>
<span id="cb9-18"><a href="baysian.html#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(dat<span class="sc">$</span>age, <span class="fu">predict</span>(fit_negexpS, <span class="fu">list</span>(<span class="at">x =</span> dat<span class="sc">$</span>age)), <span class="at">col =</span> <span class="st">&#39;skyblue&#39;</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><img src="BIO3019S-Ecoforecasting_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="baysian.html#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the residuals of the model</span></span>
<span id="cb10-2"><a href="baysian.html#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit_negexpS)</span></code></pre></div>
<p><img src="BIO3019S-Ecoforecasting_files/figure-html/unnamed-chunk-7-2.png" width="672" /></p>
<p><br></p>
<p>Here’s the equation including the additional term:</p>
<p><span class="math display">\[\begin{gather}
  \text{NDVI}_{i,t}=\alpha_i+\gamma_i\Big(1-e^{-\frac{age_{i,t}}{\lambda_i}}\Big)+
      A_i\text{sin}\Big(2\pi\times\text{age}_{i,t}+\Big[\phi+\frac{\pi}{6}(m_{i,t}-1)\Big]\Big)
\end{gather}\]</span></p>
<p>This is a sinusoidal term to capture seasonal fluctuation in NDVI and includes two new parameters:</p>
<ul>
<li><span class="math inline">\(A\)</span> is the amplitude of the seasonality</li>
<li><span class="math inline">\(\phi\)</span> adjusts the timing of the seasonal cycle to account for the month the fire occurred</li>
</ul>
<p><br></p>
<p><br></p>
<p>Version with a data model…</p>
<p><span class="math display">\[\begin{gather}
  \text{NDVI}_{i,t}\sim\mathcal{N}(\mu_{i,t},\frac{1}{\sqrt{\tau}}) \\
  \mu_{i,t}=\alpha_i+\gamma_i\Big(1-e^{-\frac{age_{i,t}}{\lambda_i}}\Big)+
      A_i\text{sin}\Big(2\pi\times\text{age}_{i,t}+\Big[\phi+\frac{\pi}{6}(m_{i,t}-1)\Big]\Big)
\end{gather}\]</span></p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="baysian.html#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ##&#39; Fit logistic model</span></span>
<span id="cb11-2"><a href="baysian.html#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="co"># ##&#39; </span></span>
<span id="cb11-3"><a href="baysian.html#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co"># ##&#39; @param dat  dataframe of day of year (doy), gcc_mean, gcc_std</span></span>
<span id="cb11-4"><a href="baysian.html#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co"># ##&#39; @param par  vector of initial parameter guess</span></span>
<span id="cb11-5"><a href="baysian.html#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="co"># ##&#39; @return  output from numerical optimization</span></span>
<span id="cb11-6"><a href="baysian.html#cb11-6" aria-hidden="true" tabindex="-1"></a>fit.negexp <span class="ot">&lt;-</span> <span class="cf">function</span>(dat, par){</span>
<span id="cb11-7"><a href="baysian.html#cb11-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-8"><a href="baysian.html#cb11-8" aria-hidden="true" tabindex="-1"></a>  <span class="do">## define log likelihood</span></span>
<span id="cb11-9"><a href="baysian.html#cb11-9" aria-hidden="true" tabindex="-1"></a>  lnL.logistic <span class="ot">&lt;-</span> <span class="cf">function</span>(theta,dat){</span>
<span id="cb11-10"><a href="baysian.html#cb11-10" aria-hidden="true" tabindex="-1"></a>    <span class="sc">-</span><span class="fu">sum</span>(<span class="fu">dnorm</span>(dat<span class="sc">$</span>gcc_mean,<span class="fu">pred.logistic</span>(theta,dat<span class="sc">$</span>doy),dat<span class="sc">$</span>gcc_std,<span class="at">log=</span><span class="cn">TRUE</span>),<span class="at">na.rm=</span><span class="cn">TRUE</span>)</span>
<span id="cb11-11"><a href="baysian.html#cb11-11" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb11-12"><a href="baysian.html#cb11-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-13"><a href="baysian.html#cb11-13" aria-hidden="true" tabindex="-1"></a>  <span class="do">## fit by numerical optimization</span></span>
<span id="cb11-14"><a href="baysian.html#cb11-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">optim</span>(par,<span class="at">fn =</span> lnL.logistic,<span class="at">dat=</span>dat)</span>
<span id="cb11-15"><a href="baysian.html#cb11-15" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p><br></p>
</div>
<div id="bayes-theorum" class="section level2" number="7.6">
<h2><span class="header-section-number">7.6</span> Bayes’ theorum</h2>
<div class="figure" style="text-align: center"><span id="fig:frequentistsVSbayesians"></span>
<img src="img/frequentistsVSbayesians.png" alt="'Frequentists vs Bayesians' from [xkcd.com/1132](https://xkcd.com/1132), used under a [CC-BY-NC 2.5 license](https://creativecommons.org/licenses/by-nc/2.5/)." width="50%" />
<p class="caption">
Figure 7.7: ‘Frequentists vs Bayesians’ from <a href="https://xkcd.com/1132">xkcd.com/1132</a>, used under a <a href="https://creativecommons.org/licenses/by-nc/2.5/">CC-BY-NC 2.5 license</a>.
</p>
</div>
<div id="applied-to-parameters-and-data" class="section level3" number="7.6.1">
<h3><span class="header-section-number">7.6.1</span> Applied to parameters and data</h3>
<p>Here I express Bayes’ theorem in the terms data analysts tend to think with, parameters (<span class="math inline">\(\theta\)</span>) and data (<span class="math inline">\(D\)</span>).</p>
<p><span class="math display">\[\begin{align*}
p(\theta|D) &amp; = \frac{p(D|\theta) \; p(\theta)}{p(D)} \;\;
\end{align*}\]</span></p>
<p><span class="math inline">\(\theta^*\)</span> indicates all possible values of <span class="math inline">\(\theta\)</span>.</p>
<p><span class="math display">\[
\underbrace{p(\theta|D)}_\text{posterior} \; = \; \underbrace{p(D|\theta)}_\text{likelihood} \;\; \underbrace{p(\theta)}_\text{prior} \; / \; \underbrace{p(D)}_\text{evidence}.
\]</span></p>
<blockquote>
<p>The “prior,” <span class="math inline">\(p(\theta)\)</span>, is the credibility of the <span class="math inline">\(\theta\)</span> values without the data <span class="math inline">\(D\)</span>. The “posterior,” <span class="math inline">\(p(\theta|D)\)</span>, is the credibility of <span class="math inline">\(\theta\)</span> values with the data <span class="math inline">\(D\)</span> taken into account. The “likelihood,” <span class="math inline">\(p(D|\theta)\)</span>, is the probability that the data could be generated by the model with parameter value <span class="math inline">\(\theta\)</span>. The “evidence” for the model, <span class="math inline">\(p(D)\)</span>, is the overall probability of the data according to the model, determined by averaging across all possible parameter values weighted by the strength of belief in those parameter values. (pp. 106–107)</p>
</blockquote>
</div>
<div id="worked-example" class="section level3" number="7.6.2">
<h3><span class="header-section-number">7.6.2</span> Worked example</h3>
<p>From <a href="https://github.com/kevindavisross/bayesian-reasoning-and-methods/blob/main/02-bayes-rule.Rmd" class="uri">https://github.com/kevindavisross/bayesian-reasoning-and-methods/blob/main/02-bayes-rule.Rmd</a></p>
<p>The mechanism that underpins all of Bayesian statistical analysis is <em>Bayes’ rule</em>, which describes how to update uncertainty in light of new information, evidence, or data.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>

<div class="example">
<span id="exm:bayes-rule1" class="example"><strong>Example 7.1  </strong></span>A <a href="https://www.pewresearch.org/science/2019/03/28/what-americans-know-about-science/">recent survey</a> of American adults asked:
“Based on what you have heard or read, which of the following two statements best describes
the scientific method?”
- 70% selected “The scientific method produces findings meant to be continually tested
and updated over time” (“iterative”).
- 14% selected “The scientific method identifies unchanging core principles and truths” (unchanging).
- 16% were not sure which of the two statements was best.
How does the response to this question change based on education level? Suppose education level is classified as: high school or less (HS), some college but no Bachelor’s degree (college), Bachelor’s degree (Bachelor’s), or postgraduate degree (postgraduate). The education breakdown is
- Among those who agree with “iterative”: 31.3% HS, 27.6% college, 22.9% Bachelor’s, and 18.2% postgraduate.
- Among those who agree with “unchanging”: 38.6% HS, 31.4% college, 19.7% Bachelor’s, and 10.3% postgraduate.
- Among those “not sure”: 57.3% HS, 27.2% college, 9.7% Bachelor’s, and 5.8% postgraduate
</div>
<ol style="list-style-type: decimal">
<li>Use the information to construct an appropriate two-way table.</li>
<li>Overall, what percentage of adults have a postgraduate degree? How is this related to the values 18.2%, 10.3%, and 5.8%?</li>
<li>What percent of those with a postgraduate degree agree that the scientific method is “iterative?” How is this related to the values provided?</li>
</ol>

<div class="solution">
\iffalse{} <span class="solution"><em>Solution. </em></span> to Example <a href="baysian.html#exm:bayes-rule1">7.1</a>
</div>
<ol style="list-style-type: decimal">
<li>Suppose there are 100000 hypothetical American adults. Of these 100000, <span class="math inline">\(100000\times 0.7 = 70000\)</span> agree with the “iterative” statement.
Of the 70000 who agree with the “iterative” statement, <span class="math inline">\(70000\times 0.182 = 12740\)</span> also have a postgraduate degree.
Continue in this way to complete the table below.</li>
<li>Overall 15.11% of adults have a postgraduate degree (15110/100000 in the table).
The overall percentage is a weighted average of the three percentages; 18.2% gets the most weight in the average because the “iterative” statement has the highest percentage of people that agree with it compared to “unchanging” and “not sure.”
<span class="math display">\[
 0.1511 = (0.70)(0.182) + (0.14)(0.103) + (0.16)(0.058)  
 \]</span></li>
<li>Of the 15110 who have a postgraduate degree 12740 agree with the “iterative” statement, and <span class="math inline">\(12740/15110 = 0.843\)</span>. 84.3% of those with a graduate degree agree that the scientific method is “iterative.” The value 0.843 is equal to the product of (1) 0.70, the overall proportion who agree with the “iterative” statement, and (2) 0.182, the proportion of those who agree with the “iterative” statement that have a postgraduate degree; divided by 0.1511, the overall proportion who have a postgraduate degree.
<span class="math display">\[
  0.843 = \frac{0.182 \times 0.70}{0.1511} 
 \]</span></li>
</ol>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">HS</th>
<th align="right">college</th>
<th align="right">Bachelors</th>
<th align="right">postgrad</th>
<th align="right">total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">iterative</td>
<td align="right">21910</td>
<td align="right">19320</td>
<td align="right">16030</td>
<td align="right">12740</td>
<td align="right">70000</td>
</tr>
<tr class="even">
<td align="left">unchanging</td>
<td align="right">5404</td>
<td align="right">4396</td>
<td align="right">2758</td>
<td align="right">1442</td>
<td align="right">14000</td>
</tr>
<tr class="odd">
<td align="left">not sure</td>
<td align="right">9168</td>
<td align="right">4352</td>
<td align="right">1552</td>
<td align="right">928</td>
<td align="right">16000</td>
</tr>
<tr class="even">
<td align="left">total</td>
<td align="right">36482</td>
<td align="right">28068</td>
<td align="right">20340</td>
<td align="right">15110</td>
<td align="right">100000</td>
</tr>
</tbody>
</table>
<p><strong>Bayes’ rule for events</strong> specifies how a prior probability <span class="math inline">\(P(H)\)</span> of event <span class="math inline">\(H\)</span> is updated in response to the evidence <span class="math inline">\(E\)</span> to obtain the posterior probability <span class="math inline">\(P(H|E)\)</span>.
<span class="math display">\[
P(H|E) = \frac{P(E|H)P(H)}{P(E)}
\]</span></p>
<ul>
<li>Event <span class="math inline">\(H\)</span> represents a particular hypothesis<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> (or model or case)</li>
<li>Event <span class="math inline">\(E\)</span> represents observed evidence (or data or information)</li>
<li><span class="math inline">\(P(H)\)</span> is the unconditional or <strong>prior probability</strong> of <span class="math inline">\(H\)</span> (prior to observing <span class="math inline">\(E\)</span>)</li>
<li><span class="math inline">\(P(H|E)\)</span> is the conditional or <strong>posterior probability</strong> of <span class="math inline">\(H\)</span> after observing evidence <span class="math inline">\(E\)</span>.</li>
<li><span class="math inline">\(P(E|H)\)</span> is the <strong>likelihood</strong> of evidence <span class="math inline">\(E\)</span> given hypothesis (or model or case) <span class="math inline">\(H\)</span></li>
</ul>

<div class="example">
<span id="exm:bayes-rule2" class="example"><strong>Example 7.2  </strong></span>Continuing the previous example. Randomly select an American adult.
</div>
<ol style="list-style-type: decimal">
<li>Consider the conditional probability that a randomly selected American adult agrees that the scientific method is “iterative” given that they have a postgraduate degree. Identify the prior probability, hypothesis, evidence, likelihood, and posterior probability, and use Bayes’ rule to compute the posterior probability.</li>
<li>Find the conditional probability that a randomly selected American adult with a postgraduate degree agrees that the scientific method is “unchanging.”</li>
<li>Find the conditional probability that a randomly selected American adult with a postgraduate degree is not sure about which statement is best.</li>
<li>How many times more likely is it for an <em>American adult</em> to have a postgraduate degree and agree with the “iterative” statement than to have a postgraduate degree and agree with the “unchanging” statement?</li>
<li>How many times more likely is it for an <em>American adult with a postgraduate degree</em> to agree with the “iterative” statement than to agree with the “unchanging” statement?</li>
<li>What do you notice about the answers to the two previous parts?</li>
</ol>

<div class="solution">
\iffalse{} <span class="solution"><em>Solution. </em></span> to Example <a href="baysian.html#exm:bayes-rule2">7.2</a>
</div>
<ol style="list-style-type: decimal">
<li>This is essentially the same question as the last part of the previous problem, just with different terminology.
<ul>
<li>The hypothesis is <span class="math inline">\(H_1\)</span>, the event that the randomly selected adult agrees with the “iterative” statement.</li>
<li>The prior probability is <span class="math inline">\(P(H_1) = 0.70\)</span>, the overall or unconditional probability that a randomly selected American adult agrees with the “iterative” statement.</li>
<li>The given “evidence” <span class="math inline">\(E\)</span> is the event that the randomly selected adult has a postgraduate degree. The marginal probability of the evidence is <span class="math inline">\(P(E)=0.1511\)</span>, which can be obtained by the law of total probability as in the previous problem.</li>
<li>The likelihood is <span class="math inline">\(P(E | H_1) = 0.182\)</span>, the conditional probability that the adult has a postgraduate degree (the evidence) given that the adult agrees with the “iterative” statement (the hypothesis).</li>
<li>The posterior probability is <span class="math inline">\(P(H_1 |E)=0.843\)</span>, the conditional probability that a randomly selected American adult agrees that the scientific method is “iterative” given that they have a postgraduate degree. By Bayes rule
<span class="math display">\[
 P(H_1 | E) = \frac{P(E | H_1) P(H_1)}{P(E)} = \frac{0.182 \times 0.70}{0.1511} = 0.843
 \]</span></li>
</ul></li>
<li>Let <span class="math inline">\(H_2\)</span> be the event that the randomly selected adult agrees with the “unchanging” statement; the prior probability is <span class="math inline">\(P(H_2) = 0.14\)</span>. The evidence <span class="math inline">\(E\)</span> is still “postgraduate degree” but now the likelihood of this evidence is <span class="math inline">\(P(E | H_2) = 0.103\)</span> under the “unchanging” hypothesis. The conditional probability that a randomly selected adult with a postgraduate degree agrees that the scientific method is “unchanging” is
<span class="math display">\[
 P(H_2 | E) = \frac{P(E | H_2) P(H_2)}{P(E)} = \frac{0.103 \times 0.14}{0.1511} = 0.095
 \]</span></li>
<li>Let <span class="math inline">\(H_3\)</span> be the event that the randomly selected adult is “not sure”; the prior probability is <span class="math inline">\(P(H_3) = 0.16\)</span>. The evidence <span class="math inline">\(E\)</span> is still “postgraduate degree” but now the likelihood of this evidence is <span class="math inline">\(P(E | H_3) = 0.058\)</span> under the “not sure” hypothesis. The conditional probability that a randomly selected adult with a postgraduate degree is “not sure” is
<span class="math display">\[
 P(H_3 | E) = \frac{P(E | H_3) P(H_3)}{P(E)} = \frac{0.058 \times 0.16}{0.1511} = 0.061
 \]</span></li>
<li>The probability that an <em>American adult</em> has a postgraduate degree and agrees with the “iterative” statement is <span class="math inline">\(P(E \cap H_1) = P(E|H_1)P(H_1) = 0.182\times 0.70 = 0.1274\)</span>. The probability that an <em>American adult</em> has a postgraduate degree and agrees with the “unchanging” statement is <span class="math inline">\(P(E \cap H_2) = P(E|H_2)P(H_2) = 0.103\times 0.14 = 0.01442\)</span>. Since
<span class="math display">\[
   \frac{P(E \cap H_1)}{P(E \cap H_2)} = \frac{0.182\times 0.70}{0.103\times 0.14} = \frac{0.1274}{0.01442} = 8.835
 \]</span>
an <em>American adult</em> is 8.835 times more likely to have a postgraduate degree and agree with the “iterative” statement than to have a postgraduate degree and agree with the “unchanging” statement.</li>
<li>The conditional probability that an <em>American adult with a postgraduate degree</em> agrees with the “iterative” statement is <span class="math inline">\(P(H_1 | E) = P(E|H_1)P(H_1)/P(E) = 0.182\times 0.70/0.1511 = 0.843\)</span>. The conditional probability that an <em>American adult with a postgraduate degree</em> agrees with the “unchanging” statement is <span class="math inline">\(P(H_2|E) = P(E|H_2)P(H_2)/P(E) = 0.103\times 0.14/0.1511 = 0.09543\)</span>. Since
<span class="math display">\[
   \frac{P(H_1 | E)}{P(H_2 | E)} = \frac{0.182\times 0.70/0.1511}{0.103\times 0.14/0.1511} = \frac{0.84315}{0.09543} = 8.835
 \]</span>
An <em>American adult with a postgraduate degree</em> is 8.835 times more likely to agree with the “iterative” statement than to agree with the “unchanging” statement.</li>
<li>The ratios are the same! Conditioning on having a postgraduate degree just “slices” out the Americans who have a postgraduate degree. The ratios are determined by the overall probabilities for Americans. The conditional probabilities, given postgraduate, simply rescale the probabilities for Americans who have a postgraduate degree to add up to 1 (by dividing by 0.1511.)</li>
</ol>
<p>Bayes rule is often used when there are multiple hypotheses or cases. Suppose <span class="math inline">\(H_1,\ldots, H_k\)</span> is a series of distinct hypotheses which together account for all possibilities<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>, and <span class="math inline">\(E\)</span> is any event (evidence). Then Bayes’ rule implies that the posterior probability of any particular hypothesis <span class="math inline">\(H_j\)</span> satisfies
<span class="math display">\[\begin{align*}
P(H_j |E) &amp; = \frac{P(E|H_j)P(H_j)}{P(E)}
\end{align*}\]</span></p>
<p>The marginal probability of the evidence, <span class="math inline">\(P(E)\)</span>, in the denominator can be calculated using the <em>law of total probability</em>
<span class="math display">\[
P(E) = \sum_{i=1}^k P(E|H_i) P(H_i)
\]</span>
The law of total probability says that we can interpret the unconditional probability <span class="math inline">\(P(E)\)</span> as a probability-weighted average of the case-by-case conditional probabilities <span class="math inline">\(P(E|H_i)\)</span> where the weights <span class="math inline">\(P(H_i)\)</span> represent the probability of encountering each case.</p>
<p>Combining Bayes’ rule with the law of total probability,
<span class="math display">\[\begin{align*}
P(H_j |E) &amp; = \frac{P(E|H_j)P(H_j)}{P(E)}\\
&amp; = \frac{P(E|H_j)P(H_j)}{\sum_{i=1}^k P(E|H_i) P(H_i)}\\
&amp; \\
P(H_j |E) &amp; \propto P(E|H_j)P(H_j)
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
P(Having Fun|Cape Town) &amp; \propto P(Cape Town|Being A Fun Place)P(Your Ability To Have Fun)
\end{align*}\]</span></p>
<p>The symbol <span class="math inline">\(\propto\)</span> is read “is proportional to.” The relative <em>ratios</em> of the posterior probabilities of different hypotheses are determined by the product of the prior probabilities and the likelihoods, <span class="math inline">\(P(E|H_j)P(H_j)\)</span>. The marginal probability of the evidence, <span class="math inline">\(P(E)\)</span>, in the denominator simply normalizes the numerators to ensure that the updated probabilities sum to 1 over all the distinct hypotheses.</p>
<p><strong>In short, Bayes’ rule says</strong><a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>
<span class="math display">\[
\textbf{posterior} \propto \textbf{likelihood} \times \textbf{prior}
\]</span></p>
<p>In the previous examples, the prior probabilities for an American adult’s perception of the scientific method are 0.70 for “iterative,” 0.14 for “unchanging,” and 0.16 for “not sure.” After observing that the American has a postgraduate degree, the posterior probabilities for an American adult’s perception of the scientific method become 0.8432 for “iterative,” 0.0954 for “unchanging,” and 0.0614 for “not sure.” The following organizes the calculations in a <strong>Bayes’ table</strong> which illustrates “posterior is proportional to likelihood times prior.”</p>
<table>
<thead>
<tr class="header">
<th align="right">hypothesis</th>
<th align="right">prior</th>
<th align="right">likelihood</th>
<th align="right">product</th>
<th align="right">posterior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">iterative</td>
<td align="right">0.70</td>
<td align="right">0.182</td>
<td align="right">0.1274</td>
<td align="right">0.8432</td>
</tr>
<tr class="even">
<td align="right">unchanging</td>
<td align="right">0.14</td>
<td align="right">0.103</td>
<td align="right">0.0144</td>
<td align="right">0.0954</td>
</tr>
<tr class="odd">
<td align="right">not sure</td>
<td align="right">0.16</td>
<td align="right">0.058</td>
<td align="right">0.0093</td>
<td align="right">0.0614</td>
</tr>
<tr class="even">
<td align="right">sum</td>
<td align="right">1.00</td>
<td align="right">NA</td>
<td align="right">0.1511</td>
<td align="right">1.0000</td>
</tr>
</tbody>
</table>
<p>The likelihood column depends on the evidence, in this case, observing that the American has a postgraduate degree. This column contains the probability of the same event, <span class="math inline">\(E\)</span> = “the American has a postgraduate degree,” under each of the distinct hypotheses:</p>
<ul>
<li><span class="math inline">\(P(E |H_1) = 0.182\)</span>, given the American agrees with the “iterative” statement</li>
<li><span class="math inline">\(P(E |H_2) = 0.103\)</span>, given the American agrees with the “unchanging” statement</li>
<li><span class="math inline">\(P(E |H_3) = 0.058\)</span>, given the American is “not sure”</li>
</ul>
<p>Since each of these probabilities is computed under a different case, these values do not need to add up to anything in particular. The sum of the likelihoods is meaningless, which is why we have listed a sum of “NA” for the likelihood column.</p>
<p>The “product” column contains the product of the values in the prior and likelihood columns. The product of prior and likelihood for “iterative” (0.1274) is 8.835 (0.1274/0.0144) times higher than the product of prior and likelihood for “unchanging” (0.0144).
Therefore, Bayes rule implies that the conditional probability that an American with a postgraduate degree agrees with “iterative” should be 8.835 times higher than the conditional probability that an American with a postgraduate degree agrees with “unchanging.”
Similarly, the conditional probability that an American with a postgraduate degree agrees with “iterative” should be <span class="math inline">\(0.1274 / 0.0093 = 13.73\)</span> times higher than the conditional probability that an American with a postgraduate degree is “not sure,”
and the conditional probability that an American with a postgraduate degree agrees with “unchanging” should be <span class="math inline">\(0.0144 / 0.0093 = 1.55\)</span> times higher than the conditional probability that an American with a postgraduate degree is “not sure.”
The last column just translates these relative relationships into probabilities that sum to 1.</p>
<p>The sum of the “product” column is <span class="math inline">\(P(E)\)</span>, the marginal probability of the evidence. The sum of the product column represents the result of the law of total probability calculation. However, for the purposes of determining the posterior probabilities, it isn’t really important what <span class="math inline">\(P(E)\)</span> is. Rather, it is the <em>ratio</em> of the values in the “product” column that determine the posterior probabilities. <span class="math inline">\(P(E)\)</span> is whatever it needs to be to ensure that the posterior probabilities sum to 1 while maintaining the proper ratios.</p>
<p>The process of conditioning can be thought of as <strong>“slicing and renormalizing.”</strong></p>
<ul>
<li>Extract the “slice” corresponding to the event being conditioned on (and discard the rest). For example, a slice might correspond to a particular row or column of a two-way table.<br />
</li>
<li>“Renormalize” the values in the slice so that corresponding probabilities add up to 1.</li>
</ul>
<p>We will see that the “slicing and renormalizing” interpretation also applies when dealing with conditional distributions of random variables, and corresponding plots. Slicing determines the <em>shape</em>; renormalizing determines the <em>scale</em>. Slicing determines relative probabilities; renormalizing just makes sure they “add up” to 1 while maintaining the proper ratios.</p>

<div class="example">
<span id="exm:unnamed-chunk-13" class="example"><strong>Example 7.3  </strong></span>Now suppose we want to compute the posterior probabilities for an American adult’s perception of the scientific method given that the randomly selected American adult has a Bachelor’s degree (instead of a postgraduate degree).
</div>
<ol style="list-style-type: decimal">
<li>Before computing, make an educated guess for the posterior probabilities. In particular, will the changes from prior to posterior be more or less extreme given the American has a Bachelor’s degree than when given the American has a postgraduate degree? Why?</li>
<li>Construct a Bayes table and compute the posterior probabilities. Compare to the posterior probabilities given postgraduate degree from the previous examples.</li>
</ol>
<p>Like the scientific method, Bayesian analysis is often an iterative process.</p>
<div class="figure" style="text-align: center"><span id="fig:seashell"></span>
<img src="img/seashell.png" alt="'Seashell' from [xkcd.com/1236](https://xkcd.com/1236), used under a [CC-BY-NC 2.5 license](https://creativecommons.org/licenses/by-nc/2.5/)." width="50%" />
<p class="caption">
Figure 7.8: ‘Seashell’ from <a href="https://xkcd.com/1236">xkcd.com/1236</a>, used under a <a href="https://creativecommons.org/licenses/by-nc/2.5/">CC-BY-NC 2.5 license</a>.
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:usingbayes"></span>
<img src="img/modifiedbayes.png" alt="'Modified Baye's Theorem' from [xkcd.com/2059](https://xkcd.com/2059), used under a [CC-BY-NC 2.5 license](https://creativecommons.org/licenses/by-nc/2.5/)." width="50%" />
<p class="caption">
Figure 7.9: ‘Modified Baye’s Theorem’ from <a href="https://xkcd.com/2059">xkcd.com/2059</a>, used under a <a href="https://creativecommons.org/licenses/by-nc/2.5/">CC-BY-NC 2.5 license</a>.
</p>
</div>
<p><span class="math inline">\(p\)</span> is unknown but expected to be around 1/3. Standard error will be approximated</p>
<p><span class="math display">\[
SE = \sqrt(\frac{p(1-p)}{n}) \approx \sqrt{\frac{1/3 (1 - 1/3)} {300}} = 0.027
\]</span></p>
<p>You can also use math in footnotes like this<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a>.</p>
<p>We will approximate standard error to 0.027<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a></p>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>Here we’ll cover Bayes’ rule for events, becaue it’s simpler to explain. Bayes’ rule comes into it’s own when applied to distributions of random variables, which I’ll expand to later. The ideas are analogous.<a href="baysian.html#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>We’re using “hypothesis” in the sense of a general scientific hypothesis, not necessarily a statistical null or alternative hypothesis.<a href="baysian.html#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>More formally, <span class="math inline">\(H_1,\ldots, H_k\)</span> is a <em>partition</em> which satisfies <span class="math inline">\(P\left(\cup_{i=1}^k H_i\right)=1\)</span> and <span class="math inline">\(H_1, \ldots, H_k\)</span> are disjoint — <span class="math inline">\(H_i\cap H_j=\emptyset , i\neq j\)</span>.<a href="baysian.html#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>“Posterior is proportional to likelihood times prior” summarizes the whole course in a single sentence.<a href="baysian.html#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>where we mention <span class="math inline">\(p = \frac{a}{b}\)</span><a href="baysian.html#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p><span class="math inline">\(p\)</span> is unknown but expected to be around 1/3. Standard error will be approximated</p>
<p><span class="math display">\[
SE = \sqrt(\frac{p(1-p)}{n}) \approx \sqrt{\frac{1/3 (1 - 1/3)} {300}} = 0.027
\]</span><a href="baysian.html#fnref6" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="practical.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/jslingsby/BIO3019S_Ecoforecasting/edit/master/07-going_bayesian.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["BIO3019S Ecoforecasting.pdf", "BIO3019S Ecoforecasting.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

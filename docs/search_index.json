[["index.html", "A Minimal Introduction to Ecological Forecasting and Reproducible Research 1 Overview 1.1 General 1.2 Module and Project details 1.3 Acknowledgements and further reading:", " A Minimal Introduction to Ecological Forecasting and Reproducible Research Jasper Slingsby 2021-09-19 1 Overview This module is a minimal introduction to Ecological Forecasting and Reproducible Research for the 3rd year undergraduate Biological Sciences BIO3019S class at the University of Cape Town. 1.1 General I provide a very brief introduction to Ecological Forecasting. We only have a two weeks, so this really is a minimalist introduction. I’ll focus on providing a broad overview of the general framework and motivation for ecological forecasting, but won’t have time to delve into the more gory theoretical and statistical details. I mostly use Ecological Forecasting as a framework to highlight various themes that are increasingly important for quantitative biologists - understanding how we inform or make decisions; managing data; working reproducibly; propagating, understanding and reducing uncertainty. The core outcomes/concepts I hope you’ll come away with: To be able to situate the role of models and the importance of forecasting in science and ecological decision making Familiarity with the concepts and understand the need for Open, Reproducible Science Familiarity with The Data Life Cycle Familiarity with Open Science tools 1.2 Module and Project details Lectures Lectures will be held live, but online 12:00 - 12:45 from the 30th September to the 13th October. The zoom link (see Vula) should stay the same for all lecture sessions. Practicals There is only one practical for this module, 2-5PM on Tuesday the 5th October. Details to follow. 1.3 Acknowledgements and further reading: Many of the following resources were instrumental in me pulling this material together and are worth spending time exploring ecoforecast.org Dietze, Michael C. 2017. Ecological Forecasting. Princeton University Press. https://doi.org/10.2307/j.ctvc7796h. Dietze, Michael C. et al. 2018. “Iterative near-Term Ecological Forecasting: Needs, Opportunities, and Challenges.” Proceedings of the National Academy of Sciences of the United States of America 115 (7): 1424–32. https://doi.org/10.1073/pnas.1710231115. All code, images, etc can be found here. I have only used images and other materials that were made available online under a non-restrictive license (Creative Commons, etc) of for which I have express permission, and have attributed my sources. Content without attribution is my own and shared under the license below. If there are any errors or any content you find concerning with regard to licensing, or that you find offensive, please contact me. Any feedback, positive or negative, is welcome! This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License. "],["quiz.html", "2 How do we make decisions?", " 2 How do we make decisions? What factors do you consider when making a decision? "],["models.html", "3 Models and decision making 3.1 The basics of making a decision 3.2 Getting quantitative 3.3 Iterative decision-making and the scientific method 3.4 The importance of prediction in ecology 3.5 Iterative ecological forecasting 3.6 Delete from here once done… - How to reference", " 3 Models and decision making 3.1 The basics of making a decision Informing decisions typically requires knowing (or guessing at) something about the future. To this end, once a problem and the need to make a decision have been identified, the factors we consider when making that decision usually include: Evidence Experience Expectation Uncertainty The relationship between these can be represented like so: Figure 3.1: The factors considered when making a decision. Your decision is typically based on your expectation of the outcome. This expectation is based on existing evidence and/or previous experience. Uncertainty is a part of each step. There are a number of reasons why the existing evidence or previous experience may be imperfect for the decision at hand, leading to uncertainty in the expectations. There may also be uncertainty in the way in which you use the evidence and experience to develop your expectation. We will come back to these sources of uncertainty later in the module, but needless to say, quantifying and understanding the uncertainty is crucial in any decision. If uncertainty is high your expectation may be no better than random, and thus useless for informing your decision. Quantifying uncertainty properly helps us circumvent two evils which could mislead decision makers: being falsely overconfident in our predictions (potentially leading to a “wrong” decision), or being falsely uncertain in our predictions (which would encourage overly conservative decisions/actions which may be wasteful or less effective). Lastly, ignoring or quantifying uncertainty incorrectly can lead to bias predictions. 3.2 Getting quantitative The nice thing about the framework above is that it is similar whether you are approaching the decision qualitatively or quantitatively (i.e. using models and data to inform your decision). Figure 3.2: Using models and data when making a decision. Following a quantitative approach the “evidence” is typically empirical data, which can be fed into a model to make forecasts that can help inform the decision. The “experience” are the current state of knowledge and your prior belief, which you use to specify the type and structure of your model (or ensemble of models) and the scenario(s) you want to evaluate. The “experience” can also help you evaluate the assumptions of your model(s), and if you are using a Bayesian model, can be included directly in the model when specifying priors (more on this later in the module). 3.3 Iterative decision-making and the scientific method Few decisions in natural resource management are once-off, and most are made repeatedly at some time-step (e.g. daily, monthly, seasonally, annually, decadally, etc). Should you burn, cull, harvest, restore, etc? While one should always evaluate the outcome of your decision, this is especially important when the decision will need to be repeated, so that you can learn from experience. Figure 3.3: Iterative decision making. When using quantitative forecasts this can be done by collecting new data and updating your prior knowledge by evaluating the outcomes of the decision against the original model forecasts. This can tell you whether your forecast was any good and whether you need to refine or replace your model, consider additional scenarios or inputs, etc. We’ll discuss doing this quantitatively in section XXX, by fusing your new data and knowledge into a new forecast. It’s worth highlighting the similarity between the iterative decision making cycle I’ve outlined in figure 3.3 and the scientific method, i.e.: Observation &gt; Hypothesis &gt; Experiment &gt; Analyze &gt; Interpret &gt; Report &gt; (Repeat) Figure 3.4: The Scientific Method overlain on iterative decision making. So a focus on iterative decision-making facilitates iterative learning (i.e. scientific progress). 3.4 The importance of prediction in ecology “prediction is the only way to demonstrate scientific understanding” (Houlahan et al. 2017) While this view may be slightly overstated, it is a very good point. If we cannot make reasonably good predictions, we’re missing something. Unfortunately, prediction has not been a central focus in ecology, impeding progress in the improvement of our ecological understanding. To make predictions we need models, and models provide structured summaries of our current ecological understanding (conceptual or quantitative, but preferably quantitative, because these are easier to compare). Without making predictions and comparing the skill of new models to old ones, we can’t track if we are making progress! Figure 3.5: Prediction… from xkcd.com/2370, used under a CC-BY-NC 2.5 license. In ecology we mostly test qualitative, imprecise hypotheses: “Does X have an effect on Y?” rather than “What is the relationship between X and Y?” or better yet “What value would we expect Y to be, given a particular value of X?”. Without testing precise hypotheses and using the results to make testable predictions we don’t know if our findings are generalizable beyond the specific data set we collected. If our results are not generalizable, then we’re not really making progress towards a better understanding of ecology. A key point here is that the predictions must be testable! We do use a lot of models in ecology, and even use them to make predictions (e.g. species distribution models (SDMs), dynamic vegetation models (DVMs), etc), but these predictions are typically 50+ years into the future, which is way to long to wait to see if our predictions were reasonable or useful. A quick aside on model validation vs testing predictions: Testing predictions with new data collected after you’ve made your predictions is the most robust way to validate a model, but you usually want to do some form of validation before you make your final predictionst o make sure the model is working reasonably well. For this we most commonly do some form of cross-validation, whereby we split the data into a “training” subset (that we use for fitting (or training) the model) and a “test” subset (that we try to predict). If your model is no good at predicting your test data, there’s probably no point in making predictions into the future… 3.5 Iterative ecological forecasting The recent growth in interest in iterative ecological forecasting seeks to not only make prediction a central focus in ecology, but to do so on a time scale that is both useful for decision makers and allows us to learn from testing our predictions (days to decades). This is a great initiative, but as we will see it poses a number of major challenges and requires a big improvement in quantitative skills in biology (hence this course…). Figure 3.6: The iterative ecological forecasting cycle in the context of the scientific method and the adaptive management and monitoring cycles (Dietze et al. 2018). The iterative ecological forecasting cycle is tightly aligned to the scientific method cycle: Hypotheses (A) are embedded in models (B). The models integrate over uncertainties in initial conditions (IC), inputs, and parameters to make probabilistic forecasts (the purple distributions, Fx, in step C), sometimes for multiple alternative scenarios. New observations are then compared with these predictions (D) to update estimates of the current state of the system (Analysis) and assess model performance (E), allowing for the selection among alternative model hypotheses (Test and Refine). The iterative forecasting cycle also feeds into adaptive management and monitoring: In Adaptive Management and decision analysis, alternative decision scenarios are generated (2) based on an assessment of a problem (1). These decision scenarios are typically used to define the scenarios (or boundary conditions) for which models are run (“Scenarios” arrow), but can also feed into scientific hypotheses (not shown). Forecasts (Fx) are key in assessing the trade-offs and relative merits between alternative decision options (3). The decision(s) taken (4) determine the monitoring requirements (5), which allow us to evaluate the outcomes and reassess the problem (1), and start the adaptive management cycle again. Note that the iterative forecast cycle is also useful for adaptive management in that the analysis and partitioning of forecast uncertainties (from step C) can provide further guidance on what and how to monitor, so as to optimize the reduction in model uncertainties. This represents Adaptive Monitoring (dashed line) and is a cycle of itself (Lindenmayer and Likens 2009), but is largely subsumed by the other cycles here so we won’t go into it any further here. Thus the iterative cycles of science, forecasting, management and monitoring are tightly intertwined and can interact continuously. What isn’t clear from Figure 3.6 is that all of this needs to be founded on a highly efficient informatics pipeline that is robust and rapidly updateable - i.e. follows Reproducible Research principles. Adding this link helps to highlight what I like to think of as “The Olympian Challenge of data-driven ecological decision making”. Figure 3.7: The Olympian Challenge of data-driven ecological decision making. Working reproducibly requires learning a lot of skills and can take a lot of effort, but is well worth it in the long run - for you as an individual, and for science in general. This is going to be the focus of the next section (and the practical), before we continue with other components of the ecological forecasting cycle. 3.6 Delete from here once done… - How to reference You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 3. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter ??. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 3.8: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 3.8. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 3.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 3.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2021) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). References "],["reproducibility.html", "4 Reproducible research 4.1 The Reproducibility Crisis 4.2 Replication and the Reproducibility Spectrum 4.3 Why work reproducibly? 4.4 Scientific Workflows 4.5 Coding and code management 4.6 Computing and software", " 4 Reproducible research 4.1 The Reproducibility Crisis “Replication is the ultimate standard by which scientific claims are judged.” (Peng 2011) Replication is one of the fundamental tenets of science and if the results of a study or experiment cannot by replicated by an independent set of investigators then whatever scientific claims were made should be treated with caution! At best, it suggests that evidence for the claim is weak or mixed, or specific to particular ecosystems or other circumstances and cannot be generalized. At worst, there was error (or even dishonesty) in the original study and the claims were plainly false. In other words, published research should be robust enough and the methods described in enough detail that anyone else should be able to repeat the study (using the publication only) and find similar results. Sadly, this is rarely the case!!! Figure 4.1: ‘Is there a reproducibility* crisis?’ Results from a survey of &gt;1500 top scientists (Baker 2016; Penny 2016). *Note that they did not discern between reproducibility and replicability, and that the terms are often used interchangeably. We have a problem… Since we’re failing the gentleman’s agreement1 that we’ll describe our methods in enough detail that anyone else should be able to repeat the study (using the publication only) and find similar results, modern scientists are trying to formalize the process in the form of Reproducible Research. Reproducible research makes use of modern software tools to share data, code and other resources required to allow others to reproduce the same result as the original study, thus making all analyses open and transparent. As you will learn in this module, working reproducibly is not just a requirement for using quantitative approaches in iterative decision-making, it is central to scientific progress!!! While full replication is a huge challenge (and sometimes impossible) to achieve, it is something all scientists should be working towards. 4.2 Replication and the Reproducibility Spectrum Understandably, some studies may not be entirely replicable purely due to the nature of the data or phenomenon (e.g. rare phenomena, long term records, loss of species or ecosystems, or very expensive once-off science projects like space missions). In these cases the “gold standard” of full replication (from new data collection to results) cannot be achieved, and we have to settle for a lower rung on the reproducibility spectrum (Figure 4.2). Figure 4.2: The Reproducibility Spectrum (Peng 2011). Reproducibility falls short of full replication because it focuses on reproducing the same result from the same data set, rather than analyzing independently collected data. While this may seem trivial, you’d be surprised at how few studies are even reproducible, let alone replicable. 4.3 Why work reproducibly? Figure 4.3: Let’s start being more specific about our miracles… Cartoon © Sidney Harris. Used with permission ScienceCartoonsPlus.com In addition to basic scientific rigour, working reproducibly is hugely valuable, because: (Adapted from “Five selfish reasons to work reproducibly” (Markowetz 2015)) It helps us avoid mistakes and/or track down errors in analyses This is what highlighted the importance of working reproducibly for me. In 2017 I published the first evidence of observed climate change impacts on biodiversity in the Fynbos Biome (Slingsby et al. 2017). The analyses were quite complicated, and when working on the revisions I found an error in my R code. Fortunately, it didn’t change the results qualitatively, but it made me realize how easy it is to make a mistake and potentially put the wrong message out there! This encouraged me to make all data and R code from the paper available, so that anyone is free to check my data and analyses and let me (and/or the world) know if they find any errors. It makes it easier to write papers e.g. Dynamic documents like RMarkdown or Jupyter Notebooks update automatically when you change your analyses, so you don’t have to copy/paste or save/insert all tables and figures (or worry about whether you included the latest versions. It helps the review process Often issues picked at by reviewers are matters of clarity/confusion. Sharing your data and analyses allows them to see exactly what you did, not just what you said you did, allowing them to identify the problem and make constructive suggestions. It’s also handy to be able to respond to a reviewer’s comment with something like “That’s a great suggestion, but not really in line with the objectives of the study. We have chosen not to include the suggested analysis, but do provide all data and code so that interested readers can explore this for themselves.” (Feel free to copy and paste - CCO 1.0) It enables continuity of the research When people leave a project (e.g. students/postdocs), or you forget what you did X days/weeks/months/years ago, it can be a serious setback for a project and make it difficult for you or a new student to pick up where things left off. If the data and workflow are well curated and documented this problem is avoided. Trust me, this is a very common problem!!! I have many papers that I (or my students) never published and may never go back to, because I know it’ll take me a few days or weeks to understand the datasets and analyses again… This is obviously incredibly important for long-term projects!!! It helps to build your reputation Working reproducibly makes it clear you’re an honest, open, careful and transparent researcher, and should errors be found in your work you’re unlikely to be accused of dishonesty (e.g. see my paper example under point 1 - although no one has told me of any errors yet…). When others reuse your data, code, etc you’re likely to get credit for it - either just informally, or formally through citations or acknowledgements (depending on the licensing conditions you specify - see “Preserve” in the Data Life Cycle). And some less selfish reasons (and relevant for ecoforecasting): It allows you (or others) to rapidly build on previous findings and analyses It allows easy comparison of new analytical approaches to older ones It makes it easy to repeat the same analyses when new data are collected or added And one more selfish reason (but don’t tell anyone I said this): Should you decide to leave biology, reproducible research skills are highly sought after in other careers like data science etc… 4.4 Scientific Workflows Figure 4.4: ‘Data Pipeline’ from xkcd.com/2054, used under a CC-BY-NC 2.5 license. Working reproducibly requires careful planning and documentation of each step in your scientific workflow from planning your data collection to sharing your results. This entails a number of overlapping/intertwined components, namely: Coding and code management (data analysis) Computing and software Data management For the rest of this section we’ll work through these components and some of the tools that help you achieve this. Data management is big enough to warrant a separate chapter. 4.5 Coding and code management Why write code Some rules of coding Version control 4.6 Computing and software Why use open source software Containers References "],["data-management.html", "5 Data Management 5.1 Why do you need to manage your data? 5.2 The Data Life Cycle 5.3 Data and decisions", " 5 Data Management 5.1 Why do you need to manage your data? Data management is often the last thing on a scientists mind when doing a new study - “I have a cool idea, and I’m going to test it!” You don’t want to “waste” time planning how you’re going to manage your data and implementing that plan… Unfortunately, this never ends well and really is a realm where “haste makes waste.” Figure 5.1: The ‘Data Decay Curve’ (Michener et al. 1997) Bad data management leads to data loss… (The “data decay curve” (Michener et al. 1997)) Your future self will hate you if you lose it before you’re finished with it - Eek!!! This is less likely in the world of Dropbox, Google Drive, iCloud etc, but I know people who had to repeat their PhD’s because they lost their data or it was on a laptop that was stolen… Data has value beyond your current project: to yourself for reuse in future projects, collaborations, etc (i.e. publications and citations), for others for follow-up studies, or combining multiple datasets for meta-analyses or synthesis etc for science on general (especially long-term ecology in a time of global change) We’ve covered this before, but it is also key for transparency and accountability. Data collection is expensive, and is often paid for with taxpayers’ money. You owe it to them to make sure that science gets the most out of that data in the long term. Lastly, good planning and data management can help iron out issues like intellectual property, permissions for ethics, collection permits, etc, in addition to outlining expectations for who will be authors on the paper(s), responsible for managing different aspects of the data etc up front. If you don’t establish these permissions and ground rules early they can result in data loss, not being able to publish the study, damage careers, and/or damage relationships in collaborations (including student-supervisor), etc. To avoid data (and relationship) decay, and to reap the benefits of good data management, it is important to consider the full Data Life Cycle. 5.2 The Data Life Cycle Figure 5.2: The Data Life Cycle, adapted from https://www.dataone.org/ Note that there are quite a few different versions of the data life cycle out there. This is the most comprehensive one I know of, and covers all the steps relevant to a range of different kinds of research projects. A full description of this data life cycle and related ecoinformatics issues can be found in (Michener and Jones 2012). Not all projects need to do all steps, nor will they necessarily follow the order here, but it is worth being aware of and considering all steps. For example, often the first thing you do when you have a new hypothesis is search around for any existing data that could be used to test it without having to spend money and time collecting new data (i.e. skip to step 6). In this case I would argue that you should still do step 1 (Plan), and you’d want to do some checking to assure the quality of the data (step 3), but you can certainly skip steps 2, 4 and 5. A meta-analysis or synthesis paper would probably do the same. On the other hand, if you’re collecting new data you would do steps 1 to 5 and possibly skip 6 and 7, although in my experience few studies do not reuse existing data (e.g. weather or various GIS data to put your new samples in context). 5.2.1 Plan Good data management begins with planning. In this step you essentially outline the plan for every step of the cycle in as much detail as possible. Usually this is done by constructing a document or Data Management Plan (DMP). While developing DMPs can seem tedious, they are essential for the reasons I gave above, and because most funders and universities now require them. Fortunately, there are a number of online data management planning tools that make it easy by providing templates and prompts to ensure that you cover all the bases, like the Digital Curation Centre’s DMPOnline and UCT’s DMP Tool. Figure 5.3: Screenshot of UCT’s Data Management Planning Tool’s Data Management Checklist. A key thing to bear in mind is that a DMP is a living document and should be regularly revised during the life of a project, especially when big changes happen - e.g. new team members, new funding, new direction, change of institution, etc. I typically develop one overarching DMP for an umbrella project (e.g. a particular grant), but then add specifics for subprojects (e.g. separate student projects etc). 5.2.2 Collect and Assure There are many, many different kinds of data that can be collected in a vast number of ways! Figure 5.4: Springer Nature Infographic illustrating the vast range of research data types. While “Collect” and “Assure” are different steps in the life cycle, I advocate that it is foolish to collect data without doing quality assurance and quality control (QA/QC) as you go, irrespective of how you are collecting the data. For example: automated logging instruments (weather stations, cameras, acoustic recorders) need to be checked that they’re logging properly, are calibrated/focused, are reporting sensible values, etc if you’re filling in data sheets, you need to check that all fields have been completed, that there are no obvious errors and that any numbers or other values look realistic. In fact, if you’re using handwritten data sheets it’s best to capture them as soon as possible (i.e. that evening), because that helps you spot errors and omissions, you have a better chance of deciphering bad handwriting or cryptic notes, and you can plot any values to see if there are suspicious outliers (e.g. because someone wrote down a measurement in centimetres when they were meant to use metres). When transcribing or capturing data into a spreadsheet or database it is often best to use data validation tricks like drop-down menus, conditional formatting, restricted value ranges etc to avoid spelling mistakes and highlight data entries that are outside the expected range of the data field. It may seem like a lot of effort to set this up, but it’ll save you a lot of time and pain in the long run!!! Increasingly, I’ve started moving towards capturing data directly into a spreadsheet on a tablet, or better yet, into an App on my phone. There are a number of “no code” app builders these days like AppSheet that inserts data directly into Google Sheets and saves photos to your Google Drive. Figure 5.5: An example data collection app I built in AppSheet that allows you to log GPS coordinates, take photos, record various fields, etc. 5.2.3 Describe There are few things worse than having a spreadsheet of what you think is the data you need, but you don’t know what the column names mean, how variables were measured, what units they’re reported in, etc… - Especially when you were the one who collected and captured the data!!! This descriptive data about the data is called metadata and is essential for making the data reusable, but is also useful for many other purposes like making the data findable (e.g. using keyword searches). In fact, metadata makes up the majority of what are called the FAIR data principles (Wilkinson et al. 2016), which largely focus on this and the next few steps of the Data Life Cycle. I’m not going to dwell on them other than to say that they are a key component of making your work reproducible, and that like reproducibility, practicing FAIR data principles is a spectrum. Figure 5.6: The FAIR data principles ErrantScience.com. Some key kinds of metadata: the study context why the data were generated who funded, created, collected, assured, managed and owns the data (not always the same person) contact details for the above when and where the data were collected where the data are stored the data format what is the file format what softwares were used (and what version) the data content what was measured how it was measured what units was it reported in what QA/QC has been applied is it raw data or a derived data product (e.g. spatially interpolated climate layers) if derived, how it was analyzed Metadata standards and interoperability Many data user communities have developed particular metadata standards or schemas in an attempt to enable the best possible description and interoperability of a data type for their needs. They are typically human and machine-readable data, so that the metadata records can also be read by machines, facilitating storing and querying multiple datasets in a common database (or across databases). Figure 5.7: How standards proliferate… from xkcd.com/927, used under a CC-BY-NC 2.5 license. Using common metadata schemas has many advantages in that they make data sharing easier, they allow you to search and integrate data across datasets, and they simplify metadata capture (i.e. having a list of required fields makes it easier to not forget any). There are many standards, but perhaps the most common ones you’ll encounter in biological sciences (other than geospatial metadata standards) are DarwinCore and Ecological Metadata Language (EML). 5.2.4 Preserve There are two major components to preserving your data: Back your data up now!!! (and repeat regularly) Losing your data can be incredibly inconvenient!!! A good friend of mine lost all of his PhD data twice. It took him 7 years to complete the degree… Beyond inconvenience, losing data can be incredibly expensive! Doing 4 extra years to get your PhD is expensive at a personal level, but if the data are part of a big project it can rapidly add up to millions - like How Toy Story 2 Almost Got Deleted. PRO TIP: Storing data on the cloud is not enough! You could easily delete that single version of all your data! You may also lose access when you change institution etc. E.g. What happens to your UCT MS OneDrive and Google Drive content when you graduate and ICTS close your email account? Long-term preservation and publication This involves the deposition of your data (and metadata!) in a data repository where it can be managed and curated over the long term. This is increasingly a requirement of funders and publishers (i.e. journals). Many journals allow you (or require you) to submit and publish your data with them as supplementary material. Unfortunately, many journals differ in how they curate the data and whether they are available open access. I prefer to publish my data in an online open access repository where you can get a permanent Digital Object Identifier (DOI) that you can link to from your paper. Another consideration, if you are keen for people to reuse your data (which if you are not you will fail this course by default) is where people are most likely to look for your data (i.e. making your data “Findable/Discoverable.” There are many “bespoke” data repositories for different kinds of data, e.g. Global databases: GenBank - for molecular data TRY - for plant traits Dryad - for generalist biological and environmental research South African databases: SANBI - for most kinds of South African biodiversity data SAEON - for South African environmental data (e.g. hydrology, meteorology, etc) and biodiversity data that don’t fit SANBI’s databases If none of these suit your data, there are also “generalist” data repositories that accept almost any kind of data, like: FigShare Zenodo UCT’s ZivaHub (which is built on and searchable through FigShare) I haven’t discussed physical samples at all. These are obviously a huge (if not bigger) challenge too, although there are some obvious homes for common biological data, like herbaria for plant collections and museums for animal specimens. 5.2.5 Discover This is perhaps the main point of the Data Life Cycle and FAIR data principles - to make data findable so that it can be reused. The biggest challenge to discovering data is that so many datasets are not online and are in the “filing cabinet in a bath in the basement under a leaking pipe” as in Figure 5.6. If you preserve and publish them in an online data repository, this overcomes the biggest hurdle. The next biggest challenge is that there is so much online that finding what you need can be quite challenging (like looking for a needle in a haystack…). This is where choosing the right portal can be important. It is also what metadata standards are aimed at - allowing interoperable searches for specific data types across multiple repositories. A final consideration is whether you have permission to use the data. You can often find out about the existence of a dataset, either online or in a paper, but the data aren’t made freely available. This is where licensing comes into play. Most data repositories require you to publish the data under a license. There are many options depending on the kind of data and what restrictions you want to put on its use. I’m not going to go into the gory details, but Creative Commons have created an extensible system of generic licenses that are easy to interpret and cover most situations. I say extensible because the licenses are made up of a string of components that can be layered over each other. For example: CCO - means it is Open - i.e. there are no restrictions on use and it is in the public domain CC BY - means by attribution - you can use the data for any purpose, but only if you indicate attribution of the data to the source or owner of the data CC BY-SA - means by attribution + share alike - i.e. you have to indicate attribution and share your derived product under the same license CC BY-ND - means by attribution + no derivatives - i.e. you have to indicate attribution, but cannot use it to make a derived product. This is often used for images - allowing you to show the image, but not to alter it. CC BY-NC - means by attribution + non-commercial - you have to indicate attribution, but cannot use it for commercial purposes (i.e. you can’t sell derived products) CC BY-NC-SA - by attribution + non-commercial + share alike CC BY-NC-ND - by attribution + non-commercial + no derivatives 5.2.6 Integrate 5.2.7 Analyze 5.3 Data and decisions latency uncertainty References "],["pracstuff.html", "6 Prac", " 6 Prac "],["baysian.html", "7 Going Bayesian 7.1 Maximum likelihood", " 7 Going Bayesian While ecological forecasting and decision support can be done with traditional statistics (often termed “frequentist statistics”) it is generally much easier to do in a Bayesian statistical framework, because: It treats all terms as probability distributions, making it easier to quantify, propagate and partition uncertainties throughout the analysis (more on this later) Bayesian analyses are inherently iterative, making it easier to update predictions as new data become available They are highly flexible, allowing one to build relatively complex models with varied data sources (and/or of varying quality) They are typically focused on estimating what properties are (i.e. the actual value of a particular parameter) and not just establishing what they are not (i.e. testing for significant difference, as is usually the focus in frequentist statistics) - although there are frequentist approaches for doing this too In this section I aim to provide a brief and soft introduction to Bayesian statistics and use an example from my own research to highlight the principles and benefits. We’ll also use this example in the pair-coding practical. 7.1 Maximum likelihood Before I can introduce Bayes, there are a few basic building blocks we need to establish first. The main one is the concept of likelihood and the estimation of maximum likelihood, since this is a major component of Bayes’ Theorum. 7.1.1 Probability vs Likelihood While probability and likelihood may seem like similar concepts, the distinction between them is fundamentally important in statistics. Probability relates to possible results given a hypothesis. e.g. if you have an unbias coin (the hypothesis), the probability of a coin toss landing heads up (the result) is 0.5. Likelihood relates to hypotheses, given the results (i.e. data). e.g. say we’ve performed 1000 tosses and the coin landed heads up 700 times (the data), it is less likely that the hypothesis of an unbias coin is true relative to a hypothesis that the coin favours heads. This may seem like a subtle and potentially arbitrary distinction, but likelihood comes into it’s own when comparing multiple hypotheses (usually specified as models). Add Postfire example demonstrate p-value by fitting lm to NDVI demonstrate fitting with MLE fit negative exponential and compare likelihoods \\(p\\) is unknown but expected to be around 1/3. Standard error will be approximated \\[ SE = \\sqrt(\\frac{p(1-p)}{n}) \\approx \\sqrt{\\frac{1/3 (1 - 1/3)} {300}} = 0.027 \\] You can also use math in footnotes like this2. We will approximate standard error to 0.0273 where we mention \\(p = \\frac{a}{b}\\)↩︎ \\(p\\) is unknown but expected to be around 1/3. Standard error will be approximated \\[ SE = \\sqrt(\\frac{p(1-p)}{n}) \\approx \\sqrt{\\frac{1/3 (1 - 1/3)} {300}} = 0.027 \\]↩︎ "],["uncertainty.html", "8 Uncertainty", " 8 Uncertainty We have finished a nice book. "],["references.html", "References", " References "]]

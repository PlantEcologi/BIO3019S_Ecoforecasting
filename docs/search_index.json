[["forecasting.html", "4 Forecasting in ecology 4.1 The importance of prediction in ecology 4.2 Iterative ecological forecasting", " 4 Forecasting in ecology 4.1 The importance of prediction in ecology “prediction is the only way to demonstrate scientific understanding” (Houlahan et al. 2017) While this view may be slightly overstated, it is a very good point. If we cannot make reasonably good predictions, we’re still missing something. Unfortunately, prediction has not been a central focus in ecology, impeding progress in the improvement of our ecological understanding. To make predictions we need models, and models provide structured summaries of our current ecological understanding (conceptual or quantitative, but preferably quantitative, because these are easier to compare). Without making predictions and comparing the skill of new models to old ones, we can’t track if we are making progress! In ecology we mostly test qualitative, imprecise hypotheses ( “Does X have an effect on Y?” rather than “What is the relationship between X and Y?” or better yet “What value would we expect Y to be, given a particular value or X?”). Without testing precise hypotheses and using the results to make testable predictions we don’t know if our findings are generalizable beyond the specific data set we collected? If our results are not generalizable, then we’re not really making progress towards a better understanding of ecology. A key point here is that the predictions must be testable! We do use a lot of models in ecology, and even use them to make predictions (e.g. species distribution models (SDMs), dynamic vegetation models (DVMs), etc), but these predictions are typically 50+ years into the future, which is way to long to wait to see if our predictions were reasonable or useful. A quick aside on model validation vs testing predictions: Testing predictions with new data collected after you’ve made your predictions is the most robust way to validate a model, but you usually want to do some form of validation before you make your final predictions. For this we usually do some form of cross-validation whereby we split the data into a “training” subset (that we use for fitting (or training) the model) and a “test” subset (that we try to predict). 4.2 Iterative ecological forecasting The recent growth in interest in iterative ecological forecasting seeks to not only make prediction a central focus in ecology, but to do so on a time scale that is both useful for decision makers and allows us to learn from testing our predictions (days to decades). This is a great initiative, but as we will see it poses a number of major challenges and requires a big improvement in quantitative skills in biology (hence this course…). Figure 4.1: The iterative ecological forecasting cycle in the context of the scientific method and the adaptive management and monitoring cycles (Dietze et al. 2018). The iterative ecological forecasting cycle is tightly aligned to the scientific method cycle: Hypotheses (A) are embedded in models (B). The models integrate over uncertainties in initial conditions (IC), inputs, and parameters to make probabilistic forecasts (the purple distributions, Fx, in step C), sometimes for multiple alternative scenarios. New observations are then compared with these predictions (D) to update estimates of the current state of the system (Analysis) and assess model performance (E), allowing for the selection among alternative model hypotheses (Test and Refine). The iterative forecasting cycle also feeds into adaptive management and monitoring: In Adaptive Management and decision analysis, alternative decision scenarios are generated (2) based on an assessment of a problem (1). These decision scenarios are typically used to define the scenarios (or boundary conditions) for which models are run (“Scenarios” arrow), but can also feed into scientific hypotheses (not shown). Forecasts (Fx) are key in assessing the trade-offs and relative merits between alternative decision options (3). The decision(s) taken (4) determine the monitoring requirements (5), which allow us to evaluate the outcomes and reassess the problem (1), and start the adaptive management cycle again. Note that the iterative forecast cycle is also useful for adaptive management in that the analysis and partitioning of forecast uncertainties (from step C) can provide further guidance on what and how to monitor, so as to optimize the reduction in model uncertainties. This represents Adaptive Monitoring (dashed line) and is a cycle of itself (Lindenmayer and Likens 2009), but is largely subsumed by the other cycles here so we won’t go into it any further. Thus the iterative cycles of science, forecasting, management and monitoring are tightly intertwined and can interact continuously. What isn’t clear from Figure 4.1 is that all of this needs to be founded on a highly efficient ecoinformatics pipeline that is robust and rapidly updateable - i.e. follows Reproducible Research principles. Adding this component helps to highlight what I like to think of as “The Olympian Challenge of data-driven ecological decision making”. Figure 4.2: The Olympian Challenge of data-driven ecological decision making. References "]]

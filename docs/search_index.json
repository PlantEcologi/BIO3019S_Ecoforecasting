[["index.html", "A Minimal Introduction to Ecological Forecasting and Reproducible Research 1 Overview 1.1 General 1.2 Module and Project details 1.3 A bit about me 1.4 Acknowledgements and further reading:", " A Minimal Introduction to Ecological Forecasting and Reproducible Research Jasper Slingsby 2021-09-19 1 Overview This module is a minimal introduction to Ecological Forecasting and Reproducible Research for the 3rd year undergraduate Biological Sciences BIO3019S class at the University of Cape Town. 1.1 General I provide a very brief introduction to Ecological Forecasting. We only have a two weeks, so this really is a minimalist introduction. I’ll focus on providing a broad overview of the general framework and motivation for ecological forecasting, but won’t have time to delve into the more gory theoretical and statistical details. I mostly use Ecological Forecasting as a framework to highlight various themes that are increasingly important for quantitative biologists - understanding how we inform or make decisions; managing data; working reproducibly; propagating, understanding and reducing uncertainty. The core outcomes/concepts I hope you’ll come away with: To be able to situate the role of models and the importance of forecasting in science and ecological decision making Familiarity with the concepts and understand the need for Open, Reproducible Science Familiarity with The Data Life Cycle Familiarity with Open Science tools 1.2 Module and Project details Lectures Lectures will be held live, but online 12:00 - 12:45 from the 30th September to the 13th October. The zoom link (see Vula) should stay the same for all lecture sessions. Practicals There is only one practical for this module, 2-5PM on Tuesday the 5th October. Details to follow. 1.3 A bit about me I’m an ecologist who has become more quantitative through time, but has little formal training in quantitative methods (i.e. I’ve learnt by doing over the past 20 years). As such, I will make elementary mistakes. In fact, the entry requirements for this course are beyond my formal training, so you may have much to teach me! If you spot any errors, confusion or contradictions please let me know and I’ll get back to you and/or update the course notes accordingly. 1.4 Acknowledgements and further reading: Many of the following resources were instrumental in me pulling this material together and are worth spending time exploring ecoforecast.org Dietze, Michael C. 2017. Ecological Forecasting. Princeton University Press. https://doi.org/10.2307/j.ctvc7796h. Dietze, Michael C. et al. 2018. “Iterative near-Term Ecological Forecasting: Needs, Opportunities, and Challenges.” Proceedings of the National Academy of Sciences of the United States of America 115 (7): 1424–32. https://doi.org/10.1073/pnas.1710231115. All code, images, etc can be found here. I have only used images and other materials that were made available online under a non-restrictive license (Creative Commons, etc) of for which I have express permission, and have attributed my sources. Content without attribution is my own and shared under the license below. If there are any errors or any content you find concerning with regard to licensing, or that you find offensive, please contact me. Any feedback, positive or negative, is welcome! This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License. "],["quiz.html", "2 How do we make decisions?", " 2 How do we make decisions? What factors do you consider when making a decision? "],["models.html", "3 Models and decision making 3.1 The basics of making a decision 3.2 Getting quantitative 3.3 Iterative decision-making and the scientific method 3.4 The importance of prediction in ecology 3.5 Iterative ecological forecasting 3.6 Delete from here once done… - How to reference", " 3 Models and decision making 3.1 The basics of making a decision Informing decisions typically requires knowing (or guessing at) something about the future. To this end, once a problem and the need to make a decision have been identified, the factors we consider when making that decision usually include: Evidence Experience Expectation Uncertainty The relationship between these can be represented like so: Figure 3.1: The factors considered when making a decision. Your decision is typically based on your expectation of the outcome. This expectation is based on existing evidence and/or previous experience. Uncertainty is a part of each step. There are a number of reasons why the existing evidence or previous experience may be imperfect for the decision at hand, leading to uncertainty in the expectations. There may also be uncertainty in the way in which you use the evidence and experience to develop your expectation. We will come back to these sources of uncertainty later in the module, but needless to say, quantifying and understanding the uncertainty is crucial in any decision. If uncertainty is high your expectation may be no better than random, and thus useless for informing your decision. Quantifying uncertainty properly helps us circumvent two evils which could mislead decision makers: being falsely overconfident in our predictions (potentially leading to a “wrong” decision), or being falsely uncertain in our predictions (which would encourage overly conservative decisions/actions which may be wasteful or less effective). Lastly, ignoring or quantifying uncertainty incorrectly can lead to bias predictions. 3.2 Getting quantitative The nice thing about the framework above is that it is similar whether you are approaching the decision qualitatively or quantitatively (i.e. using models and data to inform your decision). Figure 3.2: Using models and data when making a decision. Following a quantitative approach the “evidence” is typically empirical data, which can be fed into a model to make forecasts that can help inform the decision. The “experience” are the current state of knowledge and your prior belief, which you use to specify the type and structure of your model (or ensemble of models) and the scenario(s) you want to evaluate. The “experience” can also help you evaluate the assumptions of your model(s), and if you are using a Bayesian model, can be included directly in the model when specifying priors (more on this later in the module). 3.3 Iterative decision-making and the scientific method Few decisions in natural resource management are once-off, and most are made repeatedly at some time-step (e.g. daily, monthly, seasonally, annually, decadally, etc). Should you burn, cull, harvest, restore, etc? While one should always evaluate the outcome of your decision, this is especially important when the decision will need to be repeated, so that you can learn from experience. Figure 3.3: Iterative decision making. When using quantitative forecasts this can be done by collecting new data and updating your prior knowledge by evaluating the outcomes of the decision against the original model forecasts. This can tell you whether your forecast was any good and whether you need to refine or replace your model, consider additional scenarios or inputs, etc. We’ll discuss doing this quantitatively in section XXX, by fusing your new data and knowledge into a new forecast. It’s worth highlighting the similarity between the iterative decision making cycle I’ve outlined in figure 3.3 and the scientific method, i.e.: Observation &gt; Hypothesis &gt; Experiment &gt; Analyze &gt; Interpret &gt; Report &gt; (Repeat) Figure 3.4: The Scientific Method overlain on iterative decision making. So a focus on iterative decision-making facilitates iterative learning (i.e. scientific progress). 3.4 The importance of prediction in ecology “prediction is the only way to demonstrate scientific understanding” (Houlahan et al. 2017) While this view may be slightly overstated, it is a very good point. If we cannot make reasonably good predictions, we’re missing something. Unfortunately, prediction has not been a central focus in ecology, impeding progress in the improvement of our ecological understanding. To make predictions we need models, and models provide structured summaries of our current ecological understanding (conceptual or quantitative, but preferably quantitative, because these are easier to compare). Without making predictions and comparing the skill of new models to old ones, we can’t track if we are making progress! Figure 3.5: Prediction… from xkcd.com/2370, used under a CC-BY-NC 2.5 license. In ecology we mostly test qualitative, imprecise hypotheses: “Does X have an effect on Y?” rather than “What is the relationship between X and Y?” or better yet “What value would we expect Y to be, given a particular value of X?”. Without testing precise hypotheses and using the results to make testable predictions we don’t know if our findings are generalizable beyond the specific data set we collected. If our results are not generalizable, then we’re not really making progress towards a better understanding of ecology. A key point here is that the predictions must be testable! We do use a lot of models in ecology, and even use them to make predictions (e.g. species distribution models (SDMs), dynamic vegetation models (DVMs), etc), but these predictions are typically 50+ years into the future, which is way to long to wait to see if our predictions were reasonable or useful. A quick aside on model validation vs testing predictions: Testing predictions with new data collected after you’ve made your predictions is the most robust way to validate a model, but you usually want to do some form of validation before you make your final predictionst o make sure the model is working reasonably well. For this we most commonly do some form of cross-validation, whereby we split the data into a “training” subset (that we use for fitting (or training) the model) and a “test” subset (that we try to predict). If your model is no good at predicting your test data, there’s probably no point in making predictions into the future… 3.5 Iterative ecological forecasting The recent growth in interest in iterative ecological forecasting seeks to not only make prediction a central focus in ecology, but to do so on a time scale that is both useful for decision makers and allows us to learn from testing our predictions (days to decades). This is a great initiative, but as we will see it poses a number of major challenges and requires a big improvement in quantitative skills in biology (hence this course…). Figure 3.6: The iterative ecological forecasting cycle in the context of the scientific method and the adaptive management and monitoring cycles (Dietze et al. 2018). The iterative ecological forecasting cycle is tightly aligned to the scientific method cycle: Hypotheses (A) are embedded in models (B). The models integrate over uncertainties in initial conditions (IC), inputs, and parameters to make probabilistic forecasts (the purple distributions, Fx, in step C), sometimes for multiple alternative scenarios. New observations are then compared with these predictions (D) to update estimates of the current state of the system (Analysis) and assess model performance (E), allowing for the selection among alternative model hypotheses (Test and Refine). The iterative forecasting cycle also feeds into adaptive management and monitoring: In Adaptive Management and decision analysis, alternative decision scenarios are generated (2) based on an assessment of a problem (1). These decision scenarios are typically used to define the scenarios (or boundary conditions) for which models are run (“Scenarios” arrow), but can also feed into scientific hypotheses (not shown). Forecasts (Fx) are key in assessing the trade-offs and relative merits between alternative decision options (3). The decision(s) taken (4) determine the monitoring requirements (5), which allow us to evaluate the outcomes and reassess the problem (1), and start the adaptive management cycle again. Note that the iterative forecast cycle is also useful for adaptive management in that the analysis and partitioning of forecast uncertainties (from step C) can provide further guidance on what and how to monitor, so as to optimize the reduction in model uncertainties. This represents Adaptive Monitoring (dashed line) and is a cycle of itself (Lindenmayer and Likens 2009), but is largely subsumed by the other cycles here so we won’t go into it any further here. Thus the iterative cycles of science, forecasting, management and monitoring are tightly intertwined and can interact continuously. What isn’t clear from Figure 3.6 is that all of this needs to be founded on a highly efficient informatics pipeline that is robust and rapidly updateable - i.e. follows Reproducible Research principles. Adding this link helps to highlight what I like to think of as “The Olympian Challenge of data-driven ecological decision making”. Figure 3.7: The Olympian Challenge of data-driven ecological decision making. Working reproducibly requires learning a lot of skills and can take a lot of effort, but is well worth it in the long run - for you as an individual, and for science in general. This is going to be the focus of the next section (and the practical), before we continue with other components of the ecological forecasting cycle. 3.6 Delete from here once done… - How to reference You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 3. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter ??. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 3.8: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 3.8. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 3.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 3.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2021) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). References "],["reproducibility.html", "4 Reproducible research 4.1 The Reproducibility Crisis 4.2 Replication and the Reproducibility Spectrum 4.3 Why work reproducibly? 4.4 Scientific Workflows 4.5 Coding and code management 4.6 Computing and software", " 4 Reproducible research 4.1 The Reproducibility Crisis “Replication is the ultimate standard by which scientific claims are judged.” (Peng 2011) Replication is one of the fundamental tenets of science and if the results of a study or experiment cannot by replicated by an independent set of investigators then whatever scientific claims were made should be treated with caution! At best, it suggests that evidence for the claim is weak or mixed, or specific to particular ecosystems or other circumstances and cannot be generalized. At worst, there was error (or even dishonesty) in the original study and the claims were plainly false. In other words, published research should be robust enough and the methods described in enough detail that anyone else should be able to repeat the study (using the publication only) and find similar results. Sadly, this is rarely the case!!! Figure 4.1: ‘Is there a reproducibility* crisis?’ Results from a survey of &gt;1500 top scientists (Baker 2016; Penny 2016). *Note that they did not discern between reproducibility and replicability, and that the terms are often used interchangeably. We have a problem… Since we’re failing the gentleman’s agreement1 that we’ll describe our methods in enough detail that anyone else should be able to repeat the study (using the publication only) and find similar results, modern scientists are trying to formalize the process in the form of Reproducible Research. Reproducible research makes use of modern software tools to share data, code and other resources required to allow others to reproduce the same result as the original study, thus making all analyses open and transparent. As you will learn in this module, working reproducibly is not just a requirement for using quantitative approaches in iterative decision-making, it is central to scientific progress!!! While full replication is a huge challenge (and sometimes impossible) to achieve, it is something all scientists should be working towards. 4.2 Replication and the Reproducibility Spectrum Understandably, some studies may not be entirely replicable purely due to the nature of the data or phenomenon (e.g. rare phenomena, long term records, loss of species or ecosystems, or very expensive once-off science projects like space missions). In these cases the “gold standard” of full replication (from new data collection to results) cannot be achieved, and we have to settle for a lower rung on the reproducibility spectrum (Figure 4.2). Figure 4.2: The Reproducibility Spectrum (Peng 2011). Reproducibility falls short of full replication because it focuses on reproducing the same result from the same data set, rather than analyzing independently collected data. While this may seem trivial, you’d be surprised at how few studies are even reproducible, let alone replicable. 4.3 Why work reproducibly? Figure 4.3: Let’s start being more specific about our miracles… Cartoon © Sidney Harris. Used with permission ScienceCartoonsPlus.com In addition to basic scientific rigour, working reproducibly is hugely valuable, because: (Adapted from “Five selfish reasons to work reproducibly” (Markowetz 2015)) It helps us avoid mistakes and/or track down errors in analyses This is what highlighted the importance of working reproducibly for me. In 2017 I published the first evidence of observed climate change impacts on biodiversity in the Fynbos Biome (Slingsby et al. 2017). The analyses were quite complicated, and when working on the revisions I found an error in my R code. Fortunately, it didn’t change the results qualitatively, but it made me realize how easy it is to make a mistake and potentially put the wrong message out there! This encouraged me to make all data and R code from the paper available, so that anyone is free to check my data and analyses and let me (and/or the world) know if they find any errors. It makes it easier to write papers e.g. Dynamic documents like RMarkdown or Jupyter Notebooks update automatically when you change your analyses, so you don’t have to copy/paste or save/insert all tables and figures (or worry about whether you included the latest versions. It helps the review process Often issues picked at by reviewers are matters of clarity/confusion. Sharing your data and analyses allows them to see exactly what you did, not just what you said you did, allowing them to identify the problem and make constructive suggestions. It’s also handy to be able to respond to a reviewer’s comment with something like “That’s a great suggestion, but not really in line with the objectives of the study. We have chosen not to include the suggested analysis, but do provide all data and code so that interested readers can explore this for themselves.” (Feel free to copy and paste - CCO 1.0) It enables continuity of the research When people leave a project (e.g. students/postdocs), or you forget what you did X days/weeks/months/years ago, it can be a serious setback for a project and make it difficult for you or a new student to pick up where things left off. If the data and workflow are well curated and documented this problem is avoided. Trust me, this is a very common problem!!! I have many papers that I (or my students) never published and may never go back to, because I know it’ll take me a few days or weeks to understand the datasets and analyses again… This is obviously incredibly important for long-term projects!!! It helps to build your reputation Working reproducibly makes it clear you’re an honest, open, careful and transparent researcher, and should errors be found in your work you’re unlikely to be accused of dishonesty (e.g. see my paper example under point 1 - although no one has told me of any errors yet…). When others reuse your data, code, etc you’re likely to get credit for it - either just informally, or formally through citations or acknowledgements (depending on the licensing conditions you specify - see “Preserve” in the Data Life Cycle). And some less selfish reasons (and relevant for ecoforecasting): It allows you (or others) to rapidly build on previous findings and analyses It allows easy comparison of new analytical approaches to older ones It makes it easy to repeat the same analyses when new data are collected or added And one more selfish reason (but don’t tell anyone I said this): Should you decide to leave biology, reproducible research skills are highly sought after in other careers like data science etc… 4.4 Scientific Workflows Figure 4.4: ‘Data Pipeline’ from xkcd.com/2054, used under a CC-BY-NC 2.5 license. Working reproducibly requires careful planning and documentation of each step in your scientific workflow from planning your data collection to sharing your results. This entails a number of overlapping/intertwined components, namely: Coding and code management (data analysis) Computing and software Data management For the rest of this section we’ll work through these components and some of the tools that help you achieve this. Data management is big enough to warrant a separate chapter. 4.5 Coding and code management Why write code Some rules of coding Version control 4.6 Computing and software Why use open source software Containers References "],["data-management.html", "5 Data Management 5.1 Why do you need to manage your data? 5.2 The Data Life Cycle 5.3 Data and decisions", " 5 Data Management 5.1 Why do you need to manage your data? Data management is often the last thing on a scientists mind when doing a new study - “I have a cool idea, and I’m going to test it!” You don’t want to “waste” time planning how you’re going to manage your data and implementing that plan… Unfortunately, this never ends well and really is a realm where “haste makes waste.” Figure 5.1: The ‘Data Decay Curve’ (Michener et al. 1997) Bad data management leads to data loss… (The “data decay curve” (Michener et al. 1997)) Your future self will hate you if you lose it before you’re finished with it - Eek!!! This is less likely in the world of Dropbox, Google Drive, iCloud etc, but I know people who had to repeat their PhD’s because they lost their data or it was on a laptop that was stolen… Data has value beyond your current project: to yourself for reuse in future projects, collaborations, etc (i.e. publications and citations), for others for follow-up studies, or combining multiple datasets for meta-analyses or synthesis etc for science on general (especially long-term ecology in a time of global change) We’ve covered this before, but it is also key for transparency and accountability. Data collection is expensive, and is often paid for with taxpayers’ money. You owe it to them to make sure that science gets the most out of that data in the long term. Lastly, good planning and data management can help iron out issues like intellectual property, permissions for ethics, collection permits, etc, in addition to outlining expectations for who will be authors on the paper(s), responsible for managing different aspects of the data etc up front. If you don’t establish these permissions and ground rules early they can result in data loss, not being able to publish the study, damage careers, and/or damage relationships in collaborations (including student-supervisor), etc. To avoid data (and relationship) decay, and to reap the benefits of good data management, it is important to consider the full Data Life Cycle. 5.2 The Data Life Cycle Figure 5.2: The Data Life Cycle, adapted from https://www.dataone.org/ Note that there are quite a few different versions of the data life cycle out there. This is the most comprehensive one I know of, and covers all the steps relevant to a range of different kinds of research projects. A full description of this data life cycle and related ecoinformatics issues can be found in (Michener and Jones 2012). Not all projects need to do all steps, nor will they necessarily follow the order here, but it is worth being aware of and considering all steps. For example, often the first thing you do when you have a new hypothesis is search around for any existing data that could be used to test it without having to spend money and time collecting new data (i.e. skip to step 6). In this case I would argue that you should still do step 1 (Plan), and you’d want to do some checking to assure the quality of the data (step 3), but you can certainly skip steps 2, 4 and 5. A meta-analysis or synthesis paper would probably do the same. On the other hand, if you’re collecting new data you would do steps 1 to 5 and possibly skip 6 and 7, although in my experience few studies do not reuse existing data (e.g. weather or various GIS data to put your new samples in context). 5.2.1 Plan Good data management begins with planning. In this step you essentially outline the plan for every step of the cycle in as much detail as possible. Usually this is done by constructing a document or Data Management Plan (DMP). While developing DMPs can seem tedious, they are essential for the reasons I gave above, and because most funders and universities now require them. Fortunately, there are a number of online data management planning tools that make it easy by providing templates and prompts to ensure that you cover all the bases, like the Digital Curation Centre’s DMPOnline and UCT’s DMP Tool. Figure 5.3: Screenshot of UCT’s Data Management Planning Tool’s Data Management Checklist. A key thing to bear in mind is that a DMP is a living document and should be regularly revised during the life of a project, especially when big changes happen - e.g. new team members, new funding, new direction, change of institution, etc. I typically develop one overarching DMP for an umbrella project (e.g. a particular grant), but then add specifics for subprojects (e.g. separate student projects etc). 5.2.2 Collect and Assure There are many, many different kinds of data that can be collected in a vast number of ways! Figure 5.4: Springer Nature Infographic illustrating the vast range of research data types. While “Collect” and “Assure” are different steps in the life cycle, I advocate that it is foolish to collect data without doing quality assurance and quality control (QA/QC) as you go, irrespective of how you are collecting the data. For example: automated logging instruments (weather stations, cameras, acoustic recorders) need to be checked that they’re logging properly, are calibrated/focused, are reporting sensible values, etc if you’re filling in data sheets, you need to check that all fields have been completed, that there are no obvious errors and that any numbers or other values look realistic. In fact, if you’re using handwritten data sheets it’s best to capture them as soon as possible (i.e. that evening), because that helps you spot errors and omissions, you have a better chance of deciphering bad handwriting or cryptic notes, and you can plot any values to see if there are suspicious outliers (e.g. because someone wrote down a measurement in centimetres when they were meant to use metres). When transcribing or capturing data into a spreadsheet or database it is often best to use data validation tricks like drop-down menus, conditional formatting, restricted value ranges etc to avoid spelling mistakes and highlight data entries that are outside the expected range of the data field. It may seem like a lot of effort to set this up, but it’ll save you a lot of time and pain in the long run!!! Increasingly, I’ve started moving towards capturing data directly into a spreadsheet on a tablet, or better yet, into an App on my phone. There are a number of “no code” app builders these days like AppSheet that inserts data directly into Google Sheets and saves photos to your Google Drive. Figure 5.5: An example data collection app I built in AppSheet that allows you to log GPS coordinates, take photos, record various fields, etc. 5.2.3 Describe There are few things worse than having a spreadsheet of what you think is the data you need, but you don’t know what the column names mean, how variables were measured, what units they’re reported in, etc… - Especially when you were the one who collected and captured the data!!! This descriptive data about the data is called metadata and is essential for making the data reusable, but is also useful for many other purposes like making the data findable (e.g. using keyword searches). In fact, metadata makes up the majority of what are called the FAIR data principles (Wilkinson et al. 2016), which largely focus on this and the next few steps of the Data Life Cycle. I’m not going to dwell on them other than to say that they are a key component of making your work reproducible, and that like reproducibility, practicing FAIR data principles is a spectrum. Figure 5.6: The FAIR data principles ErrantScience.com. Some key kinds of metadata: the study context why the data were generated who funded, created, collected, assured, managed and owns the data (not always the same person) contact details for the above when and where the data were collected where the data are stored the data format what is the file format what softwares were used (and what version) the data content what was measured how it was measured what units was it reported in what QA/QC has been applied is it raw data or a derived data product (e.g. spatially interpolated climate layers) if derived, how it was analyzed Metadata standards and interoperability Many data user communities have developed particular metadata standards or schemas in an attempt to enable the best possible description and interoperability of a data type for their needs. They are typically human and machine-readable data, so that the metadata records can also be read by machines, facilitating storing and querying multiple datasets in a common database (or across databases). Figure 5.7: How standards proliferate… from xkcd.com/927, used under a CC-BY-NC 2.5 license. Using common metadata schemas has many advantages in that they make data sharing easier, they allow you to search and integrate data across datasets, and they simplify metadata capture (i.e. having a list of required fields makes it easier to not forget any). There are many standards, but perhaps the most common ones you’ll encounter in biological sciences (other than geospatial metadata standards) are DarwinCore and Ecological Metadata Language (EML). 5.2.4 Preserve There are two major components to preserving your data: Back your data up now!!! (and repeat regularly) Losing your data can be incredibly inconvenient!!! A good friend of mine lost all of his PhD data twice. It took him 7 years to complete the degree… Beyond inconvenience, losing data can be incredibly expensive! Doing 4 extra years to get your PhD is expensive at a personal level, but if the data are part of a big project it can rapidly add up to millions - like How Toy Story 2 Almost Got Deleted. PRO TIP: Storing data on the cloud is not enough! You could easily delete that single version of all your data! You may also lose access when you change institution etc. E.g. What happens to your UCT MS OneDrive and Google Drive content when you graduate and ICTS close your email account? Long-term preservation and publication This involves the deposition of your data (and metadata!) in a data repository where it can be managed and curated over the long term. This is increasingly a requirement of funders and publishers (i.e. journals). Many journals allow you (or require you) to submit and publish your data with them as supplementary material. Unfortunately, many journals differ in how they curate the data and whether they are available open access. I prefer to publish my data in an online open access repository where you can get a permanent Digital Object Identifier (DOI) that you can link to from your paper. Another consideration, if you are keen for people to reuse your data (which if you are not you will fail this course by default) is where people are most likely to look for your data (i.e. making your data “Findable/Discoverable.” There are many “bespoke” data repositories for different kinds of data, e.g. Global databases: GenBank - for molecular data TRY - for plant traits Dryad - for generalist biological and environmental research South African databases: SANBI - for most kinds of South African biodiversity data SAEON - for South African environmental data (e.g. hydrology, meteorology, etc) and biodiversity data that don’t fit SANBI’s databases If none of these suit your data, there are also “generalist” data repositories that accept almost any kind of data, like: FigShare Zenodo UCT’s ZivaHub (which is built on and searchable through FigShare) I haven’t discussed physical samples at all. These are obviously a huge (if not bigger) challenge too, although there are some obvious homes for common biological data, like herbaria for plant collections and museums for animal specimens. 5.2.5 Discover This is perhaps the main point of the Data Life Cycle and FAIR data principles - to make data findable so that it can be reused. The biggest challenge to discovering data is that so many datasets are not online and are in the “filing cabinet in a bath in the basement under a leaking pipe” as in Figure 5.6. If you preserve and publish them in an online data repository, this overcomes the biggest hurdle. The next biggest challenge is that there is so much online that finding what you need can be quite challenging (like looking for a needle in a haystack…). This is where choosing the right portal can be important. It is also what metadata standards are aimed at - allowing interoperable searches for specific data types across multiple repositories. A final consideration is whether you have permission to use the data. You can often find out about the existence of a dataset, either online or in a paper, but the data aren’t made freely available. This is where licensing comes into play. Most data repositories require you to publish the data under a license. There are many options depending on the kind of data and what restrictions you want to put on its use. I’m not going to go into the gory details, but Creative Commons have created an extensible system of generic licenses that are easy to interpret and cover most situations. I say extensible because the licenses are made up of a string of components that can be layered over each other. For example: CCO - means it is Open - i.e. there are no restrictions on use and it is in the public domain CC BY - means by attribution - you can use the data for any purpose, but only if you indicate attribution of the data to the source or owner of the data CC BY-SA - means by attribution + share alike - i.e. you have to indicate attribution and share your derived product under the same license CC BY-ND - means by attribution + no derivatives - i.e. you have to indicate attribution, but cannot use it to make a derived product. This is often used for images - allowing you to show the image, but not to alter it. CC BY-NC - means by attribution + non-commercial - you have to indicate attribution, but cannot use it for commercial purposes (i.e. you can’t sell derived products) CC BY-NC-SA - by attribution + non-commercial + share alike CC BY-NC-ND - by attribution + non-commercial + no derivatives 5.2.6 Integrate 5.2.7 Analyze 5.3 Data and decisions latency uncertainty References "],["practical.html", "6 Practical: Pair coding with GitHub 6.1 Preparation 6.2 Objectives 6.3 Prairie Phenology 6.4 Modular Design 6.5 Task 1: Create &amp; Clone Repository 6.6 Task 2: Add the first function: download.phenocam 6.7 Task 3: Collaborator adds plot.phenocam 6.8 Task 4: Owner adds pred.logistic and fit.logistic 6.9 Task 5: Collaborator adds the master script", " 6 Practical: Pair coding with GitHub 6.1 Preparation For the practical, we’ll be using the R statistical programming language and the Git version control system. We’ll also be using an integrated development environment (IDE) for each: RStudio and GitHub, respectively. First we’ll start with the necessary software. Download and install the latest version of R Download and install the latest free version of RStudio Desktop Download and install the latest version of Git - accept all the defaults Then get started with GitHub: Create a GitHub account Run through the 10 minute GitHb tutorial that is offered when you activate your GitHub Account (It’ll really help you get the idea behind what Git does!) Now you have RStudio, R and Git installed, and you have a working GitHub account that lets you do stuff online, but what remains is to get GitHub working locally and configuring RStudio to use GitHub. Install GitHub CLI (Command Line Interface). For Windows you can download the installer here Open RStudio. Select the Terminal tab (top left, next to Console) Enter gh auth login, then follow the prompts: Select GitHub.com When prompted for your preferred protocol for Git operations, select HTTPS When asked if you would like to authenticate to Git with your GitHub credentials, enter Y When asked how you would like to authenticate select Login with web browser Copy the 8-digit code and hit Enter Github.com will open in your internet browser - paste the code and hit enter If any of these steps don’t work, just start with gh auth login in Terminal again In RStudio Go to Global Options (from the Tools menu) Click Git/SVN Make sure Enable version control interface for RStudio projects is on If necessary, enter the path for your Git or SVN executable where provided Click Apply Restart RStudio Ok, now everything should be working. The next steps are to fork and clone your first repo to see if everything is working, and then to modify a file in RStudio, push it back to your forked repo, and then create a pull request for me to review and accept your changes. This let’s me know that you’ve made it through the preparation for the practical, and it gives me your GitHub username. In GitHub: Make sure you are logged in, search and navigate to JasperUCT/pullltest Click Fork, which will make a copy of the repository to your own workspace Copy the URL to your own version and follow the instructions below for cloning the repository in RStudio In RStudio: In the top-right of the window you’ll see Project: (None) - click on it and select New Project then Version Control then Git In the Repository URl: box paste the URL to your forked repo (It should look something like : https://github.com/YourGitHubUsername/your-forked-repo_name.git Project directory name should fill automatically For Create project as subdirectory of: hit Browse and navigate through your file system to where you want to put your folder for storing your Git repositories stored. I recommend something like ~Documents/GIT (If you’ve used Git before you may have set this already and can skip this step) Click Create Repository Your RStudio window should now look like this: Figure 6.1: What you should see… Note there are three files in the Files tab in the bottom-right window, and you should see a Git tab for the top-right window. It also says pulltest in the project drop-down top-right on mine, but yours will display the name you gave your forked repo. Open the README.md file in RStudio Add your message and save the file Select the Git tab in the top-right window Check the box next to README.md and click Commit Add a Commit message to say what changes you’ve made Then hit Push It will ask you to authenticate. Select Authenticate in your web browser. The web browser will ask you to give Git permissions. Allow the permissions and it should work. If you get an error at this point to the effect of “You do not have permission to push to this repository,” then you may have forgotten to fork your own repo from my one and are trying to push to mine… If so, start again from step 9. Final step!!! In GitHub Click Pull requests (top-leftish) Click New pull request (green, top-right) Click Create pull request (green, top-right) It should already be on this repo unless you’ve been doing other things in Git In the comment window put your name so I know who you are if it isn’t obvious from your GitHub username Click Create pull request (green, bottom-right) And you’re done!!! I’ll get a notification of your pull request and, if all’s in order, I’ll accept it. If not, I’ll reply with a comment on the pull request. You should receive a notification via the email you registered with your GitHub account. Just a quick acknowledgement that I have borrowed and adapted much of the following from Michael Dietze’s Pair coding practical that is a living online supplement to his book (Dietze 2017). Thanks for sharing the code under an MIT license Mike! 6.2 Objectives The primary goal of this exercise is to gain experience working collaboratively to develop a scientific workflow. As such, this assignment is best completed with a partner. Specifically, we will outline a simple analysis, break the overall job into parts, and have each person complete part of the project. To put these parts together we will be using Github. Along the way we will also be exploring the statistical concept of Likelihood. 6.3 Prairie Phenology The goal of our analysis is to investigate the phenological state of the U. Illinois Tall Grass Prairie. We will be using green chromatic coordinate (GCC), which is a color index. Before building the workflow you are encouraged to take a look at the site http://phenocam.sr.unh.edu/webcam/sites/uiefprairie/ and the raw csv data https://phenocam.sr.unh.edu/data/archive/uiefprairie2/ROI/uiefprairie2_AG_1000_1day.csv Note: older data for this site (2009-2018) is available at: http://phenocam.sr.unh.edu/data/archive/uiefprairie/ROI/uiefprairie_GR_1000_1day.csv The workflow for this analysis with have three components: Download PhenoCam data for the U. Illinois Tall Grass Prairie site Visualize the data with a mean and 95% confidence interval Fit a simple logistic model to spring leaf out for one specific year From this overall design, let’s next outline the specific steps involved as pseudocode ### Prairie Phenology Workflow ## Download phenology data ## Plot overall phenology data ## Create and visualize subset of data for leaf out ## Fit logistic model ## Visualize model and data 6.4 Modular Design From this overall design we can look for ways to modularize the analysis. One feature that jumps out is that we need to visualize the data three times, so we should definitely make a function to do that. The inputs to the function would be an x-axis (date), y-axis (gcc_mean), and error estimate (gcc_std), which we might pass as a dataframe for convinience. Since this is a graphing function we’d also like the ability to set all sorts of plot characteristics, which can be done in R by passing ... as an argument and then passing that on to the internal plot call. The proposed function interface would thus be ##&#39; Plot Phenocam data ##&#39; ##&#39; @param dat dataframe of date, gcc_mean, gcc_std ##&#39; @param ... additional graphing parameters plot.phenocam &lt;- function(dat,...) Next, because the raw data will be downloaded off the web and has embedded meta-data to handle, let’s go ahead and create a download function. This function just needs to know the URL for where to find the data. Unlike the plot function, this function will return something (the data that was downloaded), so it would be good design to document what is returned and how it will be formatted ##&#39; Download Phenocam data ##&#39; ##&#39; @param URL web address where data is located ##&#39; @return data.frame with days as rows, variables as columns download.phenocam &lt;- function(URL) Let’s also create a function to fit the logistic model to the spring leaf-out data, since we could easily see applying this same function to other data sets. The input to such a fit would obviously be the same data.frame that we’re using to make the plot. To do the fit itself we’ll use Maximum Likelihood, so in addition to the data we’ll need to provide an initial guess at the model parameters, which we’ll pass on to the numerical optimization. We’ll also want to return the full output from that numerical optimization so that we can check if it converged successfully. ##&#39; Fit logistic model ##&#39; ##&#39; @param dat dataframe of day of year (doy), gcc_mean, gcc_std ##&#39; @param par vector of initial parameter guess ##&#39; @return output from numerical optimization fit.logistic &lt;- function(dat,par) Finally, because we’ll want to make a plot of the logistic model after we’re done, let’s create a function for performing the model calculation. This function will also come in handy within the fit.logistic function. ##&#39; Logistic model ##&#39; ##&#39; @param theta parameter vector ##&#39; @param x vector of x values ##&#39; @return vector of model predictions pred.logistic &lt;- function(theta,x) At this point we’ve spent a good bit of time up front on organization – we have a detailed plan of attack and have thought carefully about what each module is responsible for doing. Each task has well-defined inputs, outputs, and goals. Rather than facing a thankless job of documenting our code after we’re done, even though we haven’t written a single line of code yet we are largely done with our documentation. What remains to do is implementation. 6.5 Task 1: Create &amp; Clone Repository Because we’re going to employ version control in our project, our first step is to create the repository that our project will be stored in. To ensure that both you and your partner get to see every step of how to work with version control, for the rest of this exercise you are going to complete every step twice, once from the perspective of the OWNER of the repository and once as the COLLABORATOR. 6.5.1 OWNER Go to your account on github.com and under the Repositories tab click on the “New” button (green with a picture of a book on it) Choose a name for your repository, but make sure it’s different from your partner’s (Don’t choose a “Repository template,” and keep it a “Public” repository) Click the “Initialize this repository with a README” checkbox Optionally also provide a Description, Add a licence (e.g. MIT), and add R to the .gitignore (check “.gitignore” and search for the R template) Click “Create Repository” Copy the URL of your new repository by clicking the clipboard icon To clone the repository, open up RStudio and create a New Project using this URL. Note: If you already have a project open it will close when you do so. Don’t worry, you can return to that project after the prac using the drop-down in the top-right of the RStudio window. Select New Project from the menu in the top right corner Select Version Control then Git Paste the URL in and click Create Project 6.6 Task 2: Add the first function: download.phenocam Within this project we’ll create separate files for each part of the analysis. To make the order of the workflow clear we’ll want to name the files systematically. In the first file we’ll implement the download.phenocam function # ##&#39; Download Phenocam data # ##&#39; # ##&#39; @param URL web address where data is located # download.phenocam &lt;- function(URL) { # ## check that we&#39;ve been passed a URL # if (length(URL) == 1 &amp; is.character(URL) &amp; substr(URL,1,4)==&quot;http&quot;) { # ## read data # dat &lt;- read.csv(URL,skip = 22) # # ## convert date # dat$date &lt;- as.Date(as.character(dat$date)) # # return(dat) # } else { # print(paste(&quot;download.phenocam: Input URL not provided correctly&quot;,URL)) # } # } 6.6.1 OWNER In RStudio, click File &gt; New File &gt; R Script Copy and Paste the above function into this file Save the file as “01_download.phenocam.R” From the Git tab, click the box next to the file you just created. This is equivalent to git add Click Commit, enter a log message, and click Commit. This is equivalent to git commit To push the change up to Github click on the green up arrow. This is equivalent to git push 6.7 Task 3: Collaborator adds plot.phenocam With the first function complete, let’s now imagine that a COLLABORATOR has been tasked with adding the second function. To do so they must first fork and clone the repository 6.7.1 COLLABORATOR Go to Github and navigate to the project repository within the OWNER’s workspace. Click Fork, which will make a copy of the repository to your own workspace. Copy the URL to your own version and follow the instructions above for cloning the repository in RStudio. Open a new file, enter the code below, and then save the file as “02_plot.phenocam.R” # ## Define ciEnvelope function # ciEnvelope &lt;- function(x,ylo,yhi,col=&quot;lightgrey&quot;,...){ # ## identify chunks of data with no missing values # has.na = apply(is.na(cbind(x,ylo,yhi)),1,sum) # block = cumsum(has.na);block[has.na&gt;0] = NA # blocks = na.omit(unique(block)) # # for(i in blocks){ # sel = which(block==i) # polygon(cbind(c(x[sel], rev(x[sel]), x[sel[1]]), c(ylo[sel], rev(yhi[sel]), # ylo[sel[1]])), col=col,border = NA,...) # } # } # ##&#39; Plot Phenocam data # ##&#39; # ##&#39; @param dat dataframe of date, gcc_mean, gcc_std # ##&#39; @param ... additional graphing parameters # plot.phenocam &lt;- function(dat,...){ # # if(!is.null(dat)){ # # ## QC flags # gcc_mean = dat$gcc_mean # gcc_mean[dat$outlierflag_gcc_mean&gt;-9999] = NA # # ## base plot # plot(dat$date,dat$gcc_mean,type=&#39;l&#39;,...) # # ## calculate CI # ylo = dat$gcc_mean-1.96*dat$gcc_std # yhi = dat$gcc_mean+1.96*dat$gcc_std # # ## add confidence envelope # ciEnvelope(dat$date,ylo,yhi) # # ## replot mean line # lines(dat$date,dat$gcc_mean,lwd=1.5) # } else { # print(&quot;plot.phenocam: input data not provided&quot;) # } # # } Follow the instructions above to Add, Commit, and Push the file back to your Github Next you want to perform a “pull request,” which will send a request to the OWNER that they pull your new code into their mainline version. From your Github page for this project, click New Pull Request. Follow the instructions, creating a title, message, and confirming that you want to create the pull request 6.7.2 OWNER Once the COLLABORATOR has created the pull request, you should get an automatic email and also be able to see the pull request under the “Pull Requests” tab on the Github page for the project. Read the description of the proposed changes and then click on “Files Changed” to view the changes to the project. New code should be in green, while deleted code will be in pink. The purpose of a pull request is to allow the OWNER to evaluat the code being added before it is added. As you read through the code, if you hover your mouse over any line of code you can insert an inline comment in the code. The COLLABORATOR would then have the ability to respond to any comments. In larger projects, all participants can discuss the code and decide whether it should be accepted or not. Furthermore, if the COLLABORATOR does any further pushes to Github before the pull request is accepted these changes will automatically become part of the pull request. While this is a very handy feature, it can also easily backfire if the COLLABORATOR starts working on something different in the meantime. This is the reason that experienced users of version control will use BRANCHES to keep different parts separate. Click on the “Conversation” page to return where you started. All participants can also leave more general comments on this page. If you are happy with the code, click “Merge Pull Request.” Alternatively, to outright reject a pull request click “Close pull request” 6.8 Task 4: Owner adds pred.logistic and fit.logistic We are now past the ‘set up’ stage for both the OWNER and the COLLABORATOR, so for this task we’ll explore the normal sequence of steps that the OWNER will use for day-to-day work 6.8.1 OWNER Pull the latest code from Github. In RStudio this is done by clicking the light blue down arrow on the Git tab. This is equivalent to the commandline git pull origin master where origin refers to where the where you did your orginal clone from and master refers to your main branch (if you use branches you can pull other branches) Next, open up a new R file, add the code below, and save as “03_logistic.R” # ##&#39; Logistic model # ##&#39; # ##&#39; @param theta parameter vector # ##&#39; @param x vector of x values # ##&#39; @return vector of model predictions # pred.logistic &lt;- function(theta,x){ # z = exp(theta[3]+theta[4]*x) # Ey = theta[1]+theta[2]*z/(1+z) # } # ##&#39; Fit logistic model # ##&#39; # ##&#39; @param dat dataframe of day of year (doy), gcc_mean, gcc_std # ##&#39; @param par vector of initial parameter guess # ##&#39; @return output from numerical optimization # fit.logistic &lt;- function(dat,par){ # # ## define log likelihood # lnL.logistic &lt;- function(theta,dat){ # -sum(dnorm(dat$gcc_mean,pred.logistic(theta,dat$doy),dat$gcc_std,log=TRUE),na.rm=TRUE) # } # # ## fit by numerical optimization # optim(par,fn = lnL.logistic,dat=dat) # } As before, add your new file under the Git tab, Commit the change, and push it back to Github To estimate the parameters in the logistic model we will use the likelihood principle which states that “a parameter value is more likely than another if it is the one for which the data are more probable.” To do this we need to define a Likelihood, which is the relationship between the value of the parameter and the probability of some observed data. [For the record, the Likelihood is not a probability distribution because it does not integrate to 1]. In this case we’ll start by assuming a Normal likelihood and use the standard deviation that’s reported in the data to represent the uncertainty. In a more detailed analysis we’d want to follow up to check both these assumptions, but it’s a simple starting point. Applying the likelihood principle we would then look for the most likely value of \\(\\theta\\), the vector parameters in the logistic model, which we call the Maximum Likelihood estimate. For a number or reasons that will become clear in later lectures, it is common to work with negative log likelihoods instead of likelihoods, in which case the negative implies that instead of looking for the maximum we’re now looking for the minimum. The fact that logarithm is a monotonic transformation means that taking the log does not change the location of this minimum. The code for this comes in three parts. First is the logistic model itself, pred.logistic, which translates the equation \\[\\theta_1 + \\theta_2 {{exp(\\theta_3 + \\theta_4 x)}\\over{1+exp(\\theta_3 + \\theta_4 x)}}\\] into code. The logistic has an overall S shape, with \\(\\theta_1\\) defining the minimum and \\(\\theta_1 + \\theta_2\\) defining the max. The midpoint of the curve – the x value where the function is halfway between the minimum and maximum – occurs at \\(-\\theta_3 / \\theta_4\\), while the slope at that point is \\(\\theta_4/4\\). Second is the negative log likelihood function, lnL.logistic, which we’re trying to minimize. The core of this is the Normal probability density, dnorm. The first arguement is the data, the second the is model, and the third is the standard deviation. The fourth arguement says that we want to return the log density, which is much more accurate if it’s performed internally than if we take the log of what’s returned by dnorm. Since we have many data points dnorm returns a vector, which we then sum up and change the sign to turn this into a minimization problem. The third part is a call to a numerical optimization function, optim, that searches through parameter space to find the set of parameters that minimize the negative log likelihood (i.e. that Maximize the Likelihood). Arguements are the initial parameter guess, the function being minimized, and any additional parameters that get passed on to that function. 6.9 Task 5: Collaborator adds the master script The day-to-day workflow for the COLLABORATOR is similar, but not exactly the same as the OWNER. The biggest differences are that the COLLABORATOR needs to pull from the OWNER, not their own repository, and needs to do a pull request after the push. 6.9.1 COLLABORATOR Pull from OWNER. Unfortunately, this has to be done from the command line rather than the pull button within RStudio, which just pulls from the COLLABORATOR’s repository. In RStudio go to Tools &gt; Shell to open a terminal At the terminal type git pull URL master where URL is the address of the OWNER’s Github repository. Because it is a pain to always remember and type in the OWNER’s URL, it is common to define this as upstream git remote add upstream URL which is a one-time task, after which you can do the pull as git pull upstream master Open a new Rmd file and add the code below. This code just flushes out the pseudocode outline we started with at the beginning of this activity. # ## Master script for phenology analysis # ## Load required functions # if(file.exists(&quot;01_download.phenocam.R&quot;)) source(&quot;01_download.phenocam.R&quot;) # if(file.exists(&quot;02_plot.phenocam.R&quot;)) source(&quot;02_plot.phenocam.R&quot;) # if(file.exists(&quot;03_logistic.R&quot;)) source(&quot;03_logistic.R&quot;) # ## Download phenology data # URL = &quot;https://phenocam.sr.unh.edu/data/archive/uiefprairie2/ROI/uiefprairie2_AG_1000_1day.csv&quot; # prairie.pheno &lt;- download.phenocam(URL) # ## Plot overall phenology data # plot.phenocam(prairie.pheno) # ## Create and visualize subset of data for leaf out # spring = as.Date(c(&quot;2019-01-01&quot;,&quot;2019-06-01&quot;)) # dat = subset(prairie.pheno,date &gt; spring[1] &amp; date &lt; spring[2], select=c(date,gcc_mean,gcc_std)) # plot.phenocam(dat) # ## Fit logistic model # dat$doy = as.POSIXlt(dat$date)$yday # par = c(0.3,0.11,-10,0.1) # fit = fit.logistic(dat,par) # ## Visualize model and data # plot.phenocam(dat) # lines(dat$date,pred.logistic(fit$par,dat$doy),col=2) Save this file as “04_Master.Rmd.” Within RStudio’s Git tab, add the file and Commit. Use the Push (up arrow) button to push this to your own repository On Github.com, submit a pull request 6.9.2 OWNER Evaluate and accept pull request. At this point your workflow should be complete and you should be able to run the analysis. References "],["baysian.html", "7 Going Bayesian 7.1 Maximum likelihood", " 7 Going Bayesian While ecological forecasting and decision support can be done with traditional statistics (often termed “frequentist statistics”) it is generally much easier to do in a Bayesian statistical framework, because: It treats all terms as probability distributions, making it easier to quantify, propagate and partition uncertainties throughout the analysis (more on this later) Bayesian analyses are inherently iterative, making it easier to update predictions as new data become available They are highly flexible, allowing one to build relatively complex models with varied data sources (and/or of varying quality) They are typically focused on estimating what properties are (i.e. the actual value of a particular parameter) and not just establishing what they are not (i.e. testing for significant difference, as is usually the focus in frequentist statistics) - although there are frequentist approaches for doing this too In this section I aim to provide a brief and soft introduction to Bayesian statistics and use an example from my own research to highlight the principles and benefits. We’ll also use this example in the pair-coding practical. 7.1 Maximum likelihood Before I can introduce Bayes, there are a few basic building blocks we need to establish first. The main one is the concept of likelihood and the estimation of maximum likelihood, since this is a major component of Bayes’ Theorum. 7.1.1 Probability vs Likelihood While probability and likelihood may seem like similar concepts, the distinction between them is fundamentally important in statistics. Probability relates to possible results given a hypothesis. e.g. if you have an unbias coin (the hypothesis), the probability of a coin toss landing heads up (the result) is 0.5. Likelihood relates to hypotheses, given the results (i.e. data). e.g. say we’ve performed 1000 tosses and the coin landed heads up 700 times (the data), it is less likely that the hypothesis of an unbias coin is true relative to a hypothesis that the coin favours heads. This may seem like a subtle and potentially arbitrary distinction, but likelihood comes into it’s own when comparing multiple hypotheses (usually specified as models). Add Postfire example demonstrate p-value by fitting lm to NDVI demonstrate fitting with MLE fit negative exponential and compare likelihoods Figure 7.1: Time-series of 16-day normalized difference vegetation index (NDVI) from the MODIS satellite mission following fire in a seasonal wetland in the Silvermine section of Table Mountain National Park. Figure 7.2: ‘Frequentists vs Bayesians’ from xkcd.com/1132, used under a CC-BY-NC 2.5 license. Figure 7.3: ‘Seashell’ from xkcd.com/1236, used under a CC-BY-NC 2.5 license. Figure 7.4: ‘Modified Baye’s Theorem’ from xkcd.com/2059, used under a CC-BY-NC 2.5 license. \\(p\\) is unknown but expected to be around 1/3. Standard error will be approximated \\[ SE = \\sqrt(\\frac{p(1-p)}{n}) \\approx \\sqrt{\\frac{1/3 (1 - 1/3)} {300}} = 0.027 \\] You can also use math in footnotes like this2. We will approximate standard error to 0.0273 where we mention \\(p = \\frac{a}{b}\\)↩︎ \\(p\\) is unknown but expected to be around 1/3. Standard error will be approximated \\[ SE = \\sqrt(\\frac{p(1-p)}{n}) \\approx \\sqrt{\\frac{1/3 (1 - 1/3)} {300}} = 0.027 \\]↩︎ "],["uncertainty.html", "8 Uncertainty", " 8 Uncertainty We have finished a nice book. "],["references.html", "References", " References "]]

[["reproducibility.html", "4 Reproducible research 4.1 Replication and the Reproducibility Spectrum 4.2 Why work reproducibly? 4.3 Scientific Workflows 4.4 Data Management 4.5 Coding and code management 4.6 Computing and software", " 4 Reproducible research Before we continue, it’s worth highlighting the similarity between the iterative decision making cycle I’ve outlined in figure ?? and the scientific method, i.e.: Observation &gt; Hypothesis &gt; Experiment &gt; Analyze &gt; Interpret &gt; Report &gt; (Repeat) Figure 4.1: The Scientific Method overlain on iterative decision making. It’s also worth noting that replication is one of the fundamental tenets of science. In other words, published research should be robust enough and the methods described in enough detail that anyone else should be able to repeat the study (using the publication only) and find similar results. Sadly, this is rarely the case!!! 4.1 Replication and the Reproducibility Spectrum “Replication is the ultimate standard by which scientific claims are judged.” (Peng 2011) If the results of a study or experiment cannot by replicated by an independent set of investigators then whatever scientific claims were made should be treated with caution! At best, it suggests that evidence for the claim is weak or mixed, or specific to particular ecosystems or other circumstances and cannot be generalized. At worst, there was error (or even dishonesty) in the original study and the claims were plainly false. Unfortunately, some studies may not be entirely replicable purely due to the nature of the data or phenomenon (e.g. rare phenomena, long term records, loss of species or ecosystems, or very expensive once-off science projects like space missions). In these cases the “gold standard” of full replication (from new data collection to results) cannot be achieved, and we have to settle for a lower rung on the reproducibility spectrum (Figure 4.2). Figure 4.2: The Reproducibility Spectrum (Peng 2011). Reproducibility falls short of full replication because it focuses on reproducing the same result from the same data set, rather than analyzing independently collected data. While this may seem trivial, you’d be surprised at how few studies are even reproducible, let alone replicable. Figure 4.3: ‘Is there a reproducibility* crisis?’ Results from a survey of &gt;1500 top scientists (Baker 2016; Penny 2016). *Note that they did not discern between reproducibility and replicability, and that the terms are often used interchangeably. We clearly have a problem… Working reproducibly is not just a requirement for using quantitative approaches in iterative decision-making, it is central to scientific progress!!! While full replication is a huge challenge (and sometimes impossible) to achieve, it is something all scientists should be working towards. 4.2 Why work reproducibly? Figure 4.4: Let’s start being more specific about our miracles… Cartoon © Sidney Harris. Used with permission ScienceCartoonsPlus.com In addition to basic scientific rigour, working reproducibly is hugely valuable, because: (Adapted from “Five selfish reasons to work reproducibly” (Markowetz 2015)) It helps us avoid mistakes and/or track down errors in analyses This is what highlighted the importance of working reproducibly for me. In 2017 I published the first evidence of observed climate change impacts on biodiversity in the Fynbos Biome (Slingsby et al. 2017). The analyses were quite complicated, and when working on the revisions I found an error in my R code. Fortunately, it didn’t change the results qualitatively, but it made me realize how easy it is to make a mistake and potentially put the wrong message out there! This encouraged me to make all data and R code from the paper available, so that anyone is free to check my data and analyses and let me (and/or the world) know if they find any errors. It makes it easier to write papers e.g. Dynamic documents like RMarkdown or Jupyter Notebooks update automatically when you change your analyses, so you don’t have to copy/paste or save/insert all tables and figures (or worry about whether you included the latest versions. It helps the review process Often issues picked at by reviewers are matters of clarity/confusion. Sharing your data and analyses allows them to see exactly what you did, not just what you said you did, allowing them to identify the problem and make constructive suggestions. It’s also handy to be able to respond to a reviewer’s comment with something like “That’s a great suggestion, but not really in line with the objectives of the study. We have chosen not to include the suggested analysis, but do provide all data and code so that interested readers can explore this for themselves.” (Feel free to copy and paste - CCO 1.0) It enables continuity of the research When people leave a project (e.g. students/postdocs), or you forget what you did X days/weeks/months/years ago, it can be a serious setback for a project and make it difficult for you or a new student to pick up where things left off. If the data and workflow are well curated and documented this problem is avoided. Trust me, this is a very common problem!!! I have many papers that I (or my students) never published and may never go back to, because I know it’ll take me a few days or weeks to understand the datasets and analyses again… This is obviously incredibly important for long-term projects!!! It helps to build your reputation Working reproducibly makes it clear you’re an honest, open, careful and transparent researcher, and should errors be found in your work you’re unlikely to be accused of dishonesty (e.g. see my paper example under point 1 - although no one has told me of any errors yet…). When others reuse your data, code, etc you’re likely to get credit for it - either just informally, or formally through citations or acknowledgements (depending on the licensing conditions you specify - see “Preserve” in the Data Life Cycle). And some less selfish reasons (and relevant for ecoforecasting): It allows you (or others) to rapidly build on previous findings and analyses It allows easy comparison of new analytical approaches to older ones It makes it easy to repeat the same analyses when new data are collected or added And one more selfish reason (but don’t tell anyone I said this): Should you decide to leave biology, reproducible research skills are highly sought after in other careers like data science etc… 4.3 Scientific Workflows Figure 4.5: ‘Data Pipeline’ from xkcd.com/2054, used under a CC-BY-NC 2.5 license. Working reproducibly requires careful planning and documentation of each step in your scientific workflow from planning your data collection to sharing your results. This entails a number of overlapping/intertwined components, namely: Data management Coding and code management (data analysis) Computing and software For the rest of this section we’ll work through these components and some of the tools that help you achieve this. 4.4 Data Management 4.4.1 Why do you need to manage your data? Data management is often the last thing on a scientists mind when doing a new study - “I have a cool idea, and I’m going to test it!” You don’t want to “waste” time planning how you’re going to manage your data and implementing that plan… Unfortunately, this never ends well and really is a realm where “haste makes waste.” Figure 4.6: The ‘Data Decay Curve’ (Michener et al. 1997) Bad data management leads to data loss… (The “data decay curve” (Michener et al. 1997)) Your future self will hate you if you lose it before you’re finished with it - Eek!!! This is less likely in the world of Dropbox, Google Drive, iCloud etc, but I know people who had to repeat their PhD’s because they lost their data or it was on a laptop that was stolen… Data has value beyond your current project: to yourself for reuse in future projects, collaborations, etc (i.e. publications and citations), for others for follow-up studies, or combining multiple datasets for meta-analyses or synthesis etc for science on general (especially long-term ecology in a time of global change) We’ve covered this before, but it is also key for transparency and accountability. Data collection is expensive, and is often paid for with taxpayers’ money. You owe it to them to make sure that science gets the most out of that data in the long term. Lastly, good planning and data management can help iron out issues like intellectual property, permissions for ethics, collection permits, etc, in addition to outlining expectations for who will be authors on the paper(s), responsible for managing different aspects of the data etc up front. If you don’t establish these permissions and ground rules early they can result in data loss, not being able to publish the study, damage careers, and/or damage relationships in collaborations (including student-supervisor), etc. To avoid data (and relationship) decay, and to reap the benefits of good data management, it is important to consider the full Data Life Cycle. 4.4.2 The Data Life Cycle Figure 4.7: The Data Life Cycle, adapted from https://www.dataone.org/ Note that there are quite a few different versions of the data life cycle out there. This is the most comprehensive one I know of, and covers all the steps relevant to a range of different kinds of research projects. A full description of this data life cycle and related ecoinformatics issues can be found in (Michener and Jones 2012). Not all projects need to do all steps, nor will they necessarily follow the order here, but it is worth being aware of and considering all steps. For example, often the first thing you do when you have a new hypothesis is search around for any existing data that could be used to test it without having to spend money and time collecting new data (i.e. skip to step 6). In this case I would argue that you should still do step 1 (Plan), and you’d want to do some checking to assure the quality of the data (step 3), but you can certainly skip steps 2, 4 and 5. A meta-analysis or synthesis paper would probably do the same. On the other hand, if you’re collecting new data you would do steps 1 to 5 and possibly skip 6 and 7, although in my experience few studies do not reuse existing data (e.g. weather or various GIS data to put your new samples in context). 4.4.2.1 Plan Good data management begins with planning. In this step you essentially outline the plan for every step of the cycle in as much detail as possible. Usually this is done by constructing a document or Data Management Plan (DMP). While developing DMPs can seem tedious, they are essential: for the reasons I gave above because most funders and universities now require them Fortunately, there are a number of online data management planning tools that make it easy by providing templates and prompts to ensure that you cover all the bases, including UCT’s DMP Tool. Figure 4.8: Screenshot of UCT’s Data Management Planning Tool’s Data Management Checklist. A key thing to bear in mind is that a DMP is a living document and should be regularly revised during the life of a project, especially when big changes happen - e.g. new team members, new funding, new direction, change of institution, etc. I typically develop one overarching DMP for an umbrella project (e.g. a particular grant), but then add specifics for subprojects (e.g. separate student projects etc). 4.4.2.2 Collect and Assure There are many, many different kinds of data that can be collected in a vast number of ways! Figure 4.9: Springer Nature Infographic illustrating the vast range of research data types. While “Collect” and “Assure” are different steps in the life cycle, I advocate that it is foolish to collect data without doing quality assurance and quality control (QA/QC) as you go, irrespective of how you are collecting the data. For example: automated logging instruments (weather stations, cameras, acoustic recorders) need to be checked that they’re logging properly, are calibrated/focused, are reporting sensible values, etc if you’re filling in data sheets, you need to check that all fields have been completed, that there are no obvious errors and that any numbers or other values look realistic. In fact, if you’re using handwritten data sheets it’s best to capture them as soon as possible (i.e. that evening), because that helps you spot errors and omissions, you have a better chance of deciphering bad handwriting or cryptic notes, and you can plot any values to see if there are suspicious outliers (e.g. because someone wrote down a measurement in centimetres when they were meant to use metres). When transcribing or capturing data into a spreadsheet or database it is often best to use data validation tricks like drop-down menus, conditional formatting, restricted value ranges etc to avoid spelling mistakes and highlight data entries that are outside the expected range of the data field. It may seem like a lot of effort to set this up, but it’ll save you a lot of time and pain in the long run!!! Increasingly, I’ve started moving towards capturing data directly into a spreadsheet on a tablet, or better yet, into an App on my phone. There are a number of “no code” app builders these days like AppSheet that inserts data directly into Google Sheets and saves photos to your Google Drive. Figure 4.10: An example data collection app I built in AppSheet that allows you to log GPS coordinates, take photos, record various fields, etc. 4.4.2.3 Describe Metadata… who created, collected and managed the data the data content and format when the data were collected where the data were collected and stored how the data were generated, processed, assured and analyzed the study context - i.e. why the data were generated 4.4.2.4 Preserve How Toy Story 2 Almost Got Deleted 4.4.2.5 Discover 4.4.2.6 Integrate 4.4.2.7 Analyze 4.5 Coding and code management Why write code Some rules of coding Version control 4.6 Computing and software Why use open source software Containers References "]]

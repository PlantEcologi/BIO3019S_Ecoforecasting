[["index.html", "A Minimal Introduction to Ecological Forecasting and Reproducible Research 1 Overview 1.1 General 1.2 Lectures and practicals 1.3 Preparation 1.4 A bit about me 1.5 Acknowledgements and further reading:", " A Minimal Introduction to Ecological Forecasting and Reproducible Research Jasper Slingsby 2021-09-28 1 Overview This module is a minimal introduction to Ecological Forecasting and Reproducible Research for the 3rd year undergraduate Biological Sciences BIO3019S class at the University of Cape Town. 1.1 General I provide a very brief introduction to the framework for Ecological Forecasting. We only have a two weeks, so this really is a minimalist introduction. I’ll focus on providing a broad overview of the general framework and motivation for ecological forecasting, but won’t have time to delve into the more gory theoretical and statistical details. I mostly use Ecological Forecasting as a framework to highlight various themes and principles that are increasingly important for quantitative biologists - understanding how we inform or make decisions; managing data; working reproducibly; propagating, understanding and reducing uncertainty, etc. Unfortunately, not all of this is fun and exciting, but as I said, it is important stuff for quantitative biologists to know. I’ll try my best to make it interesting! Hopefully by the end of the module you’ll see the value in it all - both for you as an individual and for science and society in general. The core outcomes/concepts I hope you’ll come away with: To be able to situate the role of models and the importance of forecasting in science and ecological decision making Familiarity with the concepts and understand the need for Open, Reproducible Science Familiarity with The Data Life Cycle Familiarity with value and flexibility of Bayesian statistical methods 1.2 Lectures and practicals Lectures Lectures will be held live, but online 12:00 - 12:45 from the 30th September to the 13th October. The zoom link (see Vula) should stay the same for all lecture sessions. Practicals There is only one practical for this module, 2-5PM on Tuesday the 5th October. 1.3 Preparation For the lecture content: The following 4 minute video will give yo a glossy overview of what most of this module is about You are expected to read (Dietze et al. 2018) for Monday the 4th Octtober. You can download it here. For the practical: You need to install and set up RStudio and Github and test your setup. You can find the 11-step instructions in section 6.1. This may be a bit tedious, but there’s no other option really. I’ve done my best to make it as painless as possible. It should take you about an hour if all goes well… (less if you have R and RStudio installed already). PLEASE DO THE SETUP THIS WEEK!!! I will check in on Monday to see if people are having issues… 1.4 A bit about me I’m an ecologist who has become more quantitative through time, but has little formal training in quantitative methods (i.e. I’ve learnt by doing over the past 20 years). As such, I will make elementary mistakes. In fact, the entry requirements for this course are beyond my formal training, so you may have much to teach me! If you spot any errors, confusion or contradictions, please let me know and I’ll get back to you and/or update the course notes accordingly. Hopefully by the end of the course you can suggest changes directly using pull requests to the GitHub repository for the course notes. 1.5 Acknowledgements and further reading: Many of the following resources were instrumental in me pulling this material together and are worth spending time exploring ecoforecast.org Dietze, Michael C. 2017. Ecological Forecasting. Princeton University Press. https://doi.org/10.2307/j.ctvc7796h. Dietze, Michael C. et al. 2018. “Iterative near-Term Ecological Forecasting: Needs, Opportunities, and Challenges.” Proceedings of the National Academy of Sciences of the United States of America 115 (7): 1424–32. https://doi.org/10.1073/pnas.1710231115. All code, images, etc can be found here. I have only used images and other materials that were made available online under a non-restrictive license (Creative Commons, etc) or for which I have express permission, and have attributed my sources. Content without attribution is my own and shared under the license below. If there are any errors or any content you find concerning with regard to licensing, or that you find offensive, please contact me. Any feedback, positive or negative, is welcome! This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License. References "],["quiz.html", "2 How do we make decisions?", " 2 How do we make decisions? What factors do you consider when making a decision? "],["models.html", "3 Models and decision making 3.1 The basics of making a decision 3.2 Getting quantitative 3.3 Iterative decision-making and the scientific method 3.4 The importance of prediction in ecology 3.5 Near-term iterative ecological forecasting 3.6 Reproducible research", " 3 Models and decision making 3.1 The basics of making a decision Informing decisions typically requires knowing (or guessing at) something about the future. To this end, once a problem and the need to make a decision have been identified, the factors we consider when making that decision usually include: Evidence Experience Expectation Uncertainty The relationship between these can be represented like so: Figure 3.1: The factors considered when making a decision. Your decision is typically based on your expectation of the outcome. This expectation is based on existing evidence and/or previous experience. Uncertainty is a part of each step. There are a number of reasons why the existing evidence or previous experience may be imperfect for the decision at hand, leading to uncertainty in the expectations. There may also be uncertainty in the way in which you use the evidence and experience to develop your expectation. We’ll come back to these sources of uncertainty later in the module, but needless to say, quantifying and understanding the uncertainty is crucial in any decision. If uncertainty is high your expectation may be no better than random, and thus useless for informing your decision. Quantifying uncertainty properly helps us circumvent two evils which could mislead decision makers: being falsely overconfident in our predictions (potentially leading to a “wrong” decision), or being falsely uncertain in our predictions (which would encourage overly conservative decisions/actions which may be wasteful or less effective). Lastly, ignoring or quantifying uncertainty incorrectly can lead to bias predictions. 3.2 Getting quantitative The nice thing about the framework above is that it is similar whether you are approaching the decision qualitatively or quantitatively (i.e. using models and data to inform your decision). Figure 3.2: Using models and data when making a decision. Following a quantitative approach the “evidence” is typically empirical data, which can be fed into a model to make forecasts that can help inform the decision. The “experience” are the current state of knowledge and your prior belief, which you use to specify the type and structure of your model (or ensemble of models) and the scenario(s) you want to evaluate. The “experience” can also help you evaluate the assumptions of your model(s), and if you are using a Bayesian model, can be included directly in the model when specifying priors (more on this later in the module). 3.3 Iterative decision-making and the scientific method Few decisions in natural resource management are once-off, and most are made repeatedly at some time-step (e.g. daily, monthly, seasonally, annually, decadally, etc). Should you burn, cull, harvest, restore, etc? While one should always evaluate the outcome of your decision, this is especially important when the decision will need to be repeated, so that you can learn from experience. Figure 3.3: Iterative decision making. When using quantitative forecasts this can be done by collecting new data and updating your prior knowledge by evaluating the outcomes of the decision against the original model forecasts. This can tell you whether your forecast was any good and whether you need to refine or replace your model, consider additional scenarios or inputs, etc. We’ll discuss doing this quantitatively in section ??, by fusing your new data and knowledge into a new forecast. It’s worth highlighting the similarity between the iterative decision making cycle I’ve outlined in Figure 3.3 and the scientific method, i.e.: Observation &gt; Hypothesis &gt; Experiment &gt; Analyze &gt; Interpret &gt; Report &gt; (Repeat) Figure 3.4: The Scientific Method overlain on iterative decision making. So a focus on iterative decision-making facilitates iterative learning (i.e. scientific progress). 3.4 The importance of prediction in ecology “prediction is the only way to demonstrate scientific understanding” (Houlahan et al. 2017) While this view may be slightly overstated, it is a very good point. If we cannot make reasonably good predictions, we’re missing something. Unfortunately, prediction has not been a central focus in ecology, impeding progress in the improvement of our ecological understanding. To make predictions we need models, and models provide structured summaries of our current ecological understanding (conceptual or quantitative, but preferably quantitative, because these are easier to compare). Without making predictions and comparing the skill of new models to old ones, we can’t track if we are making progress! Figure 3.5: Prediction… from xkcd.com/2370, used under a CC-BY-NC 2.5 license. In ecology we mostly test qualitative, imprecise hypotheses: “Does X have an effect on Y?” rather than “What is the relationship between X and Y?” or better yet “What value would we expect Y to be, given a particular value of X?”. Without testing precise hypotheses and using the results to make testable predictions we don’t know if our findings are generalizable beyond the specific data set we collected. If our results are not generalizable, then we’re not really making progress towards a better understanding of ecology. A key point here is that the predictions must be testable! We do use a lot of models in ecology, and even use them to make predictions (e.g. species distribution models (SDMs), dynamic vegetation models (DVMs), etc), but these predictions are typically 50+ years into the future, which is way to long to wait to see if our predictions were reasonable or useful. A quick aside on model validation vs testing predictions: Testing predictions with new data collected after you’ve made your predictions is the most robust way to validate a model, but you usually want to do some form of validation before you make your final predictionst o make sure the model is working reasonably well. For this we most commonly do some form of cross-validation, whereby we split the data into a “training” subset (that we use for fitting (or training) the model) and a “test” subset (that we try to predict). If your model is no good at predicting your test data, there’s probably no point in making predictions into the future… 3.5 Near-term iterative ecological forecasting The recent growth in interest in iterative ecological forecasting seeks to not only make prediction a central focus in ecology, but to do so on a time scale that is both useful for decision makers and allows us to learn from testing our predictions (days to decades). This is a great initiative, but as we will see it poses a number of major challenges and requires a big improvement in quantitative skills in biology (hence this course…). Figure 3.6: The iterative ecological forecasting cycle in the context of the scientific method and the adaptive management and monitoring cycles (Dietze et al. 2018). The iterative ecological forecasting cycle is tightly aligned to the scientific method cycle: Hypotheses (A) are embedded in models (B). The models integrate over uncertainties in initial conditions (IC), inputs, and parameters to make probabilistic forecasts (the purple distributions, Fx, in step C), sometimes for multiple alternative scenarios. New observations are then compared with these predictions (D) to update estimates of the current state of the system (Analysis) and assess model performance (E), allowing for the selection among alternative model hypotheses (Test and Refine). The iterative forecasting cycle also feeds into adaptive management and monitoring: In Adaptive Management and decision analysis, alternative decision scenarios are generated (2) based on an assessment of a problem (1). These decision scenarios are typically used to define the scenarios (or boundary conditions) for which models are run (“Scenarios” arrow), but can also feed into scientific hypotheses (not shown). Forecasts (Fx) are key in assessing the trade-offs and relative merits between alternative decision options (3). The decision(s) taken (4) determine the monitoring requirements (5), which allow us to evaluate the outcomes and reassess the problem (1), and start the adaptive management cycle again. Note that the iterative forecast cycle is also useful for adaptive management in that the analysis and partitioning of forecast uncertainties (from step C) can provide further guidance on what and how to monitor, so as to optimize the reduction in model uncertainties. This represents Adaptive Monitoring (dashed line) and is a cycle of itself (Lindenmayer and Likens 2009), but is largely subsumed by the other cycles here so we won’t go into it any further here. Thus the iterative cycles of science, forecasting, management and monitoring are tightly intertwined and can interact continuously. 3.6 Reproducible research What isn’t clear from Figure 3.6 is that all of this needs to be founded on a highly efficient informatics pipeline that is robust and rapidly updateable. Since the emphasis here is on near-term forecasts to inform management, if the process of adding new data and updating the forecasts is too slow, the value of the forecasts is lost. As we’ll see in future lectures, the best way to build a highly efficient informatics pipeline is to follow reproducible research principles (section 5), including good (and rapid) data management (section 7). Adding this link to Figure 3.6 helps to highlight what I like to think of as “The Olympian Challenge of data-driven ecological decision making”. Figure 3.7: The Olympian Challenge of data-driven ecological decision making. Working reproducibly requires learning a lot of skills and can take a lot of effort, but is well worth it beyond it’s utility for ecological forecasting - for you as an individual, and for science in general. This is why I decided to make it part of the title for the module and the focus of at least two lectures and the practical. References "],["forecasts.html", "4 Making forecasts 4.1 Protea demography 4.2 Post-fire recovery trajectories 4.3 Impacts of IAPs on runoff", " 4 Making forecasts We’ve got this far, and yet you haven’t stopped me and asked “What are ecological forecasts?” or “Where are we going with all this?” The focus of this section is to provide some context with examples of what I believe are some forecasting needs or opportunities in the Fynbos Biome. These examples should hopefully also help provide practical context for some of the issues we’ll address in the rest of the module. Note: These examples have not yet been developed into full near-term iterative ecological forecasts sensu Dietze et al (2018). I also don’t think they necessarily have to get all the way there to be useful. Think of it as an “ecological forecasting spectrum” where the gold standard is fully developed and automated near-term iterative ecological forecasts. 4.1 Protea demography The demography of species in the Proteaceae is used for the management of Fynbos in two ways: Firstly, at the ecosystem level, to help determine acceptable fire return intervals Secondly, at the species level, for setting guidelines for sustainable wild harvesting of Proteaceae inflorescences The management guidelines for these two use cases are currently set by “rule of thumb”1: “No fire should be permitted in fynbos until at least 50% of the population of the slowest-maturing species in an area have flowered for at least three successive seasons (or at least 90% of the individuals of the slowest maturing species in the area have flowered and produced seed). Similarly, a fire is probably not necessary unless a third or more of the plants of these slow-maturing species are senescent (i.e. dying or no longer producing flowers and seed).” (CapeNature, n.d.) “[There should be no] harvesting until at least 50% of the population had commenced flowering, a harvest of up to 50% of current season flower heads after this stage, and no harvesting at least one year prior to a prescribed burn” (Wilgen et al. 2016) Both these rules are based on the premise that the key to the persistence of Proteaceae populations is ensuring that there is a large enough seed bank present when a fire occurs for the population to recover. Do you foresee any issues with this premise? Figure 4.1: The fire-driven life-cycle of Fynbos Proteaceae species, including harvesting, taken from (Treurnicht et al. 2021). Population size/stability are determined by key demographic rates of adult fecundity (size of the canopy seed bank), post-fire seedling recruitment and adult fire survival (blue–grey boxes). These rates are affected in various ways by environmental conditions, density dependence, the timing, intensity and severity of fire, wildflower harvesting, etc 4.1.1 Demographic rates Based on extensive collection of locality and demographic data by conservation authorities (CapeNature and SANParks), citizen scientists (Protea Atlas Project and iNaturalist) and researchers, we know there are many issues with these rules of thumb, illustrated in Figures 4.2 and 4.3. Figure 4.2: Variation in demographic rates of 26 serotinous Proteaceae species of seeder and sprouter life-history types across their distribution range (Treurnicht et al. 2016). (a) Adult fire survival; (b) Individual fecundity (F); and (c) Per-capita recruitment rate (R). These issues include: species differ in their reliance on seed for their survival (e.g. sprouters vs seeders) sprouters have high persistence of adults through fires and need fewer new recruits from seed seeder adults are killed by fire, so populations depend entirely on recruitment from seed species vary in their fecundity (total number of seeds) fecundity = number of inflorescences produced multiplied by the number of seeds per inflorescence species vary in seed viability and recruitment success viability depends on pathogens, seed predators and other factors - many linked to the age of the seed or inflorescence seed-specific recruitment depends on viability and seed properties (size etc), conditions during the establishment phase (rainfall etc), finding suitable microsites, etc. per-capita recruitment is the combination of fecundity and seed-specific recruitment Figure 4.3: Intraspecific variation in (a) fecundity and (b) recruitment in response to range-wide variation in fire return interval (time since fire), adult population density and soil moisture stress (% days with soil moisture stress) for Protea punctata (Treurnicht et al. 2016). there is also intraspecific variation in fecundity and recruitment along climatic, soil, fire regime, population density, pollinator availability and other gradients and there is interspecific variation in this intraspecific variation i.e. species vary among populations in their response to climatic, soil, pollinator availability and other gradients 4.1.2 Projecting population viability A huge benefit of the herculean Proteaceae data collection (and management) effort is that it provides all the data we need to parameterize various types of models. In fact, data on the Proteaceae have been hugely important for the global development of species distribution and demographic models (see Schurr et al. (2012), but also many subsequent papers). In the context of the current discussion, the data have also been used for things like population viability analysis under varying harvesting rates (e.g. Treurnicht et al. 2021). Figure 4.4: Sensitivity to wildflower harvesting for various Proteaceae species (Treurnicht et al. 2021). Above: Intraspecific variation in sensitivity to harvesting depicted as maps for four different species with pink dots highlighting where the change in population-level extinction probability (the difference between extinction probabilities under 0% and 50% harvesting) is greater than 0.1. The white and black areas depict species-specific occurrence records and the geographical distribution of all Proteaceae in the Cape Floristic Region, respectively. Below: Interspecific variation in sensitivity to harvesting depicted as the proportion of populations per species that are highly vulnerable to harvesting. These models suggest that following the harvesting guidelines can greatly increase the probability of many populations going extinct (Figure 4.4; Treurnicht et al. (2021)), but can also be used to explore population viability under changing climate, fire regimes, etc (e.g. Merow et al. 2014) - or a combination of all of the above! 4.1.3 Near-term iterative ecological forecasts While the work by Merow et al. (2014), Treurnicht et al. (2021) and others represent ecological forecasts, they are usually either not specific about when they are forecasting to (e.g. Treurnicht et al. (2021) ran their models to estimate extinction probabilities over 100 years in response to different harvesting regimes), or they are too far into the future to be amenable to iterative assessment, learning and updating (e.g. Merow et al. (2014) project to 2050). To adapt these into near-term iterative ecological forecasts requires: Making the forecasts more near-term e.g. a range of scenarios 5-10 years into the future, or 1-2 years into the next fire cycle Setting up formalized data collection and management systems to feed data back into the modelling workflow i.e. coordinating and guiding the efforts of citizen scientists, conservation authorities, etc. Adapting the models and workflow to be able to ingest and assimilate new data and produce new forecasts automatically Make sure that the models adequately characterize and propagate uncertainty throughout the analyses Clearly, while huge effort has been invested into the demography of Proteaceae, and they are likely to be one of the lowest hanging fruit for development into near-term iterative ecological forecasts, there is a lot more work to be done! Still, the history of productive research on Proteaceae shows it would clearly be worth the effort, and we already have decades of data that allow us to learn by backcasting or forecasting from old to more recently collected data. Even if early forecasts are woefully wrong, they will help us learn and improve. “The need to start forecasting is now; the time for making ecology more predictive is here, and learning by doing is the fastest route to drive the science forward.” - Dietze et al. (2018) And some specific to fire management The fire requirements for Proteaceae are not necessarily representative of all Fynbos species (including animals!) What do you do in areas where there are no Proteaceae? 4.2 Post-fire recovery trajectories Wilson paper FRI diagram link to Protea dem models Change detection… emma.eco Developing into a state-space model with near-real time inputs? 4.3 Impacts of IAPs on runoff References "],["reproducibility.html", "5 Reproducible research 5.1 The Reproducibility Crisis 5.2 Replication and the Reproducibility Spectrum 5.3 Why work reproducibly? 5.4 Barriers to working reproducibly 5.5 Reproducible Scientific Workflows", " 5 Reproducible research 5.1 The Reproducibility Crisis “Replication is the ultimate standard by which scientific claims are judged.” (Peng 2011) Replication is one of the fundamental tenets of science and if the results of a study or experiment cannot by replicated by an independent set of investigators then whatever scientific claims were made should be treated with caution! At best, it suggests that evidence for the claim is weak or mixed, or specific to particular ecosystems or other circumstances and cannot be generalized. At worst, there was error (or even dishonesty) in the original study and the claims were plainly false. In other words, published research should be robust enough and the methods described in enough detail that anyone else should be able to repeat the study (using the publication only) and find similar results. Sadly, this is rarely the case!!! Figure 5.1: ‘Is there a reproducibility* crisis?’ Results from a survey of &gt;1500 top scientists (Baker 2016; Penny 2016). *Note that they did not discern between reproducibility and replicability, and that the terms are often used interchangeably. We have a problem… Since we’re failing the gentleman’s agreement2 that we’ll describe our methods in enough detail that anyone else should be able to repeat the study (using the publication only) and find similar results, modern scientists are trying to formalize the process in the form of Reproducible Research. Reproducible research makes use of modern software tools to share data, code and other resources required to allow others to reproduce the same result as the original study, thus making all analyses open and transparent. As you will learn in this module, working reproducibly is not just a requirement for using quantitative approaches in iterative decision-making, it is central to scientific progress!!! While full replication is a huge challenge (and sometimes impossible) to achieve, it is something all scientists should be working towards. 5.2 Replication and the Reproducibility Spectrum Understandably, some studies may not be entirely replicable purely due to the nature of the data or phenomenon (e.g. rare phenomena, long term records, loss of species or ecosystems, or very expensive once-off science projects like space missions). In these cases the “gold standard” of full replication (from new data collection to results) cannot be achieved, and we have to settle for a lower rung on the reproducibility spectrum (Figure 5.2). Figure 5.2: The Reproducibility Spectrum (Peng 2011). Reproducibility falls short of full replication because it focuses on reproducing the same result from the same data set, rather than analyzing independently collected data. While this may seem trivial, you’d be surprised at how few studies are even reproducible, let alone replicable. 5.3 Why work reproducibly? Figure 5.3: Let’s start being more specific about our miracles… Cartoon © Sidney Harris. Used with permission ScienceCartoonsPlus.com In addition to basic scientific rigour, working reproducibly is hugely valuable, because: (Adapted from “Five selfish reasons to work reproducibly” (Markowetz 2015)) It helps us avoid mistakes and/or track down errors in analyses This is what highlighted the importance of working reproducibly for me. In 2017 I published the first evidence of observed climate change impacts on biodiversity in the Fynbos Biome (Slingsby et al. 2017). The analyses were quite complicated, and when working on the revisions I found an error in my R code. Fortunately, it didn’t change the results qualitatively, but it made me realize how easy it is to make a mistake and potentially put the wrong message out there! This encouraged me to make all data and R code from the paper available, so that anyone is free to check my data and analyses and let me (and/or the world) know if they find any errors. It makes it easier to write papers e.g. Dynamic documents like RMarkdown or Jupyter Notebooks update automatically when you change your analyses, so you don’t have to copy/paste or save/insert all tables and figures (or worry about whether you included the latest versions. It helps the review process Often issues picked at by reviewers are matters of clarity/confusion. Sharing your data and analyses allows them to see exactly what you did, not just what you said you did, allowing them to identify the problem and make constructive suggestions. It’s also handy to be able to respond to a reviewer’s comment with something like “That’s a great suggestion, but not really in line with the objectives of the study. We have chosen not to include the suggested analysis, but do provide all data and code so that interested readers can explore this for themselves.” (Feel free to copy and paste - CCO 1.0) It enables continuity of the research When people leave a project (e.g. students/postdocs), or you forget what you did X days/weeks/months/years ago, it can be a serious setback for a project and make it difficult for you or a new student to pick up where things left off. If the data and workflow are well curated and documented this problem is avoided. Trust me, this is a very common problem!!! I have many papers that I (or my students) never published and may never go back to, because I know it’ll take me a few days or weeks to understand the datasets and analyses again… This is obviously incredibly important for long-term projects!!! It helps to build your reputation Working reproducibly makes it clear you’re an honest, open, careful and transparent researcher, and should errors be found in your work you’re unlikely to be accused of dishonesty (e.g. see my paper example under point 1 - although no one has told me of any errors yet…). When others reuse your data, code, etc you’re likely to get credit for it - either just informally, or formally through citations or acknowledgements (depending on the licensing conditions you specify - see “Preserve” in the Data Life Cycle). And some less selfish reasons (and relevant for ecoforecasting): It allows you (or others) to rapidly build on previous findings and analyses It allows easy comparison of new analytical approaches to older ones It makes it easy to repeat the same analyses when new data are collected or added And one more selfish reason (but don’t tell anyone I said this): Should you decide to leave biology, reproducible research skills are highly sought after in other careers like data science etc… 5.4 Barriers to working reproducibly (Adapted from “A Beginner’s Guide to Conducting Reproducible Research” (Alston and Rick 2021)) 1. Complexity There can be a bit of a learning curve in getting to know and use the tools for reproducible research effectively. One is always tempted by the “easy option” of doing it the way you already know or using “user-friendly” proprietary software. 2. Technological change Hardware and software used in analyses change over time - either changing with updates or going obsolete altogether - making it very difficult to rerun old analyses. This should be less of a problem going forward because it is something people are aware of. Documenting hardware and software versions with analyses is an easy baseline. One can also use contained computing environments as we’ll discuss below. 3. Human error Simple mistakes or failure to fully document protocols or analyses can easily make a study irreproducible. Most reproducible research tools are aimed at solving this problem. 4. Intellectual property rights Rational self-interest can lead to hesitation to share data and code via many pathways: Fear of not getting credit; Concern that the materials shared will be used incorrectly or unethically; etc Hopefully most of these issues will be solved by better awareness of licensing issues, attribution, etc, as the culture of reproducible research grows 5.5 Reproducible Scientific Workflows Figure 5.4: ‘Data Pipeline’ from xkcd.com/2054, used under a CC-BY-NC 2.5 license. Working reproducibly requires careful planning and documentation of each step in your scientific workflow from planning your data collection to sharing your results. This entails a number of overlapping/intertwined components, namely: Data management - which we’ll spend more time on in Chapter 7 File and folder management Coding and code management - i.e. the data manipulation and analyses performed Computing environment and software Sharing of the data, metadata, code, publications and any other relevant materials For the rest of this section we’ll work through these components and some of the tools that help you achieve this. 5.5.1 File and folder management Project files and folders can get unwieldy fast, and can really bog you down and inhibit productivity when you don’t know where your files are or what the latest version is. Figure 5.5: ‘Documents’ from xkcd.com/1459, used under a CC-BY-NC 2.5 license. The two main considerations for addressing this issue are a) defining a simple, common folder structure, and b) using informative file names. Folders Most ecological projects have similar requirements. Here’s a screenshot of how I usually manage my folders. Within “Data” I often have separate folders of “Raw” and “Processed” data. “Output” contains figures and tables, often in separate folders. “Code” we’ll deal with in the next section. I also often have a “Manuscript” folder if I’m working in LaTeX/Sweave or RMarkdown, although this is often in the “Code” folder (since you can embed code in RMarkdown and Sweave documents). File and folder naming Your naming conventions should be: machine readable i.e. avoid spaces and funny punctuation support searching and splitting of names human readable the contents should be self evident from the file name support sorting i.e. use numeric or character prefixes to separate files into different components or steps (e.g. “data_raw_localities.csv,” “data_clean_localities.csv,” etc) some of this can be handled with folder structure, but you don’t want too many folders either Find out more about file naming here. 5.5.2 Coding and code management Why write code? Working in point-and-click GUI-based software like Excel, Statistica, SPSS, etc may seem easier, but you’ll regret it in the long run… The beauty of writing code lies in: Automation You will inevitably have to adjust and repeat your analysis as you get feedback from supervisors, collaborators and reviewers. Rerunning code is one click, and you’re unlikely to introduce errors. Rerunning analyses in GUI-based software is lots of clicks and it’s easy to make mistakes, alter default settings, etc etc. Next time you need to do the same analysis on a different dataset you can just copy, paste and tweak your code. You code/script provides a record of your analysis Linked to the above, mature scientific coding languages like Python or R allow you to run almost any kind of analysis in one scrpited workflow, even if it has diverse components like GIS, phylogenetics, multivariate or Bayesian statistics, etc. Most proprietary software are limited to one or a few specialized areas (e.g. ArcGIS, etc), which leaves you manually exporting and importing data between multiple software packages… Most scripting environments are open source (e.g. R, Python, JavaScript, etc) Anyone wanting to use your code doesn’t have to pay for a software license It’s great for transparency - Lots of people can and have checked the background code and functions you’re using, versus only the software owner’s employees have access to the raw code for most analytical software There’s usually a culture of sharing code (online forums, with publications, etc) Here’s a motivation and some tutorials to help you learn R. Some coding rules It’s easy to write messy code. This can make it virtually indecipherable to others (and even yourself), slowing you and your collaborations down. It also makes it easy to make mistakes and not notice them. The overarching rule is to write code for people, not computers. Some basic rules: use consistent, meaningful and distinct names for variables and functions use consistent code and formatting style use commenting to document and explain what you’re doing at each step or in each function - purpose, inputs and outputs write functions rather than repeating the same code check for mistakes at every step!!! modularize code into manageable steps/chunks or even separate them into separate scripts that can all be called in order from a master script or Makefile start with a “recipe” that outlines the steps/modules (usually as commented headers etc). This is very valuable for keeping you organized and on track, e.g. a common recipe: #Header indicating purpose, author, date, version etc #Define settings #Load required libraries #Read in data #Wrangle/reformat/clean/summarize data as required #Run analyses (often multiple steps) #Wrangle/reformat/summarize analysis outputs for visualization #Visualize outputs as figures or tables avoid proprietary formats i.e. use an open source scripting langauge and open source file formats only “notebooks” like RMarkdown or Jupyter Notebooks are very handy for fulfilling roles like documentation, master/makefiles etc and can be developed into reports or manuscripts use version control!!! Version control Using version control tools like Git, SVN, etc can be challenging at first, but they can also hugely simplify your code development (and adaptation) process. While they were designed by software developers for software development, they are hugely useful for quantitative biology. I can’t speak authoritatively on version control systems (I’ve only ever used Git and GitHub), but here are the advantages as I see them. This version is specific to Git, but I imagine they all have similar functions and functionality: Words in italics are technical terms used within GitHub. You can look them up here. You’ll also cover it in the brief tutorial you’ll do when setting up your computer for the practical. They generally help project management, especially collaborations They allow you to easily share code with collaborators or the public at large - through repositories or gists (code snippets) Users can easily adapt or build on each others’ code by forking repositories and working on their own branch. This is truly powerful!!! It allows you to repeat/replicate analyses but even build websites (like this one!), etc While the whole system is online, you can also work offline by cloning the repository to your local machine. Once you have a local version you can push to or pull from the online repository to keep everything updated Changes are tracked and reversible through commits. If you change the contents of a repository you must commit them and write a commit message before pulling or pushing to the online repository. Each commit is essentially a recoverable version that can be compared or reverted to This is the essence of version control and magically frees you from folders full of lists of files named “mycode_final.R,” “mycode_finalfinal.R,” “myfinalcode_finalfinal.R” etc as per Figure 5.5 They allow collaborators or the public at large to propose changes via pull requests that allow you to merge their forked branch back to the main (or master) branch They allow you to accept and integrate changes seamlessly when you accept and merge pull requests They allow you to keep written record of changes through comments whenever a commit or pull request is made - these also track the user, date, time, etc and are useful for blaming when things go wrong There’s a system for assigning logging and tracking issues and feature requests I’m sure this is all a bit much right now, but should make more sense after the practical… 5.5.3 Computing environment and software We’ve already covered why you should use open source software whenever possible, but it bears repeating. Using proprietary software means that others have to purchase software, licenses, etc to build on your work and essentially makes it not reproducible by putting it behind a pay-wall. This is self-defeating… Another issue is that software and hardware change with upgrades, new versions or changes in the preferences within user communities (e.g. you’ll all know MicroSoft Excel, but have you heard of Quattro Pro or Lotus that were the preferred spreadsheet software of yesteryear?). Just sharing your code, data and workflow does not make your work reproducible if we don’t know what language the code is written in or if functions change or are deprecated in newer versions, breaking your code. The simplest way to avert this problem is to carefully document the hardware and versions of software used in your analyses so that others can recreate that computing environment if needed. This is very easy in R, because you can simply run the sessionInfo() function, like so: sessionInfo() ## R version 4.1.1 (2021-08-10) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 20.04.3 LTS ## ## Matrix products: default ## BLAS: /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.9.0 ## LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.9.0 ## ## locale: ## [1] LC_CTYPE=en_ZA.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=en_ZA.UTF-8 LC_COLLATE=en_ZA.UTF-8 ## [5] LC_MONETARY=en_ZA.UTF-8 LC_MESSAGES=en_ZA.UTF-8 ## [7] LC_PAPER=en_ZA.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_ZA.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] forcats_0.5.1 stringr_1.4.0 dplyr_1.0.7 purrr_0.3.4 ## [5] readr_1.4.0 tidyr_1.1.3 tibble_3.1.3 ggplot2_3.3.5 ## [9] tidyverse_1.3.1 ## ## loaded via a namespace (and not attached): ## [1] tidyselect_1.1.1 xfun_0.24 bslib_0.2.5.1 ## [4] haven_2.4.1 colorspace_2.0-2 vctrs_0.3.8 ## [7] generics_0.1.0 htmltools_0.5.1.1 yaml_2.2.1 ## [10] utf8_1.2.2 rlang_0.4.11 jquerylib_0.1.4 ## [13] pillar_1.6.2 withr_2.4.2 glue_1.4.2 ## [16] DBI_1.1.1 RColorBrewer_1.1-2 dbplyr_2.1.1 ## [19] readxl_1.3.1 modelr_0.1.8 jpeg_0.1-9 ## [22] lifecycle_1.0.0 cellranger_1.1.0 munsell_0.5.0 ## [25] gtable_0.3.0 rvest_1.0.0 evaluate_0.14 ## [28] labeling_0.4.2 knitr_1.33 fansi_0.5.0 ## [31] highr_0.9 broom_0.7.7 Rcpp_1.0.6 ## [34] backports_1.2.1 scales_1.1.1 jsonlite_1.7.2 ## [37] farver_2.1.0 fs_1.5.0 hms_1.1.0 ## [40] png_0.1-7 digest_0.6.27 stringi_1.6.2 ## [43] bookdown_0.22 grid_4.1.1 cli_3.0.1 ## [46] tools_4.1.1 magrittr_2.0.1 sass_0.4.0 ## [49] crayon_1.4.1 pkgconfig_2.0.3 ellipsis_0.3.2 ## [52] xml2_1.3.2 reprex_2.0.0 lubridate_1.7.10 ## [55] assertthat_0.2.1 rmarkdown_2.8 httr_1.4.2 ## [58] rstudioapi_0.13 R6_2.5.0 compiler_4.1.1 A “better” way to do this is to use containers like docker or singularity. These are contained, lightweight computing environments similar to virtual machines, that you can package with your software/workflow. You set your container up to have everything you need to run your code etc (and nothing extra), so anyone can download (or clone) your container, code and data and run your analyses perfectly first time. 5.5.4 Sharing of the data, code, publication etc This is touched on in more detail when we discuss data management in Chapter 7, but suffice to say there’s no point working reproducibly if you’re not going to share all the components necessary to complete your workflow… Another key component here is that ideally all your data, code, publication etc are shared Open Access - i.e. they are not stuck behind some paywall Figure 5.6: A 3-step, 10-point checklist to guide researchers toward greater reproducibility in their research (Alston and Rick 2021). References "],["practical.html", "6 Practical: Pair coding with GitHub 6.1 Preparation 6.2 The practical", " 6 Practical: Pair coding with GitHub 6.1 Preparation For the practical, we’ll be using the R statistical programming language and the Git version control system. We’ll also be using an integrated development environment (IDE) for each: RStudio and GitHub, respectively. The installation and setup can be a bit long-winded, but once done you should be good to go until you change or reformat your computer. The steps below are my summary and (hopefully) more intuitive adaptation of the instructions provided for setting up GitHub and version control with R. If my steps don’t work its probably best to read up there. First we’ll start with the necessary software. Download and install the latest version of R Download and install the latest free version of RStudio Desktop Download and install the latest version of Git - accept all the defaults Then get started with GitHub: Create a GitHub account Run through the 10 minute GitHb tutorial that is offered when you activate your GitHub Account (It’ll really help you get the idea behind what Git does!) Now you have RStudio, R and Git installed, and you have a working GitHub account that lets you do stuff online, but what remains is to get GitHub working locally and configuring RStudio to use GitHub. Install GitHub CLI (Command Line Interface). For Windows you can download the installer here Open RStudio. Select the Terminal tab (top left, next to Console) Enter gh auth login, then follow the prompts: Select GitHub.com When prompted for your preferred protocol for Git operations, select HTTPS When asked if you would like to authenticate to Git with your GitHub credentials, enter Y When asked how you would like to authenticate select Login with web browser Copy the 8-digit code and hit Enter Github.com will open in your internet browser - paste the code and hit enter If any of these steps don’t work, just start with gh auth login in Terminal again In RStudio Go to Global Options (from the Tools menu) Click Git/SVN Make sure Enable version control interface for RStudio projects is on If necessary, enter the path for your Git or SVN executable where provided Click Apply Restart RStudio Ok, now everything should be working. The next steps are to fork and clone your first repo to see if everything is working, and then to modify a file in RStudio, push it back to your forked repo, and then create a pull request for me to review and accept your changes. This let’s me know that you’ve made it through the preparation for the practical, and it gives me your GitHub username. In GitHub: Make sure you are logged in, search and navigate to JasperUCT/pullltest Click Fork, which will make a copy of the repository to your own workspace Copy the URL to your own version and follow the instructions below for cloning the repository in RStudio In RStudio: In the top-right of the window you’ll see Project: (None) - click on it and select New Project then Version Control then Git In the Repository URl: box paste the URL to your forked repo (It should look something like : https://github.com/YourGitHubUsername/your-forked-repo_name.git Project directory name should fill automatically For Create project as subdirectory of: hit Browse and navigate through your file system to where you want to put your folder for storing your Git repositories stored. I recommend something like ~Documents/GIT (If you’ve used Git before you may have set this already and can skip this step) Click Create Repository Your RStudio window should now look like this: Figure 6.1: What you should see… Note there are three files in the Files tab in the bottom-right window, and you should see a Git tab for the top-right window. It also says pulltest in the project drop-down top-right on mine, but yours will display the name you gave your forked repo. Open the README.md file in RStudio Add your message and save the file Select the Git tab in the top-right window Check the box next to README.md and click Commit Add a Commit message to say what changes you’ve made Then hit Push It will ask you to authenticate. Select Authenticate in your web browser. The web browser will ask you to give Git permissions. Allow the permissions and it should work. If you get an error at this point to the effect of “You do not have permission to push to this repository,” then you may have forgotten to fork your own repo from my one and are trying to push to mine… If so, start again from step 9. Final step!!! In GitHub Click Pull requests (top-leftish) Click New pull request (green, top-right) Click Create pull request (green, top-right) It should already be on this repo unless you’ve been doing other things in Git In the comment window put your name so I know who you are if it isn’t obvious from your GitHub username Click Create pull request (green, bottom-right) And you’re done!!! I’ll get a notification of your pull request and, if all’s in order, I’ll accept it. If not, I’ll reply with a comment on the pull request. You should receive a notification via the email you registered with your GitHub account. 6.2 The practical Just a quick acknowledgement that I have borrowed and adapted much of the framework for the following from Michael Dietze’s Pair coding practical that is a living online supplement to his book (Dietze 2017). Thanks for sharing the code under an MIT license Mike! The main differences are that I’ve changes the subject matter from looking at phenology in Tall Grass Prairie using PhenoCam data to looking at postfire vegetation growth in Fynbos using MODIS satellite Normalized Difference Vegetation Index (NDVI), a measure of vegetation “greenness.” 6.2.1 Objectives The primary goal of this exercise is to gain experience working collaboratively to develop a scientific workflow. As such, this assignment must be completed with a partner. Specifically, we will outline a simple analysis, break the overall job into parts, and have each person complete part of the project. To put these parts together we will be using Github. (Along the way we will also be exploring the statistical concept of Likelihood?). 6.2.2 Postfire regeneration - Develop framework to fit models and make plots - Questions about residuals etc etc - Compare recovery trajectories across multiple sites? - Compare recovery trajectories within sites, between 2000 and 2015 fires… 6.2.3 Prairie Phenology The goal of our analysis is to investigate the phenological state of the U. Illinois Tall Grass Prairie. We will be using green chromatic coordinate (GCC), which is a color index. Before building the workflow you are encouraged to take a look at the site http://phenocam.sr.unh.edu/webcam/sites/uiefprairie/ and the raw csv data https://phenocam.sr.unh.edu/data/archive/uiefprairie2/ROI/uiefprairie2_AG_1000_1day.csv Note: older data for this site (2009-2018) is available at: http://phenocam.sr.unh.edu/data/archive/uiefprairie/ROI/uiefprairie_GR_1000_1day.csv The workflow for this analysis with have three components: Download PhenoCam data for the U. Illinois Tall Grass Prairie site Visualize the data with a mean and 95% confidence interval Fit a simple logistic model to spring leaf out for one specific year From this overall design, let’s next outline the specific steps involved as pseudocode ### Prairie Phenology Workflow ## Download phenology data ## Plot overall phenology data ## Create and visualize subset of data for leaf out ## Fit logistic model ## Visualize model and data 6.2.4 Modular Design From this overall design we can look for ways to modularize the analysis. One feature that jumps out is that we need to visualize the data three times, so we should definitely make a function to do that. The inputs to the function would be an x-axis (date), y-axis (gcc_mean), and error estimate (gcc_std), which we might pass as a dataframe for convinience. Since this is a graphing function we’d also like the ability to set all sorts of plot characteristics, which can be done in R by passing ... as an argument and then passing that on to the internal plot call. The proposed function interface would thus be ##&#39; Plot Phenocam data ##&#39; ##&#39; @param dat dataframe of date, gcc_mean, gcc_std ##&#39; @param ... additional graphing parameters plot.phenocam &lt;- function(dat,...) Next, because the raw data will be downloaded off the web and has embedded meta-data to handle, let’s go ahead and create a download function. This function just needs to know the URL for where to find the data. Unlike the plot function, this function will return something (the data that was downloaded), so it would be good design to document what is returned and how it will be formatted ##&#39; Download Phenocam data ##&#39; ##&#39; @param URL web address where data is located ##&#39; @return data.frame with days as rows, variables as columns download.phenocam &lt;- function(URL) Let’s also create a function to fit the logistic model to the spring leaf-out data, since we could easily see applying this same function to other data sets. The input to such a fit would obviously be the same data.frame that we’re using to make the plot. To do the fit itself we’ll use Maximum Likelihood, so in addition to the data we’ll need to provide an initial guess at the model parameters, which we’ll pass on to the numerical optimization. We’ll also want to return the full output from that numerical optimization so that we can check if it converged successfully. ##&#39; Fit logistic model ##&#39; ##&#39; @param dat dataframe of day of year (doy), gcc_mean, gcc_std ##&#39; @param par vector of initial parameter guess ##&#39; @return output from numerical optimization fit.logistic &lt;- function(dat,par) Finally, because we’ll want to make a plot of the logistic model after we’re done, let’s create a function for performing the model calculation. This function will also come in handy within the fit.logistic function. ##&#39; Logistic model ##&#39; ##&#39; @param theta parameter vector ##&#39; @param x vector of x values ##&#39; @return vector of model predictions pred.logistic &lt;- function(theta,x) At this point we’ve spent a good bit of time up front on organization – we have a detailed plan of attack and have thought carefully about what each module is responsible for doing. Each task has well-defined inputs, outputs, and goals. Rather than facing a thankless job of documenting our code after we’re done, even though we haven’t written a single line of code yet we are largely done with our documentation. What remains to do is implementation. 6.2.5 Task 1: Create &amp; Clone Repository Because we’re going to employ version control in our project, our first step is to create the repository that our project will be stored in. To ensure that both you and your partner get to see every step of how to work with version control, for the rest of this exercise you are going to complete every step twice, once from the perspective of the OWNER of the repository and once as the COLLABORATOR. 6.2.5.1 OWNER Go to your account on github.com and under the Repositories tab click on the “New” button (green with a picture of a book on it) Choose a name for your repository, but make sure it’s different from your partner’s (Don’t choose a “Repository template,” and keep it a “Public” repository) Click the “Initialize this repository with a README” checkbox Optionally also provide a Description, Add a licence (e.g. MIT), and add R to the .gitignore (check “.gitignore” and search for the R template) Click “Create Repository” Copy the URL of your new repository by clicking the clipboard icon To clone the repository, open up RStudio and create a New Project using this URL. Note: If you already have a project open it will close when you do so. Don’t worry, you can return to that project after the prac using the drop-down in the top-right of the RStudio window. Select New Project from the menu in the top right corner Select Version Control then Git Paste the URL in and click Create Project 6.2.6 Task 2: Add the first function: download.phenocam Within this project we’ll create separate files for each part of the analysis. To make the order of the workflow clear we’ll want to name the files systematically. In the first file we’ll implement the download.phenocam function # ##&#39; Download Phenocam data # ##&#39; # ##&#39; @param URL web address where data is located # download.phenocam &lt;- function(URL) { # ## check that we&#39;ve been passed a URL # if (length(URL) == 1 &amp; is.character(URL) &amp; substr(URL,1,4)==&quot;http&quot;) { # ## read data # dat &lt;- read.csv(URL,skip = 22) # # ## convert date # dat$date &lt;- as.Date(as.character(dat$date)) # # return(dat) # } else { # print(paste(&quot;download.phenocam: Input URL not provided correctly&quot;,URL)) # } # } 6.2.6.1 OWNER In RStudio, click File &gt; New File &gt; R Script Copy and Paste the above function into this file Save the file as “01_download.phenocam.R” From the Git tab, click the box next to the file you just created. This is equivalent to git add Click Commit, enter a log message, and click Commit. This is equivalent to git commit To push the change up to Github click on the green up arrow. This is equivalent to git push 6.2.7 Task 3: Collaborator adds plot.phenocam With the first function complete, let’s now imagine that a COLLABORATOR has been tasked with adding the second function. To do so they must first fork and clone the repository 6.2.7.1 COLLABORATOR Go to Github and navigate to the project repository within the OWNER’s workspace. Click Fork, which will make a copy of the repository to your own workspace. Copy the URL to your own version and follow the instructions above for cloning the repository in RStudio. Open a new file, enter the code below, and then save the file as “02_plot.phenocam.R” # ## Define ciEnvelope function # ciEnvelope &lt;- function(x,ylo,yhi,col=&quot;lightgrey&quot;,...){ # ## identify chunks of data with no missing values # has.na = apply(is.na(cbind(x,ylo,yhi)),1,sum) # block = cumsum(has.na);block[has.na&gt;0] = NA # blocks = na.omit(unique(block)) # # for(i in blocks){ # sel = which(block==i) # polygon(cbind(c(x[sel], rev(x[sel]), x[sel[1]]), c(ylo[sel], rev(yhi[sel]), # ylo[sel[1]])), col=col,border = NA,...) # } # } # ##&#39; Plot Phenocam data # ##&#39; # ##&#39; @param dat dataframe of date, gcc_mean, gcc_std # ##&#39; @param ... additional graphing parameters # plot.phenocam &lt;- function(dat,...){ # # if(!is.null(dat)){ # # ## QC flags # gcc_mean = dat$gcc_mean # gcc_mean[dat$outlierflag_gcc_mean&gt;-9999] = NA # # ## base plot # plot(dat$date,dat$gcc_mean,type=&#39;l&#39;,...) # # ## calculate CI # ylo = dat$gcc_mean-1.96*dat$gcc_std # yhi = dat$gcc_mean+1.96*dat$gcc_std # # ## add confidence envelope # ciEnvelope(dat$date,ylo,yhi) # # ## replot mean line # lines(dat$date,dat$gcc_mean,lwd=1.5) # } else { # print(&quot;plot.phenocam: input data not provided&quot;) # } # # } Follow the instructions above to Add, Commit, and Push the file back to your Github Next you want to perform a “pull request,” which will send a request to the OWNER that they pull your new code into their mainline version. From your Github page for this project, click New Pull Request. Follow the instructions, creating a title, message, and confirming that you want to create the pull request 6.2.7.2 OWNER Once the COLLABORATOR has created the pull request, you should get an automatic email and also be able to see the pull request under the “Pull Requests” tab on the Github page for the project. Read the description of the proposed changes and then click on “Files Changed” to view the changes to the project. New code should be in green, while deleted code will be in pink. The purpose of a pull request is to allow the OWNER to evaluat the code being added before it is added. As you read through the code, if you hover your mouse over any line of code you can insert an inline comment in the code. The COLLABORATOR would then have the ability to respond to any comments. In larger projects, all participants can discuss the code and decide whether it should be accepted or not. Furthermore, if the COLLABORATOR does any further pushes to Github before the pull request is accepted these changes will automatically become part of the pull request. While this is a very handy feature, it can also easily backfire if the COLLABORATOR starts working on something different in the meantime. This is the reason that experienced users of version control will use BRANCHES to keep different parts separate. Click on the “Conversation” page to return where you started. All participants can also leave more general comments on this page. If you are happy with the code, click “Merge Pull Request.” Alternatively, to outright reject a pull request click “Close pull request” 6.2.8 Task 4: Owner adds pred.logistic and fit.logistic We are now past the ‘set up’ stage for both the OWNER and the COLLABORATOR, so for this task we’ll explore the normal sequence of steps that the OWNER will use for day-to-day work 6.2.8.1 OWNER Pull the latest code from Github. In RStudio this is done by clicking the light blue down arrow on the Git tab. This is equivalent to the commandline git pull origin master where origin refers to where the where you did your orginal clone from and master refers to your main branch (if you use branches you can pull other branches) Next, open up a new R file, add the code below, and save as “03_logistic.R” # ##&#39; Logistic model # ##&#39; # ##&#39; @param theta parameter vector # ##&#39; @param x vector of x values # ##&#39; @return vector of model predictions # pred.logistic &lt;- function(theta,x){ # z = exp(theta[3]+theta[4]*x) # Ey = theta[1]+theta[2]*z/(1+z) # } # ##&#39; Fit logistic model # ##&#39; # ##&#39; @param dat dataframe of day of year (doy), gcc_mean, gcc_std # ##&#39; @param par vector of initial parameter guess # ##&#39; @return output from numerical optimization # fit.logistic &lt;- function(dat,par){ # # ## define log likelihood # lnL.logistic &lt;- function(theta,dat){ # -sum(dnorm(dat$gcc_mean,pred.logistic(theta,dat$doy),dat$gcc_std,log=TRUE),na.rm=TRUE) # } # # ## fit by numerical optimization # optim(par,fn = lnL.logistic,dat=dat) # } As before, add your new file under the Git tab, Commit the change, and push it back to Github To estimate the parameters in the logistic model we will use the likelihood principle which states that “a parameter value is more likely than another if it is the one for which the data are more probable.” To do this we need to define a Likelihood, which is the relationship between the value of the parameter and the probability of some observed data. [For the record, the Likelihood is not a probability distribution because it does not integrate to 1]. In this case we’ll start by assuming a Normal likelihood and use the standard deviation that’s reported in the data to represent the uncertainty. In a more detailed analysis we’d want to follow up to check both these assumptions, but it’s a simple starting point. Applying the likelihood principle we would then look for the most likely value of \\(\\theta\\), the vector parameters in the logistic model, which we call the Maximum Likelihood estimate. For a number or reasons that will become clear in later lectures, it is common to work with negative log likelihoods instead of likelihoods, in which case the negative implies that instead of looking for the maximum we’re now looking for the minimum. The fact that logarithm is a monotonic transformation means that taking the log does not change the location of this minimum. The code for this comes in three parts. First is the logistic model itself, pred.logistic, which translates the equation \\[\\theta_1 + \\theta_2 {{exp(\\theta_3 + \\theta_4 x)}\\over{1+exp(\\theta_3 + \\theta_4 x)}}\\] into code. The logistic has an overall S shape, with \\(\\theta_1\\) defining the minimum and \\(\\theta_1 + \\theta_2\\) defining the max. The midpoint of the curve – the x value where the function is halfway between the minimum and maximum – occurs at \\(-\\theta_3 / \\theta_4\\), while the slope at that point is \\(\\theta_4/4\\). Second is the negative log likelihood function, lnL.logistic, which we’re trying to minimize. The core of this is the Normal probability density, dnorm. The first arguement is the data, the second the is model, and the third is the standard deviation. The fourth arguement says that we want to return the log density, which is much more accurate if it’s performed internally than if we take the log of what’s returned by dnorm. Since we have many data points dnorm returns a vector, which we then sum up and change the sign to turn this into a minimization problem. The third part is a call to a numerical optimization function, optim, that searches through parameter space to find the set of parameters that minimize the negative log likelihood (i.e. that Maximize the Likelihood). Arguements are the initial parameter guess, the function being minimized, and any additional parameters that get passed on to that function. 6.2.9 Task 5: Collaborator adds the master script The day-to-day workflow for the COLLABORATOR is similar, but not exactly the same as the OWNER. The biggest differences are that the COLLABORATOR needs to pull from the OWNER, not their own repository, and needs to do a pull request after the push. 6.2.9.1 COLLABORATOR Pull from OWNER. Unfortunately, this has to be done from the command line rather than the pull button within RStudio, which just pulls from the COLLABORATOR’s repository. In RStudio go to Tools &gt; Shell to open a terminal At the terminal type git pull URL master where URL is the address of the OWNER’s Github repository. Because it is a pain to always remember and type in the OWNER’s URL, it is common to define this as upstream git remote add upstream URL which is a one-time task, after which you can do the pull as git pull upstream master Open a new Rmd file and add the code below. This code just flushes out the pseudocode outline we started with at the beginning of this activity. # ## Master script for phenology analysis # ## Load required functions # if(file.exists(&quot;01_download.phenocam.R&quot;)) source(&quot;01_download.phenocam.R&quot;) # if(file.exists(&quot;02_plot.phenocam.R&quot;)) source(&quot;02_plot.phenocam.R&quot;) # if(file.exists(&quot;03_logistic.R&quot;)) source(&quot;03_logistic.R&quot;) # ## Download phenology data # URL = &quot;https://phenocam.sr.unh.edu/data/archive/uiefprairie2/ROI/uiefprairie2_AG_1000_1day.csv&quot; # prairie.pheno &lt;- download.phenocam(URL) # ## Plot overall phenology data # plot.phenocam(prairie.pheno) # ## Create and visualize subset of data for leaf out # spring = as.Date(c(&quot;2019-01-01&quot;,&quot;2019-06-01&quot;)) # dat = subset(prairie.pheno,date &gt; spring[1] &amp; date &lt; spring[2], select=c(date,gcc_mean,gcc_std)) # plot.phenocam(dat) # ## Fit logistic model # dat$doy = as.POSIXlt(dat$date)$yday # par = c(0.3,0.11,-10,0.1) # fit = fit.logistic(dat,par) # ## Visualize model and data # plot.phenocam(dat) # lines(dat$date,pred.logistic(fit$par,dat$doy),col=2) Save this file as “04_Master.Rmd.” Within RStudio’s Git tab, add the file and Commit. Use the Push (up arrow) button to push this to your own repository On Github.com, submit a pull request 6.2.9.2 OWNER Evaluate and accept pull request. At this point your workflow should be complete and you should be able to run the analysis. References "],["data.html", "7 Data Management 7.1 Why do you need to manage your data? 7.2 The Data Life Cycle 7.3 Data and decisions", " 7 Data Management 7.1 Why do you need to manage your data? Data management is often the last thing on a scientists mind when doing a new study - “I have a cool idea, and I’m going to test it!” You don’t want to “waste” time planning how you’re going to manage your data and implementing that plan… Unfortunately, this never ends well and really is a realm where “haste makes waste.” Figure 7.1: The ‘Data Decay Curve’ (Michener et al. 1997) Here are a bunch of reasons you really want to focus on doing good data management: Bad data management leads to data loss… (Figure 7.1) Your future self will hate you if you lose it before you’re finished with it - Eek!!! This is less likely in the world of Dropbox, Google Drive, iCloud etc, but I know people who had to repeat their PhD’s because they lost their data or it was on a laptop that was stolen… Also, beware cloud storage!!! It is very easy for you or a collaborator to delete/overwrite/lose access to items etc, e.g.  if someone leaves the project and deletes the folder on their Dropbox without “leaving” it first) if the “owner” of the Google Drive folder loses access to their Google account (as will happen to your UCT Google Drive access as soon as you graduate!!!) through all manner of random “accidents” Data has value beyond your current project: to yourself for reuse in future projects, collaborations, etc (i.e. publications and citations), for others for follow-up studies, or combining multiple datasets for meta-analyses or synthesis etc for science on general (especially long-term ecology in a time of global change) We’ve covered this before, but it is also key for transparency and accountability. Data collection is expensive, and is often paid for with taxpayers’ money. You owe it to them to make sure that science gets the most out of that data in the long term. Lastly, good planning and data management can help iron out issues like intellectual property, permissions for ethics, collection permits, etc, in addition to outlining expectations for who will be authors on the paper(s), responsible for managing different aspects of the data etc up front. If you don’t establish these permissions and ground rules early they can result in data loss, not being able to publish the study, damage careers, and/or damage relationships in collaborations (including student-supervisor), etc. To avoid data (and relationship) decay, and to reap the benefits of good data management, it is important to consider the full Data Life Cycle. 7.2 The Data Life Cycle Figure 7.2: The Data Life Cycle, adapted from https://www.dataone.org/ Note that there are quite a few different versions of the data life cycle out there. This is the most comprehensive one I know of, and covers all the steps relevant to a range of different kinds of research projects. A full description of this data life cycle and related ecoinformatics issues can be found in (Michener and Jones 2012). Not all projects need to do all steps, nor will they necessarily follow the order here, but it is worth being aware of and considering all steps. For example, often the first thing you do when you have a new hypothesis is search around for any existing data that could be used to test it without having to spend money and time collecting new data (i.e. skip to step 6 - “Discover”). In this case I would argue that you should still do step 1 (Plan), and you’d want to do some checking to assure the quality of the data (step 3), but you can certainly skip steps 2, 4 and 5. A meta-analysis or synthesis paper would probably do the same. On the other hand, if you’re collecting new data you would do steps 1 to 5 and possibly skip 6 and 7, although in my experience few studies do not reuse existing data (e.g. weather or various GIS data to put your new samples in context). 7.2.1 Plan Good data management begins with planning. In this step you essentially outline the plan for every step of the cycle in as much detail as possible. Usually this is done by constructing a document or Data Management Plan (DMP). While developing DMPs can seem tedious, they are essential for the reasons I gave above, and because most funders and universities now require them. Fortunately, there are a number of online data management planning tools that make it easy by providing templates and prompts to ensure that you cover all the bases, like the Digital Curation Centre’s DMPOnline and UCT’s DMP Tool. Figure 7.3: Screenshot of UCT’s Data Management Planning Tool’s Data Management Checklist. A key thing to bear in mind is that a DMP is a living document and should be regularly revised during the life of a project, especially when big changes happen - e.g. new team members, new funding, new direction, change of institution, etc. I typically develop one overarching DMP for an umbrella project (e.g. a particular grant), but then add specifics for subprojects (e.g. separate student projects etc). 7.2.2 Collect and Assure There are many, many different kinds of data that can be collected in a vast number of ways! Figure 7.4: Springer Nature Infographic illustrating the vast range of research data types. While “Collect” and “Assure” are different steps in the life cycle, I advocate that it is foolish to collect data without doing quality assurance and quality control (QA/QC) as you go, irrespective of how you are collecting the data. For example: automated logging instruments (weather stations, cameras, acoustic recorders) need to be checked that they’re logging properly, are calibrated/focused, are reporting sensible values, etc if you’re filling in data sheets, you need to check that all fields have been completed (no gaps), that there are no obvious errors and that any numbers or other values look realistic. In fact, if you’re using handwritten data sheets it’s best to capture them as soon as possible (i.e. that evening), because that helps you spot errors and omissions, you have a better chance of deciphering bad handwriting or cryptic notes, and you can plot any values to see if there are suspicious outliers (e.g. because someone wrote down a measurement in centimetres when they were meant to use metres). When transcribing or capturing data into a spreadsheet or database it is often best to use data validation tricks like drop-down menus, conditional formatting, restricted value ranges etc to avoid spelling mistakes and highlight data entries that are outside the expected range of the data field. It may seem like a lot of effort to set this up, but it’ll save you a lot of time and pain in the long run!!! Increasingly, I’ve started moving towards capturing data directly into a spreadsheet on a tablet, or better yet, into an App on my phone. There are a number of “no code” app builders these days like AppSheet that inserts data directly into Google Sheets and saves photos to your Google Drive. Figure 7.5: An example data collection app I built in AppSheet that allows you to log GPS coordinates, take photos, record various fields, etc. 7.2.3 Describe There are few things worse than having a spreadsheet of what you think is the data you need, but you don’t know what the column names mean, how variables were measured, what units they’re reported in, etc… - Especially when you were the one who collected and captured the data!!! This descriptive data about the data is called metadata and is essential for making the data reusable, but is also useful for many other purposes like making the data findable (e.g. using keyword searches). In fact, metadata makes up the majority of what are called the FAIR data principles (Wilkinson et al. 2016), which largely focus on this and the next few steps of the Data Life Cycle. I’m not going to dwell on them other than to say that they are a key component of making your work reproducible, and that like reproducibility, practicing FAIR data principles is a spectrum. Figure 7.6: The FAIR data principles ErrantScience.com. Some key kinds of metadata: the study context why the data were generated who funded, created, collected, assured, managed and owns the data (not always the same person) contact details for the above when and where the data were collected where the data are stored the data format what is the file format what softwares were used (and what version) the data content what was measured how it was measured what units was it reported in what QA/QC has been applied is it raw data or a derived data product (e.g. spatially interpolated climate layers) if derived, how it was analyzed Metadata standards and interoperability Many data user communities have developed particular metadata standards or schemas in an attempt to enable the best possible description and interoperability of a data type for their needs. They are typically human and machine-readable data, so that the metadata records can also be read by machines, to facilitate storing and querying multiple datasets in a common database (or across databases). Figure 7.7: How standards proliferate… from xkcd.com/927, used under a CC-BY-NC 2.5 license. Using common metadata schemas has many advantages in that they make data sharing easier, they allow you to search and integrate data across datasets, and they simplify metadata capture (i.e. having a list of required fields makes it easier to not forget any). There are many standards, but perhaps the most common ones you’ll encounter in biological sciences (other than geospatial metadata standards) are DarwinCore and Ecological Metadata Language (EML). Figure 7.8: SpatioTemporal Asset Catalogs (STAC; stacspec.org) aims to provide a common specification to enable online search and discovery of geospatial assets in just about any format. Last, but not least, and not necessarily related to metadata, but it does make metadata capture easier, I strongly recommend you read this short paper on how to keep your data Tidy (Wickham 2014). It will life it much easier for you once you get to the analysis step… 7.2.4 Preserve There are two major components to preserving your data: Back your data up now!!! (and repeat regularly) Losing your data can be incredibly inconvenient!!! A good friend of mine lost all of his PhD data twice. It took him 7 years to complete the degree… Beyond inconvenience, losing data can be incredibly expensive! Doing 4 extra years to get your PhD is expensive at a personal level, but if the data are part of a big project it can rapidly add up to millions - like How Toy Story 2 Almost Got Deleted. PRO TIP: Storing data on the cloud is not enough! You could easily delete that single version of all your data! You may also lose access when you change institution etc. E.g. What happens to your UCT MS OneDrive and Google Drive content when you graduate and ICTS close your email account? Long-term preservation and publication This involves the deposition of your data (and metadata!) in a data repository where it can be managed and curated over the long term. This is increasingly a requirement of funders and publishers (i.e. journals). Many journals allow you (or require you) to submit and publish your data with them as supplementary material. Unfortunately, many journals differ in how they curate the data and whether they are available open access. I prefer to publish my data in an online open access repository where you can get a permanent Digital Object Identifier (DOI) that you can link to from your paper. Another consideration, if you are keen for people to reuse your data (which if you are not you will fail this course by default) is where people are most likely to look for your data (i.e. making your data “Findable/Discoverable.” There are many “bespoke” data repositories for different kinds of data, e.g. Global databases: GenBank - for molecular data TRY - for plant traits Dryad - for generalist biological and environmental research South African databases: SANBI - for most kinds of South African biodiversity data SAEON - for South African environmental data (e.g. hydrology, meteorology, etc) and biodiversity data that don’t fit SANBI’s databases If none of these suit your data, there are also “generalist” data repositories that accept almost any kind of data, like: FigShare Zenodo UCT’s ZivaHub (which is built on and searchable through FigShare) I haven’t discussed physical samples at all. These are obviously a huge (if not bigger) challenge too, although there are some obvious homes for common biological data, like herbaria for plant collections and museums for animal specimens. 7.2.5 Discover This is perhaps the main point of the Data Life Cycle and FAIR data principles - to make data findable so that it can be reused. The biggest challenge to discovering data is that so many datasets are not online and are in the “filing cabinet in a bath in the basement under a leaking pipe” as in Figure 7.6. If you preserve and publish them in an online data repository, this overcomes the biggest hurdle. The next biggest challenge is that there is so much online that finding what you need can be quite challenging (like looking for a needle in a haystack…). This is where choosing the right portal can be important. It is also what metadata standards are aimed at - allowing interoperable searches for specific data types across multiple repositories. A final consideration is whether you have permission to use the data. You can often find out about the existence of a dataset, either online or in a paper, but the data aren’t made freely available. This is where licensing comes into play. Most data repositories require you to publish the data under a license. There are many options depending on the kind of data and what restrictions you want to put on its use. I’m not going to go into the gory details, but Creative Commons have created an extensible system of generic licenses that are easy to interpret and cover most situations. I say extensible because the licenses are made up of a string of components that can be layered over each other. For example: CCO - means it is Open - i.e. there are no restrictions on use and it is in the public domain CC BY - means by attribution - you can use the data for any purpose, but only if you indicate attribution of the data to the source or owner of the data CC BY-SA - means by attribution + share alike - i.e. you have to indicate attribution and share your derived product under the same license CC BY-ND - means by attribution + no derivatives - i.e. you have to indicate attribution, but cannot use it to make a derived product. This is often used for images - allowing you to show the image, but not to alter it. CC BY-NC - means by attribution + non-commercial - you have to indicate attribution, but cannot use it for commercial purposes (i.e. you can’t sell derived products) CC BY-NC-SA - by attribution + non-commercial + share alike CC BY-NC-ND - by attribution + non-commercial + no derivatives 7.2.6 Integrate There are a few different components to data integration in this context: Linking different kinds of data, usually through spatial and or temporal information e.g. matching you biodiversity collections with weather records or GIS information about the sites Keeping track of changes you’ve made to your data as you prepare it for analyses (versioning) e.g. you may be wanting to compare species richness across sites. This requires estimating species richness from your field data (usually lists of species by site occurrences and/or abundances) you should always keep a copy of your raw data!!! using scripting languages for data handling and analyses (e.g. R, Python, MatLab) can help you keep record of how you did any data summaries, transformations, etc, but only if you write clean, well-documented code and manage your code well!!! Curating your data such that they can easily be integrated with other, similar datasets for larger analyses or meta-analyses this is largely a metadata game, but also one of data formats etc. Many fields promote the use of common data standards with rules on measurement specifications, file formats, common data and metadata fields, controlled vocabularies etc that allow easy integration, searching and manipulation (see section 7.2.3 for more details). This is what a lot of the online databases attempt to achieve. 7.2.7 Analyze “The fun bit,” but again, there are many things to bear in mind and keep track of so that your analysis is repeatable. This is largely covered by the sections on Coding and code management and Computing environment and software in Chapter 5 7.3 Data and decisions There are some other aspects of data, while still important if the goal is purely academic, that are make-or-break when the goal is informing decisions. latency uncertainty References "],["references.html", "References", " References "]]

[["index.html", "A Minimal Introduction to Ecological Forecasting 1 Overview 1.1 General 1.2 Lectures and practicals 1.3 Preparation 1.4 A bit about me 1.5 Acknowledgements and further reading:", " A Minimal Introduction to Ecological Forecasting Jasper Slingsby 2022-07-31 1 Overview This module is a minimal introduction to Ecological Forecasting for the 3rd year undergraduate Biological Sciences BIO3019S class at the University of Cape Town. 1.1 General I provide a very brief introduction to the framework for Ecological Forecasting. We only have a two weeks, so this really is a minimalist introduction. I’ll focus on providing a broad overview of the general framework and motivation for ecological forecasting, but won’t have time to delve into the more gory theoretical and statistical details. I mostly use Ecological Forecasting as a framework to highlight various themes and principles that are increasingly important for quantitative biologists - understanding how we inform or make decisions; managing data; working reproducibly; propagating, understanding and reducing uncertainty, etc. Not all of this is fun and exciting, but as I said, it is important stuff for quantitative biologists to know. I’ll try my best to make it interesting! Hopefully by the end of the module you’ll see the value in it all - both for you as an individual and for science and society in general. The core outcomes/concepts I hope you’ll come away with: To be able to situate the role of models and the importance of forecasting in science and ecological decision making Familiarity with the concepts and understand the need for Open, Reproducible Science Familiarity with The Data Life Cycle Familiarity with the value and flexibility of Bayesian statistical methods Some familiarity with sources of uncertainty and the need to characterize, propagate, analyze, reduce and present uncertainties when forecasting 1.2 Lectures and practicals Lectures Lectures will be held live 12:00 - 12:45 from 1 - 12 August. I’ll be adding to (and mostly teaching from) these online course notes as we go along. Practicals There is only one practical for this module, 2-5PM on Tuesday the 2nd August. You will need to spend half an hour setting up the required software on your laptops before Monday the 1st August!!! See instructions below. Your report on the practical will be due on Monday, 8 August - You will be evaluated on how well you completed the Github task during the prac and your answers to a short set of questions about the analyses. Answering the questions shouldn’t take more than half an hour. 1.3 Preparation For the lecture content: The following 4 minute video will give you a glossy overview of what most of this module is about You are expected to read Dietze et al. (2018) for Thursday the 4th August. You can download it here. For the practical: You need to install and set up RStudio and Github and test your setup. You can find the 11-step instructions in section 2. This may be a bit tedious, but there’s no other option really. I’ve done my best to make it as painless as possible. It should take you about half an hour if all goes well… (less if you have R and RStudio installed already). PLEASE DO THE SETUP BEFORE WE MEET!!! I will check in on Monday the 1st to see if people are having issues, but don’t expect my help if you haven’t tried by yourself first. Trust me, I will be able to tell… 1.4 A bit about me I’m an ecologist who has become more quantitative through time, but has little formal training in quantitative methods (i.e. I’ve learnt by doing over the past 20 years). As such, I still make elementary mistakes. In fact, the entry requirements for this course are beyond my formal training, so you may have much to teach me! If you spot any errors, confusion or contradictions, please let me know and I’ll get back to you and/or update the course notes accordingly. Hopefully by the end of the course you can suggest changes directly using pull requests to the GitHub repository for the course notes. 1.5 Acknowledgements and further reading: The following resources were instrumental in me pulling this material together and are worth spending time exploring. I cite my sources throughout the course notes, so check out the references at the end of each section and the end of the course notes for more. ecoforecast.org Dietze, Michael C. 2017. Ecological Forecasting. Princeton University Press. https://doi.org/10.2307/j.ctvc7796h. Dietze, Michael C. et al. 2018. “Iterative near-Term Ecological Forecasting: Needs, Opportunities, and Challenges.” Proceedings of the National Academy of Sciences of the United States of America 115 (7): 1424–32. https://doi.org/10.1073/pnas.1710231115. All code, images, etc can be found here. I have only used images and other materials that were made available online under a non-restrictive license (Creative Commons, etc) or for which I have express permission, and have attributed my sources. Content without attribution is my own and shared under the license below. If there are any errors or any content you find concerning with regard to licensing, or that you find offensive, please contact me. Any feedback, positive or negative, is welcome! This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License. References "],["pracprep.html", "2 Preparation for the practical", " 2 Preparation for the practical For the practical, we’ll be using the R statistical programming language and the Git version control system. We’ll also be using an integrated development environment (IDE) for each: RStudio and GitHub, respectively. The installation and setup can be a bit long-winded, but once done you should be good to go until you change or reformat your computer. The steps below are my summary and (hopefully) more intuitive adaptation of the instructions provided for setting up GitHub and version control with R. If my steps don’t work its probably best to read up there. Another great resource is HappyGitWithR, who work through the steps very thoroughly on different operating systems (Windows, Mac, Linux). In general, if a step is not working, you probably have to redo the previous step… First we’ll start with the necessary software. Download and install the latest version of R Download and install the latest free version of RStudio Desktop Download and install the latest version of Git - accept all the defaults Then get started with GitHub: Create a GitHub account Run through the 10 minute GitHub tutorial that is offered when you activate your GitHub Account (It’ll really help you get the idea behind what Git does!) Now you have RStudio, R and Git installed, and you have a working GitHub account that lets you do stuff online, but what remains is to get GitHub working locally and configuring RStudio to use GitHub. Install GitHub CLI (Command Line Interface). For Windows you can download the installer here Open RStudio. Select the Terminal tab (top left, next to Console) Enter gh auth login, then follow the prompts: Select GitHub.com When prompted for your preferred protocol for Git operations, select HTTPS When asked if you would like to authenticate to Git with your GitHub credentials, enter Y When asked how you would like to authenticate select Login with web browser Copy the 8-digit code and hit Enter Github.com will open in your internet browser - paste the code and hit enter If any of these steps don’t work, just start again with gh auth login in Terminal In RStudio Go to Global Options (from the Tools menu) Click Git/SVN Make sure Enable version control interface for RStudio projects is on If necessary, enter the path for your Git or SVN executable where provided (this shouldn’t be needed, but may) Click Apply Restart RStudio Ok, now everything should be working. The next steps (explained below) are to fork and clone your first repo to see if everything is working, and then to modify a file in RStudio, push it back to your forked repo, and then create a pull request for me to review and accept your changes. This let’s me know that you’ve made it through the preparation for the practical, and it gives me your GitHub username. In GitHub: Make sure you are logged in, search and navigate to JasperUCT/pullltest (spot the 3 “l”s!) Click Fork, which will make a copy of the repository to your own workspace Copy the URL to your own version and follow the instructions below for cloning the repository in RStudio In RStudio: In the top-right of the window you’ll see Project: (None) - click on it and select New Project then Version Control then Git In the Repository URl: box paste the URL to your forked repo (It should look something like : https://github.com/YourGitHubUsername/your-forked-repo_name.git Project directory name should fill automatically For Create project as subdirectory of: hit Browse and navigate through your file system to where you want to put your folder for storing your Git repositories. I recommend something like ~Documents/GIT (If you’ve used Git before you may have set this already and can skip this step) Click Create Repository Your RStudio window should now look like this: Figure 2.1: What you should see… Note there are three files in the Files tab in the bottom-right window, and you should see a Git tab for the top-right window. It also says pullltest in the project drop-down top-right on mine, but yours will display the name you gave your forked repo. Open the README.md file in RStudio Add your message (something like “Hi! This is Real Name and I’ve made it this far!!!”) and save the file Select the Git tab in the top-right window Check the box next to README.md and click Commit Add a Commit message to say what changes you’ve made Then hit Push It will ask you to authenticate. Select Authenticate in your web browser. The web browser will ask you to give Git permissions. Allow the permissions and it should work. If you get an error at this point to the effect of “You do not have permission to push to this repository”, then you may have forgotten to fork your own repo from my one and are trying to push to mine… If so, start again from step 9. Final step!!! In GitHub (i.e. online) Click Pull requests (top-leftish) Click New pull request (green, top-right) Click Create pull request (green, top-right) It should already be on this repo unless you’ve been doing other things in Git In the comment window put your name so I know who you are if it isn’t obvious from your GitHub username Click Create pull request (green, bottom-right) And you’re done!!! I’ll get a notification of your pull request and, if all’s in order, I’ll accept it. If not, I’ll reply with a comment on the pull request. You should receive a notification via the email you registered with your GitHub account. "],["models.html", "3 Models and decision making 3.1 The basics of making a decision 3.2 Getting quantitative 3.3 Iterative decision-making 3.4 Iterative decision-making and the scientific method 3.5 The importance of prediction in ecology 3.6 Iterative near-term ecological forecasting 3.7 Iterative ecological forecasting in context 3.8 Reproducible research", " 3 Models and decision making This section is also available as a slideshow hold Ctrl or Command and click this link to view in full screen. What factors do you consider when making a decision? 3.1 The basics of making a decision Informing decisions typically requires knowing (or guessing at) something about the future. To this end, once a problem and the need to make a decision have been identified, the factors we consider when making that decision usually include: Evidence Experience Expectation Uncertainty The relationship between these can be represented like so: Figure 3.1: The factors considered when making a decision. Your decision is typically based on your expectation of the outcome. This expectation is based on existing evidence and/or previous experience. Uncertainty is a part of each step. There are a number of reasons why the existing evidence or previous experience may be imperfect for the decision at hand, leading to uncertainty in the expectations. There may also be uncertainty in the way in which you use the evidence and experience to develop your expectation. We’ll come back to these sources of uncertainty later in the module, but needless to say, quantifying and understanding the uncertainty is crucial in any decision. If uncertainty is high your expectation may be no better than random, and thus useless for informing your decision. Quantifying uncertainty properly helps us circumvent two evils which could mislead decision makers: being falsely overconfident in our predictions (potentially leading to a “wrong” decision), or being falsely uncertain in our predictions (which would encourage overly conservative decisions/actions which may be wasteful or less effective). Lastly, ignoring or quantifying uncertainty incorrectly can lead to bias predictions. 3.2 Getting quantitative The nice thing about the framework above is that it is similar whether you are approaching the decision qualitatively or quantitatively (i.e. using models and data to inform your decision). Figure 3.2: Using models and data when making a decision. Following a quantitative approach the “evidence” is typically empirical data, which can be fed into a model to make forecasts that can help inform the decision. The “experience” are the current state of knowledge and your prior belief, which you use to specify the type and structure of your model (or ensemble of models) and the scenario(s) you want to evaluate. The “experience” can also help you evaluate the assumptions of your model(s), and, if you are using a Bayesian model, can be included directly in the model when specifying your prior beliefs (more on this later in the module). Figure 3.3: A hypothetical example where a model can help you make a decision. The data (points) are the evidence, while the experience or current state of knowledge are used to specify the model (a linear model in this case). Here the relationship between effort invested and reward is nearly 1 to 1, suggesting to the decision-maker that the more effort you invest, the more reward you will reap. That said, there is scatter around in the points around the 1:1 line, suggesting some uncertainty. 3.3 Iterative decision-making Few decisions in natural resource management are once-off, and most are made repeatedly at some time-step (e.g. daily, monthly, seasonally, annually, decadally, etc). Should you burn, cull, harvest, restore, etc? While one should always evaluate the outcome of your decision, this does not always happen… Evaluating the outcome is especially important when the decision will need to be repeated, so that you can learn from experience. Figure 3.4: Iterative decision making. When using quantitative forecasts this can be done by collecting new data and updating your prior knowledge by evaluating the outcomes of the decision against the original model forecasts. This can tell you whether your forecast was any good and whether you need to refine or replace your model, consider additional scenarios or inputs, etc. We’ll discuss doing this quantitatively in section 10, by fusing your new data and knowledge into a new forecast. Figure 3.5: Revisiting our Effort to Reward example, what would you do if the decision-maker decided to invest huge effort, but the next few data points looked like this? 3.4 Iterative decision-making and the scientific method It’s worth highlighting the similarity between the iterative decision making cycle I’ve outlined in Figure 3.4 and the scientific method, i.e.: Observation &gt; Hypothesis &gt; Experiment &gt; Analyze &gt; Interpret &gt; Report &gt; (Repeat) Figure 3.6: The Scientific Method overlain on iterative decision making. So a focus on iterative decision-making facilitates iterative learning (i.e. scientific progress). 3.5 The importance of prediction in ecology “prediction is the only way to demonstrate scientific understanding” (Houlahan et al. 2017) While this view may be slightly overstated, it is a very good point. If we cannot make reasonably good predictions, we’re missing something. Unfortunately, prediction has not been a central focus in ecology, impeding progress in the improvement of our ecological understanding. In ecology we mostly test qualitative, imprecise hypotheses: “Does X have an effect on Y?” rather than “What is the relationship between X and Y?” or better yet “What value would we expect Y to be, given a particular value of X?”. Without testing precise hypotheses and using the results to make testable predictions we don’t know if our findings are generalizable beyond the specific data set we collected. If our results are not generalizable, then we’re not really making progress towards a better understanding of ecology. Figure 3.7: Prediction… from xkcd.com/2370, used under a CC-BY-NC 2.5 license. To make predictions we need models, and models provide structured summaries of our current ecological understanding (conceptual or quantitative, but preferably quantitative, because these are easier to compare). Without making predictions and comparing the skill of new models to old ones, we can’t track if we are making progress! A key point here is that the predictions must be testable! We do use a lot of models in ecology, and even use them to make predictions (e.g. species distribution models (SDMs), dynamic vegetation models (DVMs), etc), but these predictions are typically 50+ years into the future, which is way to long to wait to see if our predictions were reasonable or useful. A quick aside on model validation vs testing predictions: Testing predictions with new data collected after you’ve made your predictions is the most robust way to validate a model, but you usually want to do some form of validation before you make your final predictions to make sure the model is working reasonably well. For this we most commonly do some form of cross-validation, whereby we split the data into a “training” subset (that we use for fitting (or training) the model) and a “test” subset (that we try to predict). If your model is no good at predicting your test data, there’s probably no point in making predictions into the future… 3.6 Iterative near-term ecological forecasting The recent growth in interest in iterative ecological forecasting seeks to not only make prediction a central focus in ecology, but to do so on a time scale that is both useful for decision makers and allows us to learn from testing our predictions (days to decades). The “gold standard” here is an informatics pipeline that can ingest new data and make new forecasts automatically with minimal user input. This is a great initiative, but as we will see it poses a number of major challenges and requires a big improvement in quantitative skills in biology (hence this course…). Fortunately, as we will see during the module, any steps towards the gold standard is likely to be useful, even if you never get there. Here I break down the individual components of ecological forecasting (using figures from a lecture on data assimilation by Michael Dietze): You start with your initial conditions (data and knowledge that feeds into designing and fitting your model) You make forecasts (i.e. predictions into the future) using your model, based on your initial conditions. You monitor and collect new observations to compare with your forecasts and original observations (i.e. initial conditions). Finally, you analyze the new observations in the context of your forecasts and original observations, and update the initial conditions for the next iteration of the forecast. Figure 3.8: The iterative ecological forecasting cycle in the context of the scientific method, demonstrating how we stand to learn from making iterative forecasts. From lecture on data assimilation by Michael Dietze. (Please excuse that the colours of the distributions have changed from above…). The key steps are: Make a forecast based on your current data and understanding Collect new observations and compare them to your forecast Analyze the new observations in the context of your forecast and original data Update estimates of the current state of the system (data and understanding), before making a new forecast Two things not indicated in this diagram are: that when making the forecast and analyzing the new observations you can learn about the various sources and drivers of uncertainty in your forecast and use that to adapt or guide what and how to monitor so that you can reduce those uncertainties developing this into an automated informatics pipeline is best done in a reproducible research framework Iterative ecological forecasts are thus aimed at: applied outcomes, through providing evidence to support decision making knowledge generation through iterative learning i.e. the scientific method So it’s a great way of getting scientists to engage in real-world problems, demonstrating the value of our science, and learning by doing! 3.7 Iterative ecological forecasting in context The figure below from Dietze et al. (2018) provides an expanded representation of these conceptual links between iterative ecological forecasting, the scientific method, and decision making (here in the context of adaptive management, which is a management paradigm that focuses on learning by doing). Figure 3.9: Conceptual relationships between iterative ecological forecasting, adaptive decision-making, adaptive monitoring, and the scientific method cycles (Dietze et al. 2018). The iterative ecological forecasting cycle is tightly aligned to the scientific method cycle: Hypotheses (A) are embedded in models (B). The models integrate over uncertainties in initial conditions (IC), inputs, and parameters to make probabilistic forecasts (the purple distributions, Fx, in step C), sometimes for multiple alternative scenarios. New observations are then compared with these predictions (D) to update estimates of the current state of the system (Analysis) and assess model performance (E), allowing for the selection among alternative model hypotheses (Test and Refine). The iterative forecasting cycle also feeds into adaptive management and monitoring: In Adaptive Management and decision analysis, alternative decision scenarios are generated (2) based on an assessment of a problem (1). These decision scenarios are typically used to define the scenarios (or boundary conditions) for which models are run (“Scenarios” arrow), but can also feed into scientific hypotheses (not shown). Forecasts (Fx) are key in assessing the trade-offs and relative merits between alternative decision options (3). The decision(s) taken (4) determine the monitoring requirements (5), which allow us to evaluate the outcomes and reassess the problem (1), and start the adaptive management cycle again. Note that the iterative forecast cycle is also useful for adaptive management in that the analysis and partitioning of forecast uncertainties (from step C) can provide further guidance on what and how to monitor, so as to optimize the reduction in model uncertainties. This represents Adaptive Monitoring (dashed line) and is a cycle of itself (Lindenmayer and Likens 2009), but is largely subsumed by the other cycles here so we won’t go into it any further here. Thus the iterative cycles of science, forecasting, management and monitoring are tightly intertwined and can interact continuously. 3.8 Reproducible research What isn’t clear from Figure 3.9 is that all of this needs to be founded on a highly efficient informatics pipeline that is robust and rapidly updateable. Since the emphasis here is on near-term forecasts to inform management, if the process of adding new data and updating the forecasts is too slow, the value of the forecasts is lost. As we’ll see in future lectures, the best way to build a highly efficient informatics pipeline is to follow reproducible research principles (section 5), including good (and rapid) data management (section 8). Adding this link to Figure 3.9 helps to highlight what I like to think of as “The Olympian Challenge of data-driven ecological decision making”. Figure 3.10: The Olympian Challenge of data-driven ecological decision making. Working reproducibly requires learning a lot of skills and can take a lot of effort, but is well worth it beyond it’s utility for ecological forecasting - for you as an individual, and for science in general. This is why I decided to make it part of the title for the module and the focus of at least two lectures and the practical. References "],["forecasts.html", "4 Making forecasts 4.1 Proteaceae as a model organisms 4.2 Proteaceae as management indicators 4.3 Potential issues with the rules of thumb… 4.4 Assessing population viability 4.5 Climate and fire-driven changes in demographic rates and distribution 4.6 Near-term iterative ecological forecasts?", " 4 Making forecasts You’re probably wondering “What are ecological forecasts?” or “Where are we going with all this?” The focus of this section is to provide some context with an example of what I believe is one forecasting need and opportunity in the Fynbos Biome. I was going to give 3 examples, but rapidly realized it would be too long a lecture. I’ll be using the other examples to illustrate various principles in later lectures, but hopefully this example will provide some practical context for some of the issues we’ll address in the rest of the module. Note: This example has not yet been developed into full near-term iterative ecological forecasts sensu Dietze et al (2018). I also don’t think they necessarily have to get all the way there to be useful. Think of it as an “ecological forecasting spectrum” where the gold standard is fully developed and automated near-term iterative ecological forecasts. Figure 4.1: Conceptual relationships between iterative ecological forecasting, adaptive decision-making, adaptive monitoring, and the scientific method cycles (Dietze et al. 2018). I’ll start with a reminder that the goal here is to focus the forecasting effort on an applied real-world problem and to do so in a manner that allows us to learn and improve our scientific understanding of the system. 4.1 Proteaceae as a model organisms The Proteaceae are probably the best studied and understood plant family in the Fynbos Biome. They have been the focus of a number of large, focused research programmes and are used as indicator species for various conservation management applications. There has been extensive locality and demographic data have been collected by conservation authorities (CapeNature and SANParks), citizen scientists (Protea Atlas Project and iNaturalist) and researchers since the late 1970s and before, and this is reflected in the large (and growing) scientific literature on the family. Figure 4.2: Temporal dynamics of publications on South African Proteaceae based on a Web of Science search on 13 June 2012. Figure from Schurr et al. (2012). A huge benefit of the herculean Proteaceae data collection (and management) effort is that it provides all the data we need to parameterize various types of models. In fact, data on the Proteaceae have been hugely important for the global development of species distribution and demographic models (see Schurr et al. (2012), but also many subsequent papers). Because they are the best-studied group of plants in the Fynbos, they are heavily-utilized for informing conservation planning, management and monitoring. Some examples include informing: protected area planning wildfire management wildflower harvesting climate change monitoring I’m sure there are others I’ve forgotten… 4.2 Proteaceae as management indicators Our knowledge of the demography of the Proteaceae is used for the direct management of Fynbos in two ways: Firstly, at the species level, for setting guidelines for sustainable wild harvesting of their inflorescences Secondly, at the ecosystem level, to help determine acceptable fire return intervals A quick refresher on Proteaceae life cycles and demography: Figure 4.3: Protea cynaroides on Table Mountain showing current inflorescences and older (grey), closed cones that protect seeds from fire and release them into the post-fire environment. Figure 4.4: The fire-driven life-cycle of Fynbos Proteaceae species, including harvesting, taken from (Treurnicht et al. 2021). Population size/stability are determined by key demographic rates of adult fecundity (size of the canopy seed bank), post-fire seedling recruitment and adult fire survival (blue–grey boxes). These rates are affected in various ways by environmental conditions, density dependence, the timing, intensity and severity of fire, wildflower harvesting, etc The management guidelines are currently set by “rule of thumb”1: Wildflower harvesting: “[There should be no] harvesting until at least 50% of the population had commenced flowering, a harvest of up to 50% of current season flower heads after this stage, and no harvesting at least one year prior to a prescribed burn” (Wilgen et al. 2016) Fire return intervals: “No fire should be permitted in fynbos until at least 50% of the population of the slowest-maturing species in an area have flowered for at least three successive seasons (or at least 90% of the individuals of the slowest maturing species in the area have flowered and produced seed). Similarly, a fire is probably not necessary unless a third or more of the plants of these slow-maturing species are senescent (i.e. dying or no longer producing flowers and seed).” (CapeNature, n.d.) Both these rules are based on the premise that maintaining seed banks is the key to the persistence of Proteaceae populations. i.e. that there is a large enough seed bank present when a fire occurs for the population to recover. But is focusing on seeds alone reasonable? And if so, do the thresholds in the guidelines allow for enough seed? 4.3 Potential issues with the rules of thumb… Figure 4.5 presents extensive field data for 26 Proteaceae species from Treurnicht et al. (2016). What issues might these data suggest for the rules of thumb? Figure 4.5: Variation in demographic rates of 26 serotinous Proteaceae species of seeder and sprouter life-history types across their distribution range (Treurnicht et al. 2016). (a) Adult fire survival; (b) Individual fecundity (F); and (c) Per-capita recruitment rate (R). These issues include: species differ in their reliance on seed for their survival (e.g. sprouters vs seeders) sprouters have high persistence of adults through fires and need fewer new recruits from seed seeder adults are killed by fire, so populations depend entirely on recruitment from seed species vary in their fecundity (total number of seeds) fecundity = number of inflorescences produced multiplied by the number of seeds per inflorescence species vary in seed viability and recruitment success viability depends on pathogens, seed predators and other factors - many linked to the age of the seed or inflorescence seed-specific recruitment depends on viability and seed properties (size etc), conditions during the establishment phase (rainfall etc), finding suitable microsites, etc. per-capita recruitment is the combination of fecundity and seed-specific recruitment Figure 4.6 present variation within one species that suggests more issues… Figure 4.6: Intraspecific variation in (a) fecundity and (b) recruitment in response to range-wide variation in fire return interval (time since fire), adult population density and soil moisture stress (% days with soil moisture stress) for Protea punctata (Treurnicht et al. 2016). there is also intraspecific variation in fecundity and recruitment along climatic, soil, fire regime, population density, pollinator availability and other gradients and there is interspecific variation in this intraspecific variation i.e. species vary among populations in their response to climatic, soil, pollinator availability and other gradients 4.4 Assessing population viability Fortunately, we can address these issues by using the data in demographic models to perform population viability analysis under varying harvesting rates (e.g. Treurnicht et al. 2021). Figure 4.7: Sensitivity to wildflower harvesting for various Proteaceae species (Treurnicht et al. 2021). Above: Intraspecific variation in sensitivity to harvesting depicted as maps for four different species with pink dots highlighting where the change in population-level extinction probability (the difference between extinction probabilities under 0% and 50% harvesting) is greater than 0.1. The white and black areas depict species-specific occurrence records and the geographical distribution of all Proteaceae in the Cape Floristic Region, respectively. Below: Interspecific variation in sensitivity to harvesting depicted as the proportion of populations per species that are highly vulnerable to harvesting. These models suggest that following the current harvesting guidelines can greatly increase the probability of many populations going extinct (Figure 4.7; Treurnicht et al. (2021)), and that they would threaten a large portion of populations for some species, including the most commercially valuable ones! But is that enough? So what should the harvesting guidelines be? Are we sure they would be effective and what if there are “ecological surprises” that make them inappropriate? For example, Treurnicht et al. (2021) did not consider changing climate or fire regimes? Should we just ban wildflower harvesting altogether to be safe? While it isn’t a huge industry, a ban would be undesirable for a number of reasons: many livelihoods depend on wildflower harvesting, often among the very poor while some species are targeted and may decline as a result, at least it’s still Fynbos. Removing the option to earn from that land would risk forcing the landowner to consider more destructive land use activities or potentially convert it to other land cover types while some species may struggle under the existing guidelines, some species are largely unaffected and can quite happily be harvested What we really need is ongoing monitoring and updated forecasts that respond and adjust the guidelines to allow sustainable harvesting while not threatening the species. Ideally, these forecasts would include the impacts of changing climate and fire. 4.5 Climate and fire-driven changes in demographic rates and distribution Merow et al. (2014) combined demographic modelling and species distribution modelling using a “Demographic Distribution Model” whereby they used spatial covariation between demographic rates and environmental conditions to infer where the species can maintain positive population growth rates under current and future climate and fire conditions. Interesting side note: This analysis used integral projection models (IPMs), which you can think of as the “next generation” of the Leslie matrix model that you will be familiar with. In IPMs the state variable is size (not age) and there are far more classes (i.e. rows and columns in the “matrix”). The (very oversimplified explanation of) the advantage of this is that you can build regression models for the vital rates as a function of plant size and environmental parameters and then relate those back to your transition matrix. This allows you to explore the effect of spatial covariates (climate, soil, fire, etc) on demographic rates, which would be very difficult to do if you only had a few size classes. This also allow you to make projections for different conditions, allowing you to explorethe effetcs of climate change, altered fire regimes, etc. Figure 4.8: Estimated vital rates of Protea repens across the CFR. Figure from Merow et al. (2014). First, they modeled the species’ individual vital rates as a function of environmental variation across its range (Figure 4.8). Then they combined them to estimate the population growth rates \\(\\lambda\\) per pixel, providing an indication of where the population should remain stable or increase (\\(\\lambda&gt;1\\)) or decline and go extinct (\\(\\lambda&lt;1\\)). This was used as a threshold to map the species’ distribution (Figure 4.9). Figure 4.9: Model evaluation of the predicted population growth rate (\\(\\lambda\\)) and distribution of Protea repens across the CFR. (a) Mean \\(\\lambda\\) and (b) interquartile range of \\(\\lambda\\). (c–d) Evaluation of (a) using presence/absence data. (e) Posterior probability that \\(\\lambda\\) &gt; 1, representing a viable population. (f) Evaluation of (a) using ordinal abundance data. Figure from Merow et al. (2014). Once they’d done this validation and established that the model worked reasonably well they were able to do projections under altered fire regimes or climate conditions (Figure 4.10). Figure 4.10: Projections of the change in population growth rates of Protea repens under different scenarios. (a–b) Reducing (increasing) the observed fire return time by 4 yr. (c) Variation of mean population growth rate as a function of fire return time. The horizontal dashed line indicates where the growth rate = 1. (d) The difference between present day predictions and projections under future climate change scenario with temperature increased by 1 degree and precipitation reduced by 10%. Figure from Merow et al. (2014). There’s a lot of interesting spatial variation in the species’ expected response to changing conditions! The beauty of this analysis is that it: can tell us where we are most likely to see a negative response - guiding where to monitor can break that response down into its component vital rates - i.e. growth, seed production (fecundity), survival, etc - which are all things that are relatively easy to monitor! can easily include or be used to derive sustainable harvesting rates 4.6 Near-term iterative ecological forecasts? So we have the initial data and models required to be able to monitor and forecast multiple Proteaceae species responses to harvesting, wildfire and changing climatic conditions. While the work by Merow et al. (2014), Treurnicht et al. (2021) and others represent ecological forecasts, they currently aren’t iterative, so we are missing the opportunity to learn and refine the models. This also means that we’re not feeding up-to-date information to planners and decision makers. What do we need to do to develop them into near-term iterative ecological forecasts? Figure 4.11: The iterative ecological forecasting cycle in the context of the scientific method, demonstrating how we stand to learn from making iterative forecasts. From lecture on data assimilation by Michael Dietze. (Please excuse that the colours of the distributions have changed from above…). Firstly, they are either not specific about when they are forecasting to (e.g. Treurnicht et al. (2021) ran their models to estimate extinction probabilities over 100 years in response to different harvesting regimes), or they are too far into the future to be amenable to iterative assessment, learning and updating (e.g. Merow et al. (2014) project to 2050). We need to make the forecasts near-term, such as a range of scenarios 5-10 years into the future, or 1-2 years into the next fire cycle. Secondly, while various parties hold various datasets, we need to coordinate data collection among citizen scientists, conservation authorities, scientists, etc and centralize data management so that it can feed data back into the modelling workflow. Thirdly, we need to adapt the models and workflow to be able to ingest and assimilate new data and produce new forecasts automatically. Lastly, we need to make sure that the models adequately characterize and propagate uncertainty throughout the analyses so that we can focus data collection and model development to reduce the uncertainty in forecasts. Fortunately, the models are built in a Bayesian framework and already do a pretty good job of handling uncertainty. Clearly, while huge effort has been invested into the demography of Proteaceae, and they are likely to be one of the lowest hanging fruit for development into near-term iterative ecological forecasts, there is a lot more work to be done! Still, the history of productive research on Proteaceae shows it would clearly be worth the effort, and we already have decades of data that allow us to learn by backcasting or forecasting from old to more recently collected data. Even if early forecasts are woefully wrong, they will help us learn and improve. Once the informatics pipeline is developed, we could start adding other species - e.g. restios, ericas, animals, etc. This is important because the fire requirements for Proteaceae are not necessarily representative of all Fynbos species… “The need to start forecasting is now; the time for making ecology more predictive is here, and learning by doing is the fastest route to drive the science forward.” - Dietze et al. (2018) References "],["reproducibility.html", "5 Reproducible research 5.1 The Reproducibility Crisis 5.2 Replication and the Reproducibility Spectrum 5.3 Reproducible Scientific Workflows 5.4 Why work reproducibly? 5.5 Barriers to working reproducibly", " 5 Reproducible research 5.1 The Reproducibility Crisis “Replication is the ultimate standard by which scientific claims are judged.” (Peng 2011) Replication is one of the fundamental tenets of science and if the results of a study or experiment cannot by replicated by an independent set of investigators then whatever scientific claims were made should be treated with caution! At best, it suggests that evidence for the claim is weak or mixed, or specific to particular ecosystems or other circumstances and cannot be generalized. At worst, there was error (or even dishonesty) in the original study and the claims were plainly false. In other words, published research should be robust enough and the methods described in enough detail that anyone else should be able to repeat the study (using the publication only) and find similar results. Sadly, this is rarely the case!!! Figure 5.1: ‘Is there a reproducibility* crisis?’ Results from a survey of &gt;1500 top scientists (Baker 2016; Penny 2016). *Note that they did not discern between reproducibility and replicability, and that the terms are often used interchangeably. We have a problem… Since we’re failing the gentleman’s agreement2 that we’ll describe our methods in enough detail that anyone else should be able to repeat the study (using the publication only) and find similar results, modern scientists are trying to formalize the process in the form of Reproducible Research. Reproducible research makes use of modern software tools to share data, code and other resources required to allow others to reproduce the same result as the original study, thus making all analyses open and transparent. Working reproducibly is not just a requirement for using quantitative approaches in iterative decision-making, it is central to scientific progress!!! 5.2 Replication and the Reproducibility Spectrum While full replication is a huge challenge (and sometimes impossible) to achieve, it is something all scientists should be working towards. Understandably, some studies may not be entirely replicable purely due to the nature of the data or phenomenon (e.g. rare phenomena, long term records, loss of species or ecosystems, or very expensive once-off science projects like space missions). In these cases the “gold standard” of full replication (from new data collection to results) cannot be achieved, and we have to settle for a lower rung on the reproducibility spectrum (Figure 5.2). Figure 5.2: The Reproducibility Spectrum (Peng 2011). Reproducibility falls short of full replication because it focuses on reproducing the same result from the same data set, rather than analyzing independently collected data. While this may seem trivial, you’d be surprised at how few studies are even reproducible, let alone replicable. 5.3 Reproducible Scientific Workflows Figure 5.3: ‘Data Pipeline’ from xkcd.com/2054, used under a CC-BY-NC 2.5 license. Working reproducibly requires careful planning and documentation of each step in your scientific workflow from planning your data collection to sharing your results. This entails a number of overlapping/intertwined components, namely: Data management - which we’ll spend more time on in Chapter 8 File and folder management Coding and code management - i.e. the data manipulation and analyses performed Computing environment and software Sharing of the data, metadata, code, publications and any other relevant materials For the rest of this section we’ll work through these components and some of the tools that help you achieve this. 5.3.1 File and folder management Project files and folders can get unwieldy fast, and can really bog you down and inhibit productivity when you don’t know where your files are or what the latest version is. Figure 5.4: ‘Documents’ from xkcd.com/1459, used under a CC-BY-NC 2.5 license. The two main considerations for addressing this issue are defining a simple, common, intuitive folder structure, and using informative file names. Folders Most ecological projects have similar requirements. Here’s a screenshot of how I usually (try to) manage my folders. “Code” we’ll deal with in the next section, but obviously contains R code etc to perform analyses. Within “Data” I often have separate folders of “Raw” and “Processed” data. If the data files are big and used across multiple projects (e.g. GIS files), then they’ll often be in a separate folder elsewhere on my computer, but this is well-documented in my code. “Output” contains figures and tables, often in separate folders. I also often have a “Manuscript” folder if I’m working in LaTeX/Sweave or RMarkdown, although this is often in the “Code” folder (since you can embed code in RMarkdown and Sweave documents). File and folder naming Your naming conventions should be: machine readable i.e. avoid spaces and funny punctuation support searching and splitting of names (e.g. “data_raw_CO2.csv”, “data_clean_CO2.csv”, “data_raw_species.csv” can all be searched by keywords and can be split by “_” into 3 useful fields: type (data vs other), class (raw vs clean), variable (CO2 vs species), etc) human readable the contents should be self evident from the file name support sorting i.e. use numeric or character prefixes to separate files into different components or steps (e.g. “data_raw_localities.csv”, “data_clean_localities.csv”, etc) some of this can be handled with folder structure, but you don’t want too many folders either Find out more about file naming here. 5.3.2 Coding and code management Why write code? Working in point-and-click GUI-based software like Excel, Statistica, SPSS, etc may seem easier, but you’ll regret it in the long run… The beauty of writing code lies in: Automation You will inevitably have to adjust and repeat your analysis as you get feedback from supervisors, collaborators and reviewers. Rerunning code is one click, and you’re unlikely to introduce errors. Rerunning analyses in GUI-based software is lots of clicks and it’s easy to make mistakes, alter default settings, etc etc. Next time you need to do the same analysis on a different dataset you can just copy, paste and tweak your code. You code/script provides a record of your analysis Linked to the above, mature scientific coding languages like Python or R allow you to run almost any kind of analysis in one scripted workflow, even if it has diverse components like GIS, phylogenetics, multivariate or Bayesian statistics, etc. Most proprietary software are limited to one or a few specialized areas (e.g. ArcGIS, etc), which leaves you manually exporting and importing data between multiple software packages. This is very cumbersome, in addition to being a file-management nightmare… Most scripting environments are open source (e.g. R, Python, JavaScript, etc) Anyone wanting to use your code doesn’t have to pay for a software license It’s great for transparency - Lots of people can and have checked the background code and functions you’re using, versus only the software owner’s employees have access to the raw code for most analytical software There’s usually a culture of sharing code (online forums, with publications, etc) Here’s a motivation and some tutorials to help you learn R. Some coding rules It’s easy to write messy code. This can make it virtually indecipherable to others (and even yourself), slowing you and your collaborations down. It also makes it easy to make mistakes and not notice them. The overarching rule is to write code for people, not computers. Check out the Tidyverse style guide for R-specific guidance, but here are some basic rules: use consistent, meaningful and distinct names for variables and functions use consistent code and formatting style use commenting to document and explain what you’re doing at each step or in each function - purpose, inputs and outputs “notebooks” like RMarkdown or Jupyter Notebooks are very handy for fulfilling roles like documentation, master/makefiles etc and can be developed into reports or manuscripts write functions rather than repeating the same code modularize code into manageable steps/chunks or even separate them into separate scripts that can all be called in order from a master script or Makefile check for mistakes at every step!!! Beyond errors or warnings, do the outputs make sense? start with a “recipe” that outlines the steps/modules (usually as commented headers etc). This is very valuable for keeping you organized and on track, e.g. a common recipe in R: #Header indicating purpose, author, date, version etc #Define settings #Load required libraries #Read in data #Wrangle/reformat/clean/summarize data as required #Run analyses (often multiple steps) #Wrangle/reformat/summarize analysis outputs for visualization #Visualize outputs as figures or tables avoid proprietary formats i.e. use an open source scripting langauge and open source file formats only use version control!!! Version control Using version control tools like Git, SVN, etc can be challenging at first, but they can also hugely simplify your code development (and adaptation) process. While they were designed by software developers for software development, they are hugely useful for quantitative biology. I can’t speak authoritatively on version control systems (I’ve only ever used Git and GitHub), but here are the advantages as I see them. This version is specific to Git, but I imagine they all have similar functions and functionality: Words in italics are technical terms used within GitHub. You can look them up here. You’ll also cover it in the brief tutorial you’ll do when setting up your computer for the practical. They generally help project management, especially collaborations They allow you to easily share code with collaborators or the public at large - through repositories or gists (code snippets) Users can easily adapt or build on each others’ code by forking repositories and working on their own branch. This is truly powerful!!! It allows you to repeat/replicate analyses but even build websites (like this one!), etc While the whole system is online, you can also work offline by cloning the repository to your local machine. Once you have a local version you can push to or pull from the online repository to keep everything updated Changes are tracked and reversible through commits. If you change the contents of a repository you must commit them and write a commit message before pulling or pushing to the online repository. Each commit is essentially a recoverable version that can be compared or reverted to This is the essence of version control and magically frees you from folders full of lists of files named “mycode_final.R”, “mycode_finalfinal.R”, “myfinalcode_finalfinal.R” etc as per Figure 5.4 They allow collaborators or the public at large to propose changes via pull requests that allow you to merge their forked branch back to the main (or master) branch They allow you to accept and integrate changes seamlessly when you accept and merge pull requests They allow you to keep written record of changes through comments whenever a commit or pull request is made - these also track the user, date, time, etc and are useful for blaming when things go wrong There’s a system for assigning logging and tracking issues and feature requests I’m sure this is all a bit much right now, but should make more sense after the practical… 5.3.3 Computing environment and software We’ve already covered why you should use open source software whenever possible, but it bears repeating. Using proprietary software means that others have to purchase software, licenses, etc to build on your work and essentially makes it not reproducible by putting it behind a pay-wall. This is self-defeating… Another issue is that software and hardware change with upgrades, new versions or changes in the preferences within user communities (e.g. you’ll all know MicroSoft Excel, but have you heard of Quattro Pro or Lotus that were the preferred spreadsheet software of yesteryear?). Just sharing your code, data and workflow does not make your work reproducible if we don’t know what language the code is written in or if functions change or are deprecated in newer versions, breaking your code. The simplest way to avert this problem is to carefully document the hardware and versions of software used in your analyses so that others can recreate that computing environment if needed. This is very easy in R, because you can simply run the sessionInfo() function, like so: sessionInfo() ## R version 4.2.1 (2022-06-23) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 20.04.4 LTS ## ## Matrix products: default ## BLAS: /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.9.0 ## LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.9.0 ## ## locale: ## [1] LC_CTYPE=en_ZA.UTF-8 ## [2] LC_NUMERIC=C ## [3] LC_TIME=en_ZA.UTF-8 ## [4] LC_COLLATE=en_ZA.UTF-8 ## [5] LC_MONETARY=en_ZA.UTF-8 ## [6] LC_MESSAGES=en_ZA.UTF-8 ## [7] LC_PAPER=en_ZA.UTF-8 ## [8] LC_NAME=C ## [9] LC_ADDRESS=C ## [10] LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_ZA.UTF-8 ## [12] LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats graphics grDevices utils datasets ## [6] methods base ## ## other attached packages: ## [1] hrbrthemes_0.8.0 forcats_0.5.1 stringr_1.4.0 ## [4] dplyr_1.0.9 purrr_0.3.4 readr_2.1.2 ## [7] tidyr_1.2.0 tibble_3.1.7 ggplot2_3.3.6 ## [10] tidyverse_1.3.1 ## ## loaded via a namespace (and not attached): ## [1] Rcpp_1.0.8.3 lattice_0.20-45 ## [3] lubridate_1.8.0 png_0.1-7 ## [5] assertthat_0.2.1 digest_0.6.29 ## [7] utf8_1.2.2 R6_2.5.1 ## [9] cellranger_1.1.0 backports_1.4.1 ## [11] reprex_2.0.1 evaluate_0.15 ## [13] httr_1.4.3 highr_0.9 ## [15] pillar_1.7.0 gdtools_0.2.4 ## [17] rlang_1.0.2 readxl_1.4.0 ## [19] rstudioapi_0.13 extrafontdb_1.0 ## [21] jquerylib_0.1.4 Matrix_1.4-1 ## [23] rmarkdown_2.14 labeling_0.4.2 ## [25] splines_4.2.1 extrafont_0.18 ## [27] munsell_0.5.0 broom_0.8.0 ## [29] compiler_4.2.1 modelr_0.1.8 ## [31] xfun_0.30 systemfonts_1.0.4 ## [33] pkgconfig_2.0.3 mgcv_1.8-40 ## [35] htmltools_0.5.2 tidyselect_1.1.2 ## [37] bookdown_0.26 fansi_1.0.3 ## [39] crayon_1.5.1 tzdb_0.3.0 ## [41] dbplyr_2.1.1 withr_2.5.0 ## [43] grid_4.2.1 nlme_3.1-157 ## [45] jsonlite_1.8.0 Rttf2pt1_1.3.10 ## [47] gtable_0.3.0 lifecycle_1.0.1 ## [49] DBI_1.1.2 magrittr_2.0.3 ## [51] scales_1.2.0 cli_3.3.0 ## [53] stringi_1.7.6 farver_2.1.0 ## [55] fs_1.5.2 xml2_1.3.3 ## [57] bslib_0.3.1 ellipsis_0.3.2 ## [59] generics_0.1.2 vctrs_0.4.1 ## [61] tools_4.2.1 glue_1.6.2 ## [63] jpeg_0.1-9 hms_1.1.1 ## [65] fastmap_1.1.0 yaml_2.3.5 ## [67] colorspace_2.0-3 rvest_1.0.2 ## [69] knitr_1.39 haven_2.5.0 ## [71] sass_0.4.1 Containers A “better” way to do this is to use containers like docker or singularity. These are contained, lightweight computing environments similar to virtual machines, that you can package with your software/workflow. You set your container up to have everything you need to run your code etc (and nothing extra), so anyone can download (or clone) your container, code and data and run your analyses perfectly first time. 5.3.4 Sharing of the data, code, publication etc This is touched on in more detail when we discuss data management in Chapter 8, but suffice to say there’s no point working reproducibly if you’re not going to share all the components necessary to complete your workflow… Another key component here is that ideally all your data, code, publication etc are shared Open Access - i.e. they are not stuck behind some paywall Figure 5.5: A 3-step, 10-point checklist to guide researchers toward greater reproducibility in their research (Alston and Rick 2021). 5.4 Why work reproducibly? Figure 5.6: Let’s start being more specific about our miracles… Cartoon © Sidney Harris. Used with permission ScienceCartoonsPlus.com In addition to basic scientific rigour, working reproducibly is hugely valuable, because: (Adapted from “Five selfish reasons to work reproducibly” (Markowetz 2015)) Its transparent and open, helping us avoid mistakes and/or track down errors in analyses This is what highlighted the importance of working reproducibly for me. In 2017 I published the first evidence of observed climate change impacts on biodiversity in the Fynbos Biome (Slingsby et al. 2017). The analyses were quite complicated, and when working on the revisions I found an error in my R code. Fortunately, it didn’t change the results qualitatively, but it made me realize how easy it is to make a mistake and potentially put the wrong message out there! This encouraged me to make all data and R code from the paper available, so that anyone is free to check my data and analyses and let me (and/or the world) know if they find any errors. It makes it easier to write papers e.g. Dynamic documents like RMarkdown or Jupyter Notebooks update automatically when you change your analyses, so you don’t have to copy/paste or save/insert all tables and figures - or worry about whether you included the latest versions. It helps the review process Often issues picked at by reviewers are matters of clarity/confusion. Sharing your data and analyses allows them to see exactly what you did, not just what you said you did, allowing them to identify the problem and make constructive suggestions. It’s also handy to be able to respond to a reviewer’s comment with something like: “That’s a great suggestion, but not really in line with the objectives of the study. We have chosen not to include the suggested analysis, but do provide all data and code so that interested readers can explore this for themselves.” (Feel free to copy and paste - CCO 1.0) It enables continuity of the research When people leave a project (e.g. students/postdocs), or you forget what you did X days/weeks/months/years ago, it can be a serious setback for a project and make it difficult for you or a new student to pick up where things left off. If the data and workflow are well curated and documented this problem is avoided. Trust me, this is a very common problem!!! I have many papers that I (or my students) never published and may never go back to, because I know it’ll take me a few days or weeks to understand the datasets and analyses again… This is obviously incredibly important for long-term projects! A little bit of extra effort early on can save a lot of time further down the road!!! It helps to build your reputation Working reproducibly makes it clear you’re an honest, open, careful and transparent researcher, and should errors be found in your work you’re unlikely to be accused of dishonesty (e.g. see my paper example under point 1 - although no one has told me of any errors yet…). When others reuse your data, code, etc you’re likely to get credit for it - either just informally, or formally through citations or acknowledgements (depending on the licensing conditions you specify - see “Preserve” in the Data Life Cycle). And some less selfish reasons (and relevant for ecoforecasting): It speeds progress in science by allowing you (or others) to rapidly build on previous findings and analyses Somewhat linked to point 4, but here the focus is on building on published work. For example, if I read a paper and have an idea (or develop a new hypothesis) that may explain some of their results or add to the story, I can start from where they left off rather than collecting new data, recoding their whole analysis, etc before I can even start. It allows easy comparison of new analytical approaches (methods, models, etc) to older ones Linked to 6, but more specific to model or methods development where the need to “benchmark” your new method relative to an older one is important. If the benchmark model exists you can make an honest comparison, but if you have to set up the older one yourself, some may accuse you of gaming the system by chosing settings etc that advantage your new method. It makes it easy to repeat the same analyses when new data are collected or added This is key for iterative forecasting, but also useful if you want to apply the same method/model to a new data set (e.g. applying Merow et al. (2014)’s analysis on all 26 Proteaceae studied by Treurnicht et al. (2016)). Most skills and software used in Reproducible Research are very useful beyond Reproducible Research alone! e.g. GitHub - in addition to versioning code etc is great for code management, collaborative projects and can be used for all kinds of things like building websites (e.g. these course notes). And one more selfish reason (but don’t tell anyone I said this): Reproducible research skills are highly sought after in careers like data science etc… Skills are important should you decide to leave biology… Even within biology, more and more environmental organizations and NGOs are hiring data scientists or scientists with strong data and quantitative skills. Some examples I know of: The South African Environmental Observation Network (SAEON - especially their data node uLwazi) The Endangered Wildlife Trust (EWT) The Nature Conservancy 5.5 Barriers to working reproducibly (Adapted from “A Beginner’s Guide to Conducting Reproducible Research” (Alston and Rick 2021)) 1. Complexity There can be a bit of a learning curve in getting to know and use the tools for reproducible research effectively. One is always tempted by the “easy option” of doing it the way you already know or using “user-friendly” proprietary software. 2. Technological change Hardware and software used in analyses change over time - either changing with updates or going obsolete altogether - making it very difficult to rerun old analyses. This should be less of a problem going forward because: it is something people are now aware of (so we’re working on solutions) we’re increasingly using open source software, for which older versions are usually still made available and there is little risk of it disappearing when the software company stops supporting the software or goes bankrupt documenting hardware and software versions with analyses is an easy baseline increasingly people are using contained computing environments as we’ll discuss below 3. Human error Simple mistakes or failure to fully document protocols or analyses can easily make a study irreproducible. Most reproducible research tools are aimed at solving this problem. 4. Intellectual property rights Rational self-interest can lead to hesitation to share data and code via many pathways: Fear of not getting credit; Concern that the materials shared will be used incorrectly or unethically; etc Hopefully most of these issues will be solved by better awareness of licensing issues, attribution, etc, as the culture of reproducible research grows References "],["practical.html", "6 Practical: Pair coding with GitHub 6.1 Objectives 6.2 Postfire regeneration 6.3 Modular Design 6.4 Task 1: Create &amp; Clone Repository 6.5 Task 2: Add the first function: download.NDVI 6.6 Task 3: Collaborator adds plot.NDVI 6.7 Task 4: Owner adds functions for model fitting using MLE 6.8 Task 5: Collaborator adds the master script, Owner answers the questions", " 6 Practical: Pair coding with GitHub Just a quick acknowledgement that I have adapted much of the framework for the following from Michael Dietze’s Pair coding practical that is a living online supplement to the book (Dietze 2017a). Thanks for sharing the code under an MIT license Mike! The main differences are that I’ve changes the subject matter from looking at phenology in Tall Grass Prairie using PhenoCam data to looking at postfire vegetation growth in Fynbos using MODIS satellite Normalized Difference Vegetation Index (NDVI), a measure of vegetation “greenness”. This includes changing the core model we fit too. To complete this practical, you need to have done the preparation outlined in section 2. 6.1 Objectives The primary goal of this exercise is to gain experience working collaboratively to develop a scientific workflow. As such, this assignment must be completed with a partner. Specifically, we will outline an analysis, break the overall job into parts, and have each person complete part of the project. To put these parts together we will be using GitHub. Along the way we will also be exploring the statistical concept of Likelihood by fitting the same model and estimating the parameters with two different approaches - Least Squares (what you’d usually use for traditional linear models etc) and Maximum Likelihood. 6.2 Postfire regeneration The goal of our analysis is to investigate the regeneration of Fynbos vegetation after fire by exploring a time-series of 16-day composite Normalized Difference Vegetation Index (NDVI) from the MODIS satellite mission, a measure of vegetation “greenness”. You can read up on the data product here. We’ll be using data for one location (250m by 250m pixel) from the Silvermine section of Table Mountain National Park, and will be exploring the NDVI trajectory since the March 2015 fire. You can view the raw data in comma-separated value (.csv) format here. The workflow for this analysis with have three components: Download MODIS NDVI data Visualize the data Fit two competing negative exponential models using non-linear least squares (NLS) Fit the same models using Maximum Likelihood Estimation (MLE) and compare them using Akaike’s Information Criterion (AIC) From this overall design, let’s next outline the specific steps involved as pseudocode ### Fynbos Postfire Workflow ## 1. Download NDVI data for one postfire recovery cycle ## 2. Visualize data ## 3. Fit two variants of the negative exponential model with NLS ## 4. Visualize models and data ## 5. Fit two variants of the negative exponential model with MLE ## 6. Visualize models and data ## 7. Compare model variants using AIC 6.3 Modular Design From this overall design we can look for ways to modularize the analysis. One feature that jumps out is that we need to visualize the data three times (steps 2, 4 and 6), so we should definitely make a function to do that. The inputs to the function would be an independent variable (age) and a dependent variable (NDVI), which we might pass to the function as a dataframe for convenience. Since this is a graphing function, we’d also like the ability to set all the usual plotting parameters. This can be done in R by passing ... as an argument to our new function, which allows us to pass other arguments to the internal plot() call. The proposed function interface (i.e. metadata about inputs and outputs, but excluding the code) would thus be: ##&#39; Plot NDVI data ##&#39; ##&#39; @param dat dataframe that contains columns &quot;age&quot; and &quot;NDVI&quot; ##&#39; @param fit a fitted model to overlay on the data if present ##&#39; @param ... additional graphing parameters ##&#39; plot.NDVI &lt;- function(dat, fit = NA, ...) Next, because the raw data will be downloaded off the web and we need to convert the dates from “character” to “Date” class and convert the NDVI data from Digital Numbers (DN; most satellite sensors store and transmit data as DN for various reasons such as because they are smaller and easier to transmit) to true NDVI values, let’s go ahead and create a download function. This function just needs to know the URL for where to find the data. Unlike the plot function, this function will return something (the data that was downloaded), so it would be good design to document what is returned and how it will be formatted: ##&#39; Download NDVI data ##&#39; ##&#39; @param URL web address where data is located ##&#39; @return data.frame with 16-day windows as rows, variables as columns. Variables include calendar date (as class &quot;Date&quot; and NDVI values as class &quot;numeric&quot; ranging -1 to 1.) download.NDVI &lt;- function(URL) Next we’ll fit two variants of a negative exponential model to the postfire NDVI trajectory data, and we’re going to do this twice - once using non-linear least squares (NLS), and again using Maximum Likelihood Estimation (MLE). Fortunately, there is already an efficient base R function for fitting the NLS model nls(), so we’ll only need to define functions for the MLE fits. The input to such a fit would obviously be the same data.frame that we’re using to make the plot. We’ll also need to input a vector of initial guesses at the model parameters to help the numerical optimization converge, and we’ll want to return the full output from that numerical optimization so that we can check if it converged successfully. Finally, optimizing MLE requires defining the model, and defining a function to optimize the parameters by minimizing the negative log of the likelihood We’re going to make (a) - the model - an independent function that we can call within (b), but can also call independently. The reason we want to be able to call the model independently is that the MLE optimization will only return the list if parameters of the model (and not the model itself), so we won’t be able to plot the curve. If we make (a) a separate function, we can feed it the estimated model parameters to perform the model calculation and predict the shape of the curve, allowing us to plot it. In short, since we need the same function for two separate applications (the optimization and the plotting), it’s more efficient to define it as a separate function. First, we’ll do the functions for the simpler variant of the model (don’t be alarmed by the mysterious parameter names, they’ll be explained in in a later section: ##&#39; Function (a) to define the model for the simple negative exponential model using MLE: ##&#39; @param theta vector of model parameters in order: alpha, gamma, lambda ##&#39; @param x vector of x values ##&#39; @return vector of model predictions pred.negexp &lt;- function(theta, x) ##&#39; Function (b) to fit the simple negative exponential model and minimize the -ln.likelihood ##&#39; @param dat dataframe of NDVI, age ##&#39; @param par vector of initial parameter guesstimates (on order of theta) ##&#39; @return output from numerical optimization fit.negexp.MLE &lt;- function(dat,par) Then we’ll do the functions for the full model: ##&#39; Function (a) to define the full model using MLE: ##&#39; @param theta vector of model parameters in order: alpha, gamma, lambda, A, phi (NOTE THE TWO EXTRA PARAMETERS!) ##&#39; @param x vector of x values ##&#39; @return vector of model predictions pred.negexpS &lt;- function(theta, x) ##&#39; Function (b) to fit the full model and minimize the -ln.likelihood ##&#39; @param dat dataframe of NDVI, age ##&#39; @param par vector of initial parameter guesstimates (on order of theta) ##&#39; @return output from numerical optimization fit.negexpS.MLE &lt;- function(dat,par) At this point we’ve spent a good bit of time up front on organization – we have a detailed plan of attack and have thought carefully about what each module is responsible for doing. Each task has well-defined inputs, outputs, and goals. Rather than facing a thankless job of documenting our code after we’re done, even though we haven’t written a single line of code yet, we are largely done with our documentation. What remains to do is implementation. 6.4 Task 1: Create &amp; Clone Repository Because we’re going to employ version control in our project, our first step is to create the repository that our project will be stored in. To ensure that both you and your partner get to see every step of how to work with version control, for the rest of this exercise you are going to complete every step, either from the perspective of the OWNER of the repository or as the COLLABORATOR. 6.4.1 OWNER We begin with each of you creating a repository for which you are the “Owner” and your partner will be the “Collaborator” on. In this case I am using Github Classroom to run and evaluate the assignment, so the first few steps are a little different to how you would do this in a usual workflow. Go to the Github classroom link I shared with you via Vula Scroll until you find your name and click the arrow &gt; Follow the prompts until it tells you Your team’s assignment repository has been created and gives you a link to a Git repository (or “repo”) which should start https://github.com/PlantEcologi/ecoforecast-pair-coding-... Now skip to step 6 below …Ok fine. Here’s a random step 5 for symmetry’s sake… How you would normally create a repo (ignore this for the prac): Go to your account on github.com and under the Repositories tab click on the “New” button (green with a picture of a book on it) Choose a name for your repository, but make sure it’s different from your partner’s (Don’t choose a “Repository template”, and keep it a “Public” repository) Click the “Initialize this repository with a README” checkbox Optionally also provide a Description, Add a licence (e.g. MIT), and add R to the .gitignore (check “.gitignore” and search for the R template) Click “Create Repository” Copy the URL of your new repository To clone the repository to your computer, open up RStudio so we can create a New Project using this URL. Note: If you already have a project open it will close when you do so. Don’t worry, you can return to that project after the prac using the drop-down in the top-right of the RStudio window. Select New Project from the drop-down menu in the top right corner Select Version Control then Git Paste the URL in and click Create Project 6.5 Task 2: Add the first function: download.NDVI Within this project we’ll create separate files for each part of the analysis. To make the order of the workflow clear we’ll want to name the files systematically. In the first file we’ll implement the download.NDVI() function. NOTE: You’ll see that I’ve provided the R code for how I actually accessed the data using library(“MODISTools”), but have left that commented out. The function can take quite a long time so I thought best that we just read the output of that call from a file I’ve saved to the GitHub repository. ##&#39; Download MODIS NDVI data ##&#39; @param URL web address where data is located ##&#39; ## 1) How I really did it. For the prac we&#39;ll use option 2 because it&#39;s faster ## library(&quot;MODISTools&quot;) #Call R library MODISTools that allows us to download MODIS satellite data directly into R ## ## ndvi &lt;- mt_subset(product = &quot;MOD13Q1&quot;, ## lat = -34.100875, ## lon = 18.449375, ## band = &quot;250m_16_days_NDVI&quot;, ## start = &quot;2000-01-01&quot;, ## end = &quot;2021-10-01&quot;, ## progress = FALSE) ## ## 2) How we&#39;ll do it for the prac: Read the data from a .csv file in my github repository for the course notes ## download.NDVI &lt;- function(URL) { # Wrap function in an if/else loop that checks if the URL is valid if (length(URL) == 1 &amp; is.character(URL) &amp; substr(URL,1,4)==&quot;http&quot;) { # Read in data modat &lt;- read.csv(URL) # Convert Digital Numbers (more efficient for data storage) to NDVI modat$NDVI &lt;- modat$value*0.0001 # Convert calendar_date to class &quot;Date&quot; modat$calendar_date &lt;- as.Date(as.character(modat$calendar_date)) # Return the data return(modat) } else { # If the URL is not valid return... print(paste(&quot;download.NDVI: Input URL not provided correctly&quot;,URL)) } } 6.5.1 OWNER In RStudio, click File &gt; New File &gt; R Script Copy and Paste the above function into this file Select the code you have pasted in go to the menu Code &gt; Comment/Uncomment lines or alternatively use the quick keystroke Ctrl + Shift + c at once to remove the comments # Save the file as “01_download.NDVI.R” in the default directory for the R project Note: Using the exact file name is important, because we’ll be calling this file from other code later From the “Git” tab (top-right window in RStudio), click the box next to the file you just created. This is equivalent to git add if you were doing this command line in bash or terminal Click Commit, enter a log message, and click Commit again. This is equivalent to git commit in command line To “push” the change up to the online Git repo on Github click on the green up arrow. This is equivalent to git push. Note: It is generally good practice to always git pull by clicking the blue down arrow before pushing, so that you sync any changes any collaborators may have made since you last pulled or pushed. In this case we know this is the first thing added to the repo and no one else has access anyway. At this stage you can check on the Github Classroom Scoreboard to see if you have completed everything successfully. Your team should be scoring 1/5 at this stage. You may need to give it a minute to run the checks. 6.6 Task 3: Collaborator adds plot.NDVI With the first function complete, let’s now imagine that a COLLABORATOR has been tasked with adding the second function. To do so they must first fork and clone the repository 6.6.1 COLLABORATOR Go to Github and navigate to the project repository within the OWNER’s workspace. Click Fork, which will make a copy of the repository to your own Github workspace. Copy the URL to your own version of the repo and follow the instructions above for cloning the repository to your local machine in RStudio. Open a new file, enter the code below, and then save the file as “02_plot.NDVI.R” ##&#39; Plot NDVI data ##&#39; ##&#39; @param dat dataframe that contains columns &quot;age&quot; and &quot;NDVI&quot; ##&#39; @param fit a fitted model to overlay on the data if present ##&#39; @param ... additional graphing parameters ##&#39; plot.NDVI &lt;- function(dat, fit = NA, ...){ if(!is.null(dat)){ # Begin if/else statement # Base plot of the data points plot(dat$age, dat$NDVI, ylab = &quot;NDVI&quot;, xlab = &quot;Postfire age (Years)&quot;) if(!is.na(fit[1])){ #Begin inner if statement # Overlay the fitted model on the plot lines(dat$age, predict(fit, list(x = dat$age)), col = &#39;skyblue&#39;, lwd = 3) } # End inner if statement } else { print(&quot;plot.NDVI: input data not provided or invalid&quot;) } # End if/else statement } Follow the instructions above to Add, Commit, and Push the file back to your Github repo Next you want to perform a “pull request”, which will send a request to the OWNER that they pull your new code into their mainline version. From your Github repo (not the owner’s!), click on Pull requests (top left) and hit the green New Pull Request button. Follow the instructions, creating a title, message, and confirming that you want to create the pull request (you may be asked to confirm a couple of times). 6.6.2 OWNER Once the COLLABORATOR has created the pull request, you should get an automatic email and also be able to see the pull request under the `Pull Requests” tab on your Github page for the project. Read the description of the proposed changes and then click on Files Changed to view the changes to the project. New code should be in green, while deleted code will be in pink. The purpose of a pull request is to allow the OWNER to evaluate the code being added before it is added. You can see this option under the Review button. You can also hover your mouse over any line of code and insert an inline comment in the code (don’t do this during the prac). The COLLABORATOR would then have the ability to respond to any comments. In larger projects, all participants can discuss the code and decide whether it should be accepted or not. Furthermore, if the COLLABORATOR does any further pushes to Github before the pull request is accepted these changes will automatically become part of the pull request. While this is a very handy feature, it can also easily backfire if the COLLABORATOR starts working on something different in the meantime. This is the reason that experienced users of version control will use BRANCHES to keep different parts separate. Click on the Conversation page to return where you started. All participants can also leave more general comments on this page. If you are happy with the code, click Merge Pull Request. Alternatively, to outright reject a pull request you could click Close pull request (but please don’t do this in the prac unless your partner has made an error - PLEASE CHECK IF THEY’VE ADDED THE RIGHT FUNCTION ETC) The Github Classroom Scoreboard should reflect 2/5 for your team at this stage. 6.7 Task 4: Owner adds functions for model fitting using MLE We are now past the ‘set up’ stage for both the OWNER and the COLLABORATOR, so for this task we’ll explore the normal sequence of steps that the OWNER will use for day-to-day work 6.7.1 OWNER Pull the latest code from Github (which includes the new function added by your collaborator). In RStudio this is done by clicking the light blue down arrow on the Git tab. This is equivalent to the command line git pull origin master where origin refers to where you did your original clone from and master refers to your main branch (if you use branches you can pull other branches). Once you’ve pulled the latest code you should see the latest function file added to the files you can see in the Files tab of the bottom-right window of RStudio. Next, open up a new R file, add the code below, and save as “03_negexp.R” ##&#39; Functions to fit negative exponential model using MLE ##&#39; ##&#39; 1) Fit SIMPLE negative exponential model using maximum likelihood estimation ##&#39; ##&#39; Function (a) to define the model for the SIMPLE negative exponential model using MLE: ##&#39; @param theta parameter vector in order: alpha, gamma, lambda ##&#39; @param x vector of x values ##&#39; @return vector of model predictions pred.negexp &lt;- function(theta, x){ NDVI = theta[1] + theta[2] * (1 - exp(- x/theta[3])) } ##&#39; Function (b) to fit the SIMPLE negative exponential model and minimize the -ln.likelihood ##&#39; @param dat dataframe of NDVI, age ##&#39; @param par vector of initial parameter guesstimates (on order of theta) ##&#39; @return output from numerical optimization fit.negexp.MLE &lt;- function(dat,par){ ## define log likelihood lnL.negexp &lt;- function(theta,dat){ -sum(dnorm(dat$NDVI, pred.negexp(theta, dat$age), 0.001, log=TRUE), na.rm=TRUE) #Note that I added a standard deviation of 0.001 (in reality we should get that from the MODIS data) } ## fit by numerical optimization optim(par, fn = lnL.negexp, dat=dat, control = list(maxit = 1000)) } ########################################## ##&#39; 2) Fit negative exponential plus mystery term using maximun likelihood estimation ##&#39; Function (a) to define the FULL model using MLE: ##&#39; @param theta parameter vector in order: alpha, gamma, lambda, A, phi ##&#39; @param x vector of x values ##&#39; @return vector of model predictions pred.negexpS &lt;- function(theta, x){ NDVI = theta[1] + theta[2] * (1 - exp(- x/theta[3])) + theta[4] * sin(2*pi*x + (theta[5] + pi/6*(3 - 1))) } ##&#39; Function (b) to fit the full model and minimize the -ln.likelihood ##&#39; ##&#39; @param dat dataframe of NDVI, age ##&#39; @param par vector of initial parameter guesstimates (on order of theta) ##&#39; @return output from numerical optimization fit.negexpS.MLE &lt;- function(dat,par){ ## define log likelihood lnL.negexpS &lt;- function(theta,dat){ -sum(dnorm(dat$NDVI, pred.negexpS(theta, dat$age), 0.001, log=TRUE), na.rm=TRUE) #Note that I added a standard deviation of 0.001 (in reality we should get that from the MODIS data) } ## fit by numerical optimization optim(par, fn = lnL.negexpS, dat=dat, control = list(maxit = 1000)) } As before, add your new file under the Git tab, Commit the change, and push it back to Github The Github Classroom Scoreboard should reflect 3/5 for your team at this stage. A quick explanation of the Maximum Likelihood Estimation (MLE) functions To estimate the parameters in the model this function uses the likelihood principle which states that “a parameter value is more likely than another if it is the one for which the data are more probable”. In other words, the maximum likelihood estimate of a parameter is the value of the parameter for which the probability of obtaining the observed data if the highest. To use MLE we need to define a Likelihood, which is the relationship between the value of the parameter and the probability of some observed data. [For the record, the Likelihood is not a probability distribution because it does not integrate to 1]. In this case we’re assuming a Normal likelihood (hence the use of dnorm() in the function) and use a standard deviation (0.001) that I’ve made up to represent the uncertainty (ideally this should come from the data; e.g. the radiative transfer modelling that was used to estimate the “surface reflectance” MODIS product from the “top of atmosphere reflectance” that is actually observed by the satellite). In a more detailed analysis we’d want to follow up to check both these assumptions, but it’s a simple starting point for this practical demonstration. Applying the likelihood principle we would then look for the most likely value of \\(\\theta\\), the vector of parameters in the model (\\(\\alpha\\), \\(\\gamma\\) and \\(\\lambda\\) in the simpler model, and adding \\(A\\) and \\(\\phi\\) in the full model - see model equations below), which we call the Maximum Likelihood estimate. For a number or reasons that we won’t go into in this module, it is common to work with negative log likelihoods instead of likelihoods, in which case the negative implies that instead of looking for the maximum we’re now looking for the minimum (perhaps a bit difficult to get your head around, but it’s a small trade-off that makes for much easier mathematics). The fact that logarithm is a monotonic transformation means that taking the log does not change the location of this minimum. The code for this comes in three parts. First are the models themselves, pred.negexp() and pred.negexpS(), which translate the equations: For the “simple” model (pred.negexp()) it’s just a negative exponential: \\[\\begin{gather} \\text{NDVI}_{i,t}=\\alpha_i+\\gamma_i\\Big(1-e^{-\\frac{age_{i,t}}{\\lambda_i}}\\Big) \\end{gather}\\] For the “full” model (pred.negexpS()) we add a sine term: \\[\\begin{gather} \\text{NDVI}_{i,t}=\\alpha_i+\\gamma_i\\Big(1-e^{-\\frac{age_{i,t}}{\\lambda_i}}\\Big)+ A_i\\text{sin}\\Big(2\\pi\\times\\text{age}_{i,t}+\\Big[\\phi+\\frac{\\pi}{6}(m_{i,t}-1)\\Big]\\Big) \\end{gather}\\] Where: \\(\\alpha\\) is the NDVI at time 0 (i.e. directly after the fire) \\(\\gamma\\) is the maximum average increase in NDVI i.e. the maximum NDVI reached by the blue curve is \\(\\alpha + \\gamma\\) \\(\\lambda\\) is the rate of increase in NDVI \\(A\\) is the amplitude of the sine term \\(\\phi\\) adjusts the timing of the sine term to account for the month the fire occurred Second is the negative log likelihood function, lnL.negexp(), which we’re trying to minimize. The core of this is the Normal probability density, dnorm(). The first argument is the data, the second the is model, and the third is the standard deviation. The fourth argument says that we want to return the log density, which is much more accurate if it’s performed internally than if we take the log of what’s returned by dnorm. Since we have many data points dnorm returns a vector, which we then sum up and change the sign to turn this into a minimization problem. The third part is a call to a numerical optimization function, optim, that searches through parameter space to find the set of parameters that minimize the negative log likelihood (i.e. that Maximize the Likelihood). Arguments are the initial parameter guesstimates, the function being minimized, and any additional parameters that get passed on to that function. 6.8 Task 5: Collaborator adds the master script, Owner answers the questions The day-to-day workflow for the COLLABORATOR is similar, but not exactly the same as the OWNER. The biggest differences are that the COLLABORATOR needs to pull from the OWNER, not their own repository, and needs to do a pull request after the push. 6.8.1 COLLABORATOR Pull from OWNER. Unfortunately, this has to be done from the command line rather than the pull button within RStudio, which just pulls from the COLLABORATOR’s repository. In RStudio go to Tools &gt; Shell to open a terminal At the terminal type git pull URL main where URL is the address of the OWNER’s Github repository. Because it is a pain to always remember and type in the OWNER’s URL, it is common to define this as upstream git remote add upstream URL which is a one-time task, after which you can do the pull as git pull upstream main Navigate to https://github.com/jslingsby/BIO3019S_Ecoforecasting/blob/master/_04_Master.Rmd, copy the raw contents Open a new Rmd file in RStudio and paste the contents. Save this file as “04_Master.Rmd”. Within RStudio’s Git tab, add the file and Commit. Use the Push (up arrow) button to push this to your own repository On Github.com, submit a pull request 6.8.2 OWNER Evaluate and accept pull request. At this point your workflow should be complete and you should be able to run the analysis. The Github Classroom Scoreboard should reflect 5/5 for your team at this stage, but you still need to answer the questions and knit and upload the .html output (steps 2-5). If the scoreboard reflects 4/5 then most likely your “04_Master.Rmd” file won’t knit properly, suggesting you’ve done something wrong. You need to fix this bfore moving on to the next step. Pull the updated repo to your local machine Open “04_Master.Rmd” and answer the questions wherever you see the numbered questions “Q” and “&gt;Answer:” Click Knit (top-centre of script window in RStudio) to generate an html document (it should pop up). Note that if you change an answer in the .Rmd file you will need to Knit it again for it to be reflected in the .html Once complete, commit “04_Master.Rmd” and “04_Master.html” and push them back to the repo. Again, if you need to change any answers in the .Rmd you need to do the whole Knit &gt; Commit &gt; Push. I will only mark the latest version pushed before the assignment deadline. Finally, upload the same “04_Master.html” to Vula so that your assignment is marked as complete and I can add a grade there. References "],["practical-output-master-script-for-postfire-analysis.html", "7 Practical output: Master script for postfire analysis 7.1 Source functions, get data and plot 7.2 Fit models using Non-linear Least Squares (NLS) 7.3 Compare NLS models using ANOVA 7.4 Fit models using Maximum Likelihood Estimation (MLE) 7.5 Compare MLE models using Akaike’s information criterion (AIC)", " 7 Practical output: Master script for postfire analysis 7.1 Source functions, get data and plot First we’ll source() (i.e. “run all code in”) the scripts with the functions we made. Then we’ll set the URL, read in the data with download.NDVI(), and plot it with plot.NDVI(). ## Load required functions by running source() on the individual function files if(file.exists(&quot;praccode/01_download.NDVI.R&quot;)) source(&quot;praccode/01_download.NDVI.R&quot;) if(file.exists(&quot;praccode/02_plot.NDVI.R&quot;)) source(&quot;praccode/02_plot.NDVI.R&quot;) if(file.exists(&quot;praccode/03_negexp.R&quot;)) source(&quot;praccode/03_negexp.R&quot;) ## Download NDVI data URL = &quot;https://raw.githubusercontent.com/jslingsby/BIO3019S_Ecoforecasting/master/data/modisdata.csv&quot; dat &lt;- download.NDVI(URL) # Convert &quot;calendar_date&quot; to postfire age in days since fire - assuming the first date in the times eries is the time of the fire dat$age &lt;- (as.numeric(dat$calendar_date) - min(as.numeric(dat$calendar_date), na.rm = T))/365.25 ## Plot overall NDVI time series plot.NDVI(dat) Q1: This plot suggests that Fynbos greenness (NDVI) as observed from satellite saturates with time since fire. Why do you think it saturates rather than increasing linearly with time? Answer 1: 7.2 Fit models using Non-linear Least Squares (NLS) Now we’ll fit the simple and full negative exponential models using Non-linear Least Squares (NLS). First the simpler model: ## Simple model # set parameters par &lt;- c(alpha = 0.2, gamma = 0.4, lambda = 0.5) # fit model fit_negexp &lt;- nls(NDVI ~ alpha + gamma * (1 - exp(- age/lambda)), data = dat, start = par, trace = F, control = nls.control(maxiter = 500)) # plot plot.NDVI(dat = dat, fit = fit_negexp) And let’s look at the model summary with parameter estimates # print model summary summary(fit_negexp) ## ## Formula: NDVI ~ alpha + gamma * (1 - exp(-age/lambda)) ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## alpha 0.25107 0.02887 8.695 1.04e-14 *** ## gamma 0.32371 0.02723 11.887 &lt; 2e-16 *** ## lambda 1.17687 0.21396 5.500 1.84e-07 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.07302 on 135 degrees of freedom ## ## Number of iterations to convergence: 12 ## Achieved convergence tolerance: 3.924e-06 Now the full model: ## Full model # set parameters par &lt;- c(alpha = 0.2, gamma = 0.4, lambda = 0.5, A = 0.6, phi = 0) # fit model fit_negexpS &lt;- nls(NDVI ~ alpha + gamma * (1 - exp(- age/lambda)) + A*sin(2*pi*age + (phi + pi/6*(3 - 1))), data = dat, start = par, trace = F, control = nls.control(maxiter = 500)) # plot plot.NDVI(dat = dat, fit = fit_negexpS) # print model summary summary(fit_negexpS) ## ## Formula: NDVI ~ alpha + gamma * (1 - exp(-age/lambda)) + A * sin(2 * pi * ## age + (phi + pi/6 * (3 - 1))) ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## alpha 0.207522 0.024948 8.318 9.31e-14 *** ## gamma 0.364746 0.023926 15.245 &lt; 2e-16 *** ## lambda 0.989154 0.126064 7.846 1.25e-12 *** ## A 0.063136 0.007114 8.875 4.12e-15 *** ## phi -0.839167 0.111887 -7.500 8.10e-12 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.05835 on 133 degrees of freedom ## ## Number of iterations to convergence: 15 ## Achieved convergence tolerance: 6.939e-06 Lots more parameters… Q2: How do the estimates for the common parameters compare? Answer 2: 7.3 Compare NLS models using ANOVA Modelers often want to know which of a set of models are better. One way to do this when comparing nested* models using least squares is using analysis of variance (ANOVA). In this case the anova() function will take the model objects as arguments, and return an ANOVA testing whether the full model results in a significant reduction in the residual sum of squares (and thus is better at capturing the data), returning an F-statistic, Degrees of Freedom (the difference in the number of parameters between the models) and p-value. *i.e. one model is a subset of the other, as in our case anova(fit_negexp, fit_negexpS) ## Analysis of Variance Table ## ## Model 1: NDVI ~ alpha + gamma * (1 - exp(-age/lambda)) ## Model 2: NDVI ~ alpha + gamma * (1 - exp(-age/lambda)) + A * sin(2 * pi * age + (phi + pi/6 * (3 - 1))) ## Res.Df Res.Sum Sq Df Sum Sq F value Pr(&gt;F) ## 1 135 0.71976 ## 2 133 0.45280 2 0.26696 39.207 4.12e-14 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Q3: Which model is better? Answer 3: Q4: How many degrees of freedom are there in this ANOVA and why (i.e. what are they)? Answer 4: 7.4 Fit models using Maximum Likelihood Estimation (MLE) First let’s fit the simpler model: ## Fit the simpler model using MLE # set parameters par &lt;- c(alpha = 0.2, gamma = 0.4, lambda = 0.5) # fit model fit_negexpMLE &lt;- fit.negexp.MLE(dat, par) # plot plot.NDVI(dat) # add curve with MLE parameters lines(dat$age, pred.negexp(fit_negexpMLE$par,dat$age), col = &#39;skyblue&#39;, lwd = 3) fit_negexpMLE ## $par ## alpha gamma lambda ## 0.2510442 0.3237419 1.1767370 ## ## $value ## [1] 359053.6 ## ## $counts ## function gradient ## 118 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL Then the full model: ## Fit the full model using MLE # set parameters par &lt;- c(alpha = 0.2, gamma = 0.4, lambda = 0.5, A = 0.6, phi = 0) # fit model fit_negexpMLES &lt;- fit.negexpS.MLE(dat, par) # plot plot.NDVI(dat) # add curve with MLE parameters lines(dat$age, pred.negexpS(fit_negexpMLES$par,dat$age), col = &#39;skyblue&#39;, lwd = 3) fit_negexpMLES ## $par ## alpha gamma lambda A ## 0.20772317 0.36449293 0.98919689 0.06310554 ## phi ## -0.83741663 ## ## $value ## [1] 225574.7 ## ## $counts ## function gradient ## 914 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL 7.5 Compare MLE models using Akaike’s information criterion (AIC) Note that we can’t compare our MLE models using ANOVA because our custom functions do not return full model fits like the nls() function - only the parameter estimates, negative log-likelihoods and a few other diagnostics. Another way to compare models (and probably the most common) is using the Akaike information criterion (AIC), which is an estimator of prediction error (i.e. relative quality) of statistical models for a given set of data. The formula for the Akaike information criterion is: \\(AIC = 2K -2(ln(L))\\) Where: \\(k\\) = the number of estimated parameters in the model \\(L\\) = maximum value of the likelihood function for the model Since we have our negative log likelihoods (i.e. \\(-ln(L)\\) in the formula above), we can calculate the AICs and compare them. AIC_simple = 6 + 2*fit_negexpMLE$value AIC_simple ## [1] 718113.1 AIC_full = 6 + 2*fit_negexpMLES$value AIC_full ## [1] 451155.3 When comparing models, the lower the AIC the better, and in general a difference in AIC of 3 or more is analagous to the models being significantly different at an \\(\\alpha\\) of \\(p &lt; 0.05\\). AIC_simple - AIC_full ## [1] 266957.8 Q5: Is there a preferred model and if so, which one? Answer 5: The nice thing about AIC is that the models you compare do not have to be nested like they do for ANOVA, as long as the data are the same. There are a few other constraints however… Here are the AIC scores for our pair of NLS models: AIC(fit_negexp, fit_negexpS) ## df AIC ## fit_negexp 4 -325.7135 ## fit_negexpS 6 -385.6718 You’ll notice that these are completely different to the AICs for the MLE models… Q6: Why is it not okay to compare the AIC of these NLS models with the AIC of the MLE models? Hint: type ?AIC into the R console and do some reading. Answer 6: "],["data.html", "8 Data Management 8.1 Why do you need to manage your data? 8.2 The Data Life Cycle 8.3 Data and decisions", " 8 Data Management 8.1 Why do you need to manage your data? Data management is often the last thing on a scientists mind when doing a new study - “I have a cool idea, and I’m going to test it!”. You don’t want to “waste” time planning how you’re going to manage your data and implementing that plan… Unfortunately, this never ends well and really is a realm where “haste makes waste”. Figure 8.1: The ‘Data Decay Curve’ (Michener et al. 1997) Here are a bunch of reasons you really want to focus on doing good data management: Bad data management leads to data loss… (Figure 8.1) Your future self will hate you if you lose it before you’re finished with it!!! This is less likely in the world of Dropbox, Google Drive, iCloud etc, but I know people who had to repeat their PhD’s because they lost their data because it was on a laptop that crashed or was stolen… Also, beware cloud storage!!! It’s very easy for you or a collaborator to delete/overwrite/lose access to items, e.g.  if someone leaves the project and deletes the folder on their Dropbox without “leaving” it first if the “owner” of the Google Drive folder loses access to their Google account (as will happen to your UCT Google Drive access as soon as you graduate!!!) through all manner of random “accidents” Data has value beyond your current project: to yourself for reuse in future projects, collaborations, etc (i.e. publications and citations), for others for follow-up studies, or combining multiple datasets for meta-analyses or synthesis etc for science on general (especially long-term ecology in a time of global change) We’ve covered this before, but sharing your data is key for transparency and accountability. Data collection is expensive, and is often paid for with taxpayers’ money. You owe it to your funder (and humanity in general) to make sure that science gets the most out of your data in the long term. Lastly, good planning and data management can help iron out issues up front, like: intellectual property, permissions for ethics, collection permits, etc, outlining expectations for who will be authors on the paper(s), responsibilities for managing different aspects of the data etc If you don’t establish these permissions and ground rules early they can result in data loss, not being able to publish the study, damage relationships in collaborations (including student-supervisor), and ultimately damage careers… To avoid data (and relationship) decay, and to reap the benefits of good data management, it is important to consider the full Data Life Cycle. 8.2 The Data Life Cycle Figure 8.2: The Data Life Cycle, adapted from https://www.dataone.org/ Note that there are quite a few different versions of the data life cycle out there. This is the most comprehensive one I know of, and covers all the steps relevant to a range of different kinds of research projects. A full description of this data life cycle and related ecoinformatics issues can be found in (Michener and Jones 2012). Not all projects need to do all steps, nor will they necessarily follow the order here, but it is worth being aware of and considering all steps. For example: Often the first thing you do when you have a new hypothesis is start by searching for any existing data that could be used to test it without having to spend money and time collecting new data (i.e. skip to step 6 - “Discover”). In this case I would argue that you should still do step 1 (Plan), and you’d want to do some checking to assure the quality of the data (step 3), but you can certainly skip steps 2, 4 and 5. A meta-analysis or synthesis paper would probably do the same. If you’re collecting new data you would do steps 1 to 5 and possibly skip 6 and 7, although in my experience few studies do not reuse existing data (e.g. weather or various GIS data to put your new samples into context). 8.2.1 Plan Good data management begins with planning. In this step you essentially outline the plan for every step of the cycle in as much detail as possible. Usually this is done by constructing a document or Data Management Plan (DMP). While developing DMPs can seem tedious, they are essential for the reasons I gave above, and because most funders and universities now require them. Fortunately, there are a number of online data management planning tools that make it easy by providing templates and prompts to ensure that you cover all the bases, like the Digital Curation Centre’s DMPOnline and UCT’s DMP Tool. Figure 8.3: Screenshot of UCT’s Data Management Planning Tool’s Data Management Checklist. A key thing to bear in mind is that a DMP is a living document and should be regularly revised during the life of a project, especially when big changes happen - e.g. new team members, new funding, new direction, change of institution, etc. I typically develop one overarching DMP for an umbrella project (e.g. a particular grant), but then add specifics for subprojects (e.g. separate student projects etc). 8.2.2 Collect and Assure There are many, many different kinds of data that can be collected in a vast number of ways! Figure 8.4: Springer Nature Infographic illustrating the vast range of research data types. While “Collect” and “Assure” are different steps in the life cycle, I advocate that it is foolish to collect data without doing quality assurance and quality control (QA/QC) as you go, irrespective of how you are collecting the data. For example: automated logging instruments (weather stations, cameras, acoustic recorders) need to be checked that they’re logging properly, are calibrated/focused, are reporting sensible values, etc if you’re filling in data sheets, you need to check that all fields have been completed (no gaps), that there are no obvious errors and that any numbers or other values look realistic. In fact, if you’re using handwritten data sheets it’s best to capture them as soon as possible (i.e. that evening), because that helps you spot errors and omissions, you have a better chance of deciphering bad handwriting or cryptic notes, and you can plot any values to see if there are suspicious outliers (e.g. because someone wrote down a measurement in centimetres when they were meant to use metres). When transcribing or capturing data into a spreadsheet or database it is often best to use data validation tricks like drop-down menus, conditional formatting, restricted value ranges etc to avoid spelling mistakes and highlight data entries that are outside the expected range of the data field. It may seem like a lot of effort to set this up, but it’ll save you a lot of time and pain in the long run!!! Increasingly, I’ve started moving towards capturing data directly into a spreadsheet with data validation rules using a phone or tablet. There are also a number of “no code” app builders these days like AppSheet that allows you to easily collect data on your phone and sync data directly into Google Sheets and photos to your Google Drive. AppSheet is proprietary software, but does allow a lot of utility with their free accounts. QField is another handy open source app built on QGIS, allowing you to setup maps and forms in QGIS on your workstation and deploy them to your phone, tablet etc. It seems to be getting better all the time. Figure 8.5: An example data collection app I built in AppSheet that allows you to log GPS coordinates, take photos, record various fields, etc. Tidy Data Last, but not least. We haven’t discussed different data formats etc, but if you are working with tabular data (i.e. spreadsheets, tables) I strongly recommend you read this short paper on how to keep your data Tidy (Wickham 2014). Following these principles will make life much easier for you once you get to the analysis step… 8.2.3 Describe There are few things worse than having a spreadsheet of what you think is the data you need, but you don’t know what the column names mean, how variables were measured, what units they’re reported in, etc… - Especially when you were the one who collected and captured the data!!! This descriptive data about the data is called metadata and is essential for making the data reusable, but is also useful for many other purposes like making the data findable (e.g. using keyword searches). In fact, metadata makes up the majority of what are called the FAIR data principles (Wilkinson et al. 2016), which largely focus on this and the next few steps of the Data Life Cycle. I’m not going to dwell on them other than to say that they are a key component of making your work reproducible, and that like reproducibility, practicing FAIR data principles is a spectrum. Figure 8.6: The FAIR data principles ErrantScience.com. Some key kinds of metadata: the study context why the data were generated who funded, created, collected, assured, managed and owns the data (not always the same person) contact details for the above when and where the data were collected where the data are stored the data format what is the file format what softwares were used (and what version) the data content what was measured how it was measured what the columns and rows are what units it’s reported in what QA/QC has been applied is it raw data or a derived data product (e.g. spatially interpolated climate layers) if derived, how it was analyzed etc Metadata standards and interoperability Many data user communities have developed particular metadata standards or schemas in an attempt to enable the best possible description and interoperability of a data type for their needs. They are typically human and machine-readable data, so that the metadata records can also be read by machines, to facilitate storing and querying multiple datasets in a common database (or across databases). Imagine how difficult it would be to pay for something electronically if banks didn’t use common metadata standards? Each transaction would require someone to manually look things up in multiple tables etc etc. Chaos!!! Figure 8.7: How standards proliferate… from xkcd.com/927, used under a CC-BY-NC 2.5 license. Using common metadata schemas has many advantages in that they make data sharing easier, they allow you to search and integrate data across datasets, and they simplify metadata capture (i.e. having a list of required fields makes it easier to not forget any). There are many standards, but perhaps the most common ones you’ll encounter in biological sciences (other than geospatial metadata standards) are DarwinCore and Ecological Metadata Language (EML). There’s even new standards for documenting ecological forecasts! Figure 8.8: An example of a geospatial metadata standard. SpatioTemporal Asset Catalogs (STAC; stacspec.org) aims to provide a common specification to enable online search and discovery of geospatial assets in just about any format. 8.2.4 Preserve There are two major components to preserving your data: Back your data up now!!! (and repeat regularly) Losing your data can be incredibly inconvenient!!! A good friend of mine lost all of his PhD data twice. It took him 7 years to complete the degree… Beyond inconvenience, losing data can be incredibly expensive! Doing 4 extra years to get your PhD is expensive at a personal level, but if the data are part of a big project it can rapidly add up to millions - like How Toy Story 2 Almost Got Deleted. PRO TIP: Storing data on the cloud is not enough! You could easily delete that single version of all your data! You may also lose access when you change institution etc. E.g. What happens to your UCT MS OneDrive and Google Drive content when you graduate and ICTS close your email account? Long-term preservation and publication This involves the deposition of your data (and metadata!) in a data repository where it can be managed and curated over the long term. This is increasingly a requirement of funders and publishers (i.e. journals). Many journals allow you (or require you) to submit and publish your data with them as supplementary material. Unfortunately, many journals differ in how they curate the data and whether they are available open access. I prefer to publish my data in an online open access repository where you can get a permanent Digital Object Identifier (DOI) that you can link to from your paper. Another consideration, if you are keen for people to reuse your data (which if you are not you will fail this course by default) is where people are most likely to look for your data (i.e. making your data “Findable/Discoverable”). There are many “bespoke” discipline-specific data repositories for different kinds of data, e.g. Global databases: GenBank - for molecular data TRY - for plant traits Dryad - for generalist biological and environmental research South African databases: SANBI - for most kinds of South African biodiversity data SAEON - for South African environmental data (e.g. hydrology, meteorology, etc) and biodiversity data that don’t fit SANBI’s databases If none of these suit your data, there are also “generalist” data repositories that accept almost any kind of data, like: FigShare Zenodo UCT’s ZivaHub (which is built on and searchable through FigShare) I haven’t discussed physical samples at all. These are obviously a huge (if not bigger) challenge too, although there are some obvious homes for common biological data, like herbaria for plant collections and museums for animal specimens. 8.2.5 Discover This is perhaps the main point of the Data Life Cycle and FAIR data principles - to make data findable so that it can be reused. The biggest challenge to discovering data is that so many datasets are not online and are in the “filing cabinet in a bath in the basement under a leaking pipe” as in Figure 8.6. If you preserve and publish them in an online data repository, this overcomes the biggest hurdle. The next biggest challenge is that there is so much online that finding what you need can be quite challenging (like looking for a needle in a haystack…). This is where choosing the right portal can be important. It is also what metadata standards are aimed at - allowing interoperable searches for specific data types across multiple repositories. A final consideration is whether you have permission to use the data. You can often find out about the existence of a dataset, either online or in a paper, but the data aren’t made freely available. This is where licensing comes into play. Most data repositories require you to publish the data under a license. There are many options depending on the kind of data and what restrictions you want to put on its use. I’m not going to go into the gory details, but Creative Commons have created an extensible system of generic licenses that are easy to interpret and cover most situations. I say extensible because the licenses are made up of a string of components that can be layered over each other. For example: CCO - means it is Open - i.e. there are no restrictions on use and it is in the public domain CC BY - means by attribution - you can use the data for any purpose, but only if you indicate attribution of the data to the source or owner of the data CC BY-SA - means by attribution + share alike - i.e. you have to indicate attribution and share your derived product under the same license CC BY-ND - means by attribution + no derivatives - i.e. you have to indicate attribution, but cannot use it to make a derived product. This is often used for images - allowing you to show the image, but not to alter it. CC BY-NC - means by attribution + non-commercial - you have to indicate attribution, but cannot use it for commercial purposes (i.e. you can’t sell derived products) CC BY-NC-SA - by attribution + non-commercial + share alike CC BY-NC-ND - by attribution + non-commercial + no derivatives NOTE: As an aside, for some reason software licenses are a bit more complicated and code is rarely shared under CC licenses, other than CCO. A rough equivalent of CC BY for code is MIT, and there are others that can add various constraints similar to Creative Commons’ SA, ND and NC. See here. 8.2.6 Integrate There are a few different components to data integration in this context: Linking different kinds of data, usually through spatial and or temporal information e.g. matching you biodiversity collections with weather records or GIS information about the sites Keeping track of changes you’ve made to your data as you prepare it for analyses (versioning) e.g. you may be wanting to compare species richness across sites. This requires estimating species richness from your field data (usually lists of species by site occurrences and/or abundances) you should always keep a copy of your raw data!!! using scripting languages for data handling and analyses (e.g. R, Python, MatLab) can help you keep record of how you did any data summaries, transformations, etc, but only if you write clean, well-documented code and manage your code well!!! Curating your data such that they can easily be integrated with other, similar datasets for larger analyses or meta-analyses this is largely a metadata game, but also one of data formats etc. Many fields promote the use of common data standards with rules on measurement specifications, file formats, common data and metadata fields, controlled vocabularies etc that allow easy integration, searching and manipulation (see section 8.2.3 for more details). This is what a lot of the discipline-specific online databases attempt to achieve. 8.2.7 Analyze “The fun bit”, but again, there are many things to bear in mind and keep track of so that your analysis is repeatable. This is largely covered by the sections on Coding and code management and Computing environment and software in Chapter 5 8.3 Data and decisions This is not part of The Data Life Cycle per se, but it’s worth remembering that there are some other aspects of data, while still important if the goal is purely academic, that are make-or-break when the goal is informing decisions, e.g. (mostly paraphrased from Dietze et al. (2018)): Latency - the time between data collection and it’s availability for modelling. Depending on the model’s need for the latest data, if the latency is too long then it can preclude the ability to make useful forecasts (e.g. it’s very difficult to make useful weekly forecasts if you only get new data once a year…). Uncertainty - the model can only ever be as good as the data (GIGO: garbage in = garbage out). We’ll also see that assimilating data into forecasts requires uncertainty estimates. Not including uncertainty can create bias or overconfident predictions… Unfortunately, very rarely does anyone report the uncertainty in their data… Unfortunately, there are also many reasons why data can never be certain - sampling variability, random and systematic instrument errors, calibration uncertainty, classification errors (e.g. species identification), transcription errors, corrupt files (or collaborators!) etc. I live by the creed there is nothing as sinister as a certainty. If you tell me your model has an \\(R^2\\) of 1, I will tell you (with 100% certainty, ironically) that your model is wrong. Repeated sampling - most forecasts are in time and thus require time-series to develop the models. Frequent repeated sampling can often come at a trade-off with the spatial extent (or spatial replication) of sampling though. Forecasters need to optimize this trade-off to inform the most efficient data collection while also reducing uncertainty in the model. Interoperability - this is largely covered under the Describe, Preserve, Discover and Integrate steps in The Data Life Cycle References "],["bayesian.html", "9 Going Bayesian 9.1 Least Squares 9.2 Maximum likelihood 9.3 Bayes’ Theorem", " 9 Going Bayesian In this section I aim to provide a brief and (relatively) soft introduction to Bayesian statistical theory. While ecological forecasting and decision support can be done with traditional statistics (often termed “frequentist statistics”) it is generally much easier to do in a Bayesian statistical framework. Bayesian approaches have several advantages for forecasting: NOTE: there are frequentist approaches for doing much of this, but they are typically cumbersome “add-ons” that require many additional assumptions. Once you’re using Bayes you can achieve all this without much extra work. They are typically focused on estimating what properties are (i.e. the actual value of a particular parameter) and not just establishing what they are not (i.e. testing for significant difference (null hypothesis testing), as is usually the focus in frequentist statistics) They are highly flexible, allowing one to build relatively complex models with varied data sources (and/or of varying quality), especially Hierarchical Bayesian models They can easily treat all terms as probability distributions, making it easier to quantify, propagate, partition and represent uncertainties throughout the analysis probabilistically. More on this later, but it addresses two of the key components of the ecological forecasting cycle: being able to present the uncertainty in the forecast to the decision maker being able to analyze the uncertainty in the forecast, in the hope that this can guide reducing the uncertainty and improving the next forecast, e.g. through targeted data collection or altering the model. They provide an iterative probabilistic framework that mirrors the scientific method, allowing us to formalize learning from new evidence in the context of existing knowledge. In the context of forecasting, this framework makes it easier to update predictions as new data become available, completing the forecasting cycle. Before I can introduce Bayes, there are a few basic building blocks we need to establish first. How the method of Least Squares works, and its limitations, especially it’s inflexible, implicit data model. The concept of likelihood and the estimation of maximum likelihood, since this is a major component of Bayes’ Theorem. In fact, while likelihood is technically still a frequentist method, it has the advantages 1 &amp; most of 2 above, without going full Bayes. NOTE: This really is a minimalist introduction that only provides the tidbits I need you to know to follow the rest of the module. This note is a proviso to make it clear that I am withholding important details, before anyone accuses me of lies of ommission. 9.1 Least Squares Traditional parametric statistics like regression analysis and analysis of variance (ANOVA) rely on Ordinary Least Squares (OLS). There are a few other flavours of least squares that allow a bit more flexibility (e.g. nonlinear (NLS) that we used in the practical in section 6, partial least squares, etc), but I’m not going to go into these distinctions. In general, least squares approaches “fit” (i.e. estimate the parameters of) models by minimizing the sums of the squared residuals. Let’s explore this by looking at an example of a linear regression. Figure 9.1: A hypothetical example showing a linear model fit. Here the model (blue line) is drawn through the cloud of points so as to minimize the sum of the squared vertical (y-axis) differences (i.e. residuals) between each point and the regression line. Let’s redraw this highlighting the residuals: Figure 9.2: The linear model highlighting the residuals (lollypops) relative to the values predicted by the model (open circles along the regression line). So the grey lines linking each observed data point to the regression line are the residuals. Note that they are vertical and not orthogonal to the regression line, because they represent the variance in Y (Reward in this case) that is not explained by X (Effort in this case). The open black circles are the Y-values that our linear model predicts for a given X-value. There is no scatter in the predicted values, because the scatter is residual variance that the model cannot account for and predict. Now let’s have a look at a histogram of the residuals: Figure 9.3: A histogram of the residuals from the linear model above. In this case, the residuals approximate a normal distribution (or should, since I generated them from a normal distribution…). You’ll recall that this is one of the assumptions when using linear models or ANOVA (often termed “homoscedasticity” or “homogeneity of variance”) - i.e. it is an assumption of the Least Squares method. Least squares cannot handle residuals that are not normally distributed. If the residuals were not normally distributed, then either we are fitting the wrong model (e.g. we should consider a non-linear rather than a linear model), or our assumptions are violated and we should not be using this technique! The reason Least Squares requires this assumption is that for minimizing the sums of squares to work, a unit change in the residuals should be scale invariant. In other words, the difference between 1 and 2 needs to be the same as the difference between 101 and 102. This is only the case when the residuals are normally distributed (versus log-scale for example). If the scale is variable, then minimizing the sums of squares does not work. People often try to get around the assumption of homogeneity of variance by tranforming their data (e.g. log or arcsine transform) to try to get them to an invariant scale. So to look at the shortcomings of Least Squares: 9.1.1 Least Squares analyses don’t explicitly include a data model It’s useful at this stage to make a distinction between data models and process models. The process model is the bit you’ll be used to, where we describe how the model creates a prediction for a particular set of inputs or covariates (i.e. the linear model in the case above). The data model describes the residuals (i.e. the mismatch between the process model and the data). It is also often called the data observation process. Least Squares analyses don’t explicitly include a data model, because the reliance on minimizing the sums of squares means that the data model in a Least Squares analysis can only ever be a normal distribution (i.e. homogeneity of variance). This is a major limitation of the Least Squares method, because: in reality, the data model can take many forms e.g. Binomial coin flips, Poisson counts of individuals, Exponential waiting times, etc this is where Maximum Likelihood comes into it’s own recall that in the practical we specified two separate functions for the MLE analyses. One specified the process model (pred.negexp/S), the other specified the likelihood function (fit.negexp/S.MLE), which includes the data model. there are times when one would like to include additional information in the data model e.g. sampling variability (e.g. different observers or instruments), measurement errors, instrument calibration, proxy data, unequal variances, missing data, etc this is where Bayesian models come into their own 9.1.2 Least squares focuses on what the parameters are not, rather than what they are Least Squares focuses on significance testing - the ability to reject (or to fail to reject) the null hypothesis. In the case of a linear model, the null hypothesis is that the slope is zero (i.e. there is no effect of X on Y) and sometimes includes that the intercept should be zero too (although this is not usually required). I don’t have the time to go through the full explanation of how the null hypothesis is tested in this lecture, but it is useful to highlight that the linear model is only considered useful when you can reject the null hypothesis that the slope is zero (usually at an alpha of P &lt; 0.05). While this is a start, it doesn’t tell you anything about the probability of the parameters actually being the estimates you arrives at by minimizing the sums of squares?! 9.2 Maximum likelihood Maximum likelihood is a method used for estimating model parameters The likelihood principle states a that parameter value is more likely than another if it is the one for which the data are more probable. Figure 9.4: The probability of a observed point (the dotted line) being generated under two alternative hypotheses (sets of parameter values). In this case H1 is more likely, because the probability density at \\(x\\) = 1 is higher for H1 than H2 (roughly 0.25 vs 0.025). This makes H1 0.25/0.025 = 10 times more likely. Maximum Likelihood Estimation (MLE) applies this principle by optimizing the parameter values such that they maximize the likelihood that the process described by the given model produced the data that were observed. To relate this to Figure 9.4, the parameter values would be a continuum of hypotheses to select from, and of course there would be far more data points that you’d need to calculate the probabilities for. Viewed differently, when using MLE we assume that we have the correct model and apply MLE to choose the parameters so as to maximize the conditional probability of the data given those parameter estimates. The notation for this conditional probability is \\(P(Data|Parameters)\\). This process leaves us knowing the likelihood of the best set of parameters and the conditional probability of the data given those parameters \\(P(Data|Parameters)\\). The problem is that in the context of forecasting (and many other modelling approaches) what we really want is to know is the conditional probability of the parameters given the data \\(P(Parameters|Data)\\), because this allow us to express the uncertainty in the parameter estimates as probabilities3. To get there, we need to apply a little probability theory, which provides a somewhat surprising and very useful byproduct. First, let’s brush up on our probability theory… 9.2.1 Joint, conditional and marginal probabilities Here we’ll look at the interactions between two random variables and illustrate it with the Venn diagram below. Figure 9.5: Venn diagram illustrating ten events (points) across two sets \\(x\\) and \\(y\\). First, we can define the joint probability, \\(P(x,y)\\), which is the probability of both \\(x\\) and \\(y\\) occurring simultaneously, which in the case of Figure 9.5 is the probability of occurring within the overlap of the circles = 3/10. Needless to say the joint probability of \\(P(y,x)\\) is identical to \\(P(x,y)\\) (= 3/10). Second, based on the joint probability we can define two conditional probabilities: the probability of \\(x\\) given \\(y\\), \\(P(x|y)\\), and the probability of \\(y\\) given \\(x\\), \\(P(y|x)\\). In the context of the Venn diagram these would be: the probability of being in set \\(x\\) given that we are only considering the points in set \\(y\\) (= 3/6), and the probability of being in set \\(y\\) given that we are only considering the points in set \\(x\\) (= 3/7). Last, we can define two marginal probabilities for \\(x\\) and \\(y\\), which are just the separate probabilities of being in set \\(x\\) or being in set \\(y\\) given the full set of events, i.e.: \\(P(x)\\) = 7/10 and \\(P(y)\\) = 6/10. We can also show that the joint probabilities are the product of the conditional and marginal probabilities: \\[\\begin{align*} P(x,y) = P(x|y) P(y) = 3/6 * 6/10 = 0.3 \\end{align*}\\] And by the same token: \\[\\begin{align*} P(y,x) = P(y|x) P(y) = 3/7 * 7/10 = 0.3 \\end{align*}\\] Why I’m telling you all this is because knowing this last equation (that the joint probabilities are the product of the conditional and marginal probabilities) means that we can derive the information we want to know, \\(P(Parameters|Data)\\), as a function of the information maximum likelihood estimation has given us, \\(P(Data|Parameters)\\)… 9.3 Bayes’ Theorem From here I’ll refer to the parameters as \\(\\theta\\) and the data as (\\(D\\)), because writing them out in full is a bit clunky and grownups don’t usually do that. Since we are interested in the conditional probability of the parameters given the data, \\(p(\\theta|D)\\), we need to take the equations above and solve for \\(p(\\theta|D)\\). From above, we know \\[\\begin{align*} p(\\theta,D) = p(\\theta|D)p(D) \\end{align*}\\] and we know that the joint probabilities are identical, so we can also write \\[\\begin{align*} p(\\theta,D) = p(D|\\theta)p(\\theta) \\end{align*}\\] so we can rewrite this to compare the right hand side of the last two equations \\[\\begin{align*} p(\\theta|D)p(D) = p(D|\\theta)p(\\theta) \\end{align*}\\] which if we solve for \\(p(\\theta|D)\\) is \\[\\begin{align*} p(\\theta|D) &amp; = \\frac{p(D|\\theta) \\; p(\\theta)}{p(D)} \\;\\; \\end{align*}\\] which is known as Bayes’ Theorem. 9.3.1 The beauty of Bayes’ Theorem Now let’s unwrap our birthday present and see what we got! Rewriting the terms on one line allows us to label them with the names by which they are commonly known: \\[ \\underbrace{p(\\theta|D)}_\\text{posterior} \\; = \\; \\underbrace{p(D|\\theta)}_\\text{likelihood} \\;\\; \\underbrace{p(\\theta)}_\\text{prior} \\; / \\; \\underbrace{p(D)}_\\text{evidence} \\] First, let’s start with the last term, the evidence as this will allow us to simplify things. The marginal probability of the data, \\(p(D)\\), is now called the evidence for the model, and represents the overall probability of the data according to the model, determined by averaging across all possible parameter values weighted by the strength of belief in those parameter values. This sounds quite complicated, but never fear, all you need to know is that the implication of having \\(p(D)\\) in the denominator is that it normalizes the numerators to ensure that the updated probabilities sum to 1 over all possible parameter values. In other words, the evidence term ensures that the posterior, \\(p(\\theta|D)\\), is expressed as a probability distribution, and can otherwise mostly be ignored. This allows us to focus on the important bits, and to express Bayes’ Rule as \\[ \\underbrace{p(\\theta|D)}_\\text{posterior} \\; \\propto \\; \\underbrace{p(D|\\theta)}_\\text{likelihood} \\;\\; \\underbrace{p(\\theta)}_\\text{prior} \\; \\] Which reads “The posterior is proportional to the likelihood times the prior”. This leaves us with three terms. First, we have the likelihood, \\(p(D|\\theta)\\). This is unchanged and still represents the probability that the data could be generated by the model with parameter values \\(\\theta\\), and when used in analyses still works to find the likelihood profiles of the parameters. Next, what we have been calling the conditional probability of the parameters given the data, \\(p(\\theta|D)\\), is now called the posterior, and represents the credibility of the parameter values \\(\\theta\\), taking the data, \\(D\\), into account. In other words, it gives us a probability distribution for the values any parameter can take, essentially allowing us to represent uncertainty in the model and forecasts as probabilities, which is the holy grail we were after!!! Last, we’ll look at the marginal probability of the parameters, \\(p(\\theta)\\), which is now called the prior. While \\(p(\\theta)\\) is almost an unexpected consequence of having solved the equation for \\(p(\\theta|D)\\), this is where the magic lies! Firstly, we know that \\(p(\\theta|D)\\) is a necessary requirement for us to be able to represent the posterior, \\(p(\\theta|D)\\), as a probability distribution. That it helps us do this is magic in itself, but what is \\(p(\\theta)\\) itself? The prior represents the credibility of the parameter values, \\(\\theta\\), without the data, \\(D\\). How can we know anything about the parameter values without the data you may ask? In short, the answer is because we are applying the scientific method, whereby we interrogate new evidence (the data) in the context of previous knowledge or information to update our understanding. The term \\(p(\\theta)\\) is called the prior, because it represents our prior belief of what the parameters should be, before interrogating the data. In other words, the prior is our “context of previous knowledge or information”. The nice thing is that if we don’t have much previous knowledge or information, we can specify the prior to represent that. The prior is incredibly powerful, but, as always the Peter Parker Principle applies - “With great power comes great responsibility!” The prior is incredibly powerful, because: It allows Bayesian analyses to be iterative. The posterior from one analysis can become the prior for the next! This provides a formal probabilistic framework for the scientific method! New evidence must be considered in the context of previous knowledge or information, and provides us the opportunity to update our beliefs. This is also ideal for iterative forecasting, because the previous forecast can be the prior for the next, (and the observed outcome of the previous forecast window can be the new data that’s fed into the likelihood function for the next forecast). It makes Bayesian modelling incredibly flexible by allowing us to specify complex models like hierarchical models in a completely probabilistic framework relatively intuitively. For example, in some cases we may not have any strong direct “prior beliefs” about the parameter values, but we may know something about another parameter or process that influences the parameter value of interest. In this case we can specify a prior on our prior (usually called a hyperprior). Another key component here is that we can specify separate or linked priors or hyperpriors for different parameters or priors. The prior also comes with great responsibility, because: In short, it is very easy to specify an inappropriate prior and bias the outcome of your analysis! First and foremost, as tempting as it may be, the one place the prior CANNOT come from is the data used in the analysis!!! Second, while many people favour specifying “uninformative” priors, it is often incredibly difficult to know what “uninformative” is for different models or parameters. For example, say you were trying to build a model to solve a murder mystery and predict who the likely culprit is. One of the parameters may be the gender of the culprit. If you’re lazy, you may say “we set an uninformative prior that there was a 50/50 chance of the culprit being male or female”. In truth, this “uninformative” prior may well bias your result for a number of reasons: Firstly, sex ratios are rarely 50/50 (e.g. in South Africa a quick Google search suggests the M/F ratio is supposedly 1:1.03). In this case your prior would be unfairly biasing the model towards predicting that that the culprit is a woman. Acknowledging this, you may then set your sex ratio as the observed ratio for the population in the region where the murder took place, but there are further complexities that may bias your results. e.g. Observed data suggest that far more men commit murder than women, but going with the observed sex ratio for the population makes the assumption that men and women are equally likely to commit murder. Next, your prior has not taken into account the profile of the victim - man, woman, child, etc - but the sex ratios of murderers varies hugely wither you’re considering androcide, femicide or infanticide. Lastly, what about gender identity and sexual orientation (LGBTQI+)? Have your sources of prior information taken this into account and would this affect the outcome? Here’s a great Guardian article on why considering priors are important for Covid testing and criminal cases. It should help you get your head around this. We’ll get stuck into examples showing the value of Bayes in ecological in the next lecture. Note that we have the likelihood of the parameter estimates, but \\(likelihood \\neq probability\\). I don’t want to spend time on the distinction in the lecture, but it is a very important concept. The area under a probability distribution sums to 1, but the area under a likelihood profile curve does not. The reason for this is that probability relate to the set of possible results, which are exhaustive and mutually exclusive, whereas likelihood relates to the possible hypotheses, which are neither exhaustive nor mutually exclusive (i.e. there’s no limit to the number of potential hypotheses, and hypotheses can be nested or overlapping).↩︎ "],["datafusion.html", "10 Data Fusion 10.1 A quick reminder of Bayes Theorem and the benefits it provides 10.2 Data fusion", " 10 Data Fusion This lecture builds on section 9, “Going Bayesian”, focusing on how the opportunities for imputing (or “fusing”) data with models increase as one moves from the method of Least Squares to Hierarchical Bayesian models. First a little revision of our last lecture… 10.1 A quick reminder of Bayes Theorem and the benefits it provides \\[ \\underbrace{p(\\theta|D)}_\\text{posterior} \\; \\propto \\; \\underbrace{p(D|\\theta)}_\\text{likelihood} \\;\\; \\underbrace{p(\\theta)}_\\text{prior} \\; \\] Where the posterior is a probability distribution representing the credibility of the parameter values \\(\\theta\\), taking the data, \\(D\\), into account. The likelihood, \\(p(D|\\theta)\\) represents the probability that the data could be generated by the model with parameter values \\(\\theta\\). This term looks for the best estimate of the parameters using Maximum Likelihood Estimation, where the likelihood of the parameters are maximized by choosing the parameters that maximize the probability of the data. The prior probability distribution is the marginal probability of the parameters, \\(p(\\theta)\\), and represents the credibility of the parameter values, \\(\\theta\\), without the data, \\(D\\). Since it must not be inferred from the data, it is specified using our prior belief of what the parameters should be, before interrogating the data. Prior information can be sourced from previous analyses (of other data!), literature, meta-analyses, expert opinion or ecological theory. We need to be very careful about how we specify our priors so as not to bias the analysis! 10.1.1 The benefits of Bayes They are focused on estimating what properties are (i.e. the actual values of particular parameters) and not null hypothesis testing. They are highly flexible, allowing one to build relatively complex models with varied data sources (and/or of varying quality), especially Hierarchical Bayesian models. This is important for Data Fusion that we’ll discuss today. They can easily treat all terms as probability distributions, making it easier to quantify, propagate, partition and represent uncertainties throughout the analysis probabilistically. Which is the focus of section 11 (our next lecture). They provide an iterative probabilistic framework that makes it easier to update predictions as new data become available, mirroring the scientific method and completing the forecasting cycle. This closing of the forecast loop is termed Data Assimilation and we’ll discuss it in the final lecture in section 12. 10.2 Data fusion Data can enter (or be fused with) a model in a variety of ways. Here we’ll discuss these and then give an example by revisiting the Fynbos postfire recovery model we played with in the practical. The opportunities for data fusion are linked to model structure, so we’ll revisit how some aspects of model structure change as we move from Least Squares to Maximum Likelihood Estimation to “single-level” Bayes to Hierarchical Bayes and the data fusion opportunities provided by each. 10.2.1 Model layers Least Squares The method of Least Squares makes no explicit distinction between the process model (that models the drivers determining the pattern observed) and the data model (that models the observation error or data observation process, i.e. the factors that may cause mismatch between the process model and the data). This is because we require homogeneity of variance in order to minimize the sums of squares, so the data model can only ever be a normal distribution. Looking back at the practical, what this means is that for the Nonlinear Least Squares all we fitted was the process model, which for our simpler negative exponential model not including the seasonality term: \\[\\begin{gather} \\text{NDVI}_{i,t}=\\alpha_i+\\gamma_i\\Big(1-e^{-\\frac{age_{i,t}}{\\lambda_i}}\\Big) \\end{gather}\\] The process model was the only opportunity for inputting data, and here we input time series for NDVI and date (or postfire age since our time series started at the time of a fire). The only way to add more data sources is to make your model more complex (e.g. we added the month of the fire when we added the seasonality term to our model). Maximum Likelihood MLE does make a distinction between the data model and the process model, like so: Figure 10.1: Maximum Likelihood makes a distinction between the process and data models. I didn’t make the distinction in the equations presented in the practical, but we did in the functions, where we included the data model: \\[\\begin{gather} NDVI_{i,t}\\sim\\mathcal{N}(\\mu_{i,t},\\frac{1}{\\sqrt{\\tau}}) \\\\ \\end{gather}\\] which is the likelihood function. In this case we specified a Gaussian (normal) distribution around the mean, \\(\\mu\\), which is described by the process model. We then had a separate function for the process model: \\[\\begin{gather} \\mu_{i,t}=\\alpha_i+\\gamma_i\\Big(1-e^{-\\frac{age_{i,t}}{\\lambda_i}}\\Big)\\\\ \\end{gather}\\] Where \\(\\mu\\) is now the mean of the process model, not accounting for the residual error described by the data model - similar to the open circles in Figure 9.2. What splitting the data and process models means, is that we are now feeding out dependent variable (NDVI) to the data model, and our independent variable (time or postfire age) to the process model. The beauty of having a separate data model is that you are no longer restricted to the normal distribution and have flexibility to specify probability distributions that suit the data observation process, e.g. Binomial coin flips, Poisson counts of individuals, Exponential waiting times, etc. You can even specify custom data models with their own covariates, which is useful if you have information on things like instrument drift and calibration, etc - i.e. another opportunity for data fusion. Side note: You’d be interested to know that specifying a normal distribution in the likelihood (i.e. the data model) means that our MLE analysis is identical to the NLS analysis. In fact, you can do the maths to show that MLE with a normal likelihood is exactly the same as Least Squares. Any differences between the parameter estimates the two methods provided in the practical were purely because we had to solve the equations numerically (i.e. estimate) rather than analytically (i.e. diret calculation). Where MLE gets useful is when you start doing interesting things with the data model. Single-level Bayes When we use Bayesian models we now have the priors, which are essentially models describing our prior expectation for each of the parameters in the process model, like so: Figure 10.2: Bayesian models include an additional layer; parameter models that describe our prior expectation for the parameters in the process. When implementing Bayesian models, we just specify the priors as parameter models, e.g.: \\[\\begin{gather} \\alpha_i\\sim\\mathcal{N}(\\mu_{\\alpha},\\frac{1}{\\sqrt{\\tau_{\\alpha}}})\\\\ \\gamma_i\\sim\\mathcal{N}(\\mu_{\\gamma},\\frac{1}{\\sqrt{\\tau_{\\gamma}}})\\\\ \\lambda_i\\sim\\mathcal{N}(\\mu_{\\lambda},\\frac{1}{\\sqrt{\\tau_{\\lambda}}})\\\\ \\end{gather}\\] Where (in this case) we’re saying we believe the three parameters from our simpler postfire recovery model are all sampled from normal distributions with independent means and variances. Note that you need priors (i.e. parameter models) for all parameters. They don’t all have to be independent though, which can be useful, for example if you have multiple separate sets drawn from the same population, etc. While you can’t specify new data in the priors in single-level Bayes (because that is then a Hierarchical model, coming next) it does still provide new opportunities for data fusion, because the conditional nature of Bayes Theorem allows you to chain multiple likelihoods (i.e. data models) together. I’m not going to go into the details of this, but one important consideration is that there need to be links between all terms - e.g. two datasets (\\(D_1\\) and \\(D_2\\)) that share the same parameters (\\(\\theta\\)): \\[ \\underbrace{p(D_1|\\theta)}_\\text{likelihood 1} \\;\\; \\underbrace{p(D_2|\\theta)}_\\text{likelihood 2} \\;\\;\\underbrace{p(\\theta)}_\\text{prior} \\; \\] or one dataset (\\(D_1\\)) is conditional on another (\\(D_2\\)), that is conditional on the parameters (\\(\\theta\\)): \\[ \\underbrace{p(D_1|D_2)}_\\text{likelihood 1} \\;\\; \\underbrace{p(D_2|\\theta)}_\\text{likelihood 2} \\;\\;\\underbrace{p(\\theta)}_\\text{prior} \\; \\] etc. Hierarchical Bayes Hierarchical Bayes is a form of multilevel modelling that provides incredible flexibility for model specification. Of course, greater flexibility brings more opportunities to tie yourself into knots, but such is life. Most of this flexibility comes from specifying the priors as models with inputs and specifying relationships among the priors, or from one prior to multiple layers in the model. We don’t have time to explore all the options (because they’re almost endless). In the example I give below, we used a set of environmental covariates (soil, climate, topography, etc.) to explain the variation in the priors and constrain the parameters of the postfire recovery curve. In other words, Hierarchical Bayesian models allow you to fuse data through the parameter model. Of course, when you start treating your priors as parameters like this, you have to specify priors on your priors (hyperpriors) to your parameter model. Figure 10.3: Hierarchical Bayesian models allow considerable flexibility through the inclusion of hyperparameters that can drive the priors. 10.2.2 Hierarchical Bayes and postfire recovery example Until now we have only been considering a single postfire recovery curve in isolation, but one can fit every MODIS pixel in the Fynbos Biome in one model, like Wilson, Latimer, and Silander (2015) did, (here including the seasonality term in the process model): Figure 10.4: Schematic of the Hierarchical Bayesian model of postfire NDVI recovery developed by Wilson, Latimer, and Silander (2015). The beauty of this model is that it simultaneously estimates the posterior distributions of the parameters (by maximizing their likelihood given the observed NDVI data) while also estimating their relationship with a set of environmental covariates. This was done by fusing multiple data sources at different levels of the model. to the data model we’ve passed the \\(NDVI\\) time-series for each pixel note that we could fuse more data here as covariates or additional likelihood functions if we thought that additional complexity was useful to the process model we’ve passed the fire history for each pixel (as a time-series of vegetation \\(age\\) since fire) and the month (\\(m\\)) in which each fire occurred (needed for the seasonality term) to the parameter model we’ve passed static layers of each environmental covariate The advantages of including the regression of the parameters on environmental covariates are many: It allows you to explore the dependence of recovery trajectories on environmental covariates This allows one to project the expected parameters (and recovery trajectories) under altered environments (e.g. future climate scenarios - as Wilson, Latimer, and Silander (2015) did) It allows you to predict the expected postfire recovery trajectory for any site with known environmental covariates It puts an additional constraint on the parameter estimates for each pixel (i.e. “borrowing strength” across the analysis), down-weighting the effects of anomalous or noisy NDVI data The structure of the model allows you to easily estimate any missing data (i.e. inverse modelling) all within one model run A brief note on data fusion, model complexity and uncertainty: Increasing model complexity can provide many opportunities for fusing new data into your model, but you should be aware that this comes with trade-offs. Firstly, all data are uncertain (even if they don’t come with with uncertainty estimates), so adding new data sets or types to your model includes adding new sources of uncertainty. For example, a major failing of the postfire recovery model is that we did not include estimates of uncertainty in the covariates. This falsely reduces the uncertainty, resulting in overconfidence in our parameter estimates… Secondly, more terms and interactions in the model creates greater opportunity for strange feedbacks and trade-offs in the model’s mechanics, especially where there is non-identifiability (where multiple parameters can influence the outcome, but there’s not enough information in the model for it to partition their influence). This can bias or produce unrealistic estimates etc… Last, this is all over and above the usual dangers of overfitting. Utility of this model for decision makers While the model does not make explicit near-term forecasts in its current form, it can give us an estimate of the expected NDVI signal for any location for a given vegetation age since fire and time of year, which can be very useful! Slingsby, Moncrieff, and Wilson (2020) used the model to develop a proof of concept for a near-real time satellite change detection system for the Fynbos Biome. Figure 10.5: Overview of the near-real time satellite change detection workflow from Slingsby, Moncrieff, and Wilson (2020). The work flow comprises four major steps (Figure 10.5): Fit the model to get parameter estimates that describe the postfire recovery trajectory for all pixels Evaluate the deviation of the observed NDVI from model expectation using departure metrics to identify areas that are not ‘behaving’ as expected. Diagnose the change agents driving deviations through a combination of: interpreting the model predictions and observed NDVI signal exploring high resolution imagery like Sentinel or PlanetLabs field visits Finally, as new information is learned or false deviations are detected, the model is iteratively updated (manually at this stage) to improve its predictive accuracy. Preliminary results from the Cape Peninsula are very promising for detecting fire scars, vegetation clearing, vegetation mortality (e.g. drought impacts), and alien plant invasions. Figure 10.6: Examples of changes detected by Slingsby, Moncrieff, and Wilson (2020). Postfire recovery curves (a, d, g, j, l, n) show the model predictions (dark grey \\(=\\) 50% Confidence Interval, light grey \\(=\\) 95% Confidence Interval) and observed MODIS NDVI signal (blue line) for different impacts. Satellite images and ground photos allow for diagnosis of detected anomalies, including fire at Karbonkelberg (a-c), gradual clearing of alien vegetation at Miller’s Point (d-f), clearing of indigenous vegetation for a housing development near Silvermine (g-i, labeled 1), high mortality in a dense stand of Leucadendron coniferum near Silvermine (h-k, labelled 2), a flush of alien Australian Port Jackson Willow (Acacia saligna) recruitment from seed at Silvermine triggered by fire (l, m), and retarded postfire vegetation recovery and high mortality of the large fire-resistant shrub Mimetes fimbriifolius due to drought near the Cape of Good Hope (n, o). Satellite imagery © 2017 Planet Labs Inc. So managers can get an overview of change in the landscape every time new MODIS data are collected (daily, or 16-day averages). Figure 10.7: Overview map highlighting some of the major changes detected by Slingsby, Moncrieff, and Wilson (2020). We currently have a grant from NASA and are working with CapeNature, SANParks, SANBI, SAEON and others to develop into a fully operational system - currently called EMMA (Ecosystem Monitoring for Management Application). We’re also exploring methods to turn it into a bona fide near-term iterative ecological forecasting system, which brings us to… References "],["uncertainty.html", "11 Uncertainty 11.1 Sources and types of uncertainty 11.2 Propagating uncertainty 11.3 Analyzing and reducing uncertainty 11.4 Propagating and partitioning uncertainty in the impacts of invasive alien plants on streamflow", " 11 Uncertainty Uncertainty determines the utility of a forecast. If the uncertainty in a forecast is too high, then it is of no utility to a decision maker. Similarly, if the uncertainty is not properly quantified and presented, it can lead to poor decisions. This leaves forecasters with four overarching questions: What determines the limits to the utility of predictions? What determines prediction uncertainty? How can we propagate uncertainty through our models and into our predictions? How can we reduce prediction uncertainty? Question 1 is very much dependent on question 2, but more specifically it depends on the combination of the rate of accumulation of uncertainty (or loss of proficiency) through time, as it tends towards being no better than chance, combined with the precision or “forecast proficiency threshold” required for the decision in question. Together these determine the “ecological forecast horizon” (Figure 11.1; Petchey et al. (2015)). Some forecasts may lose proficiency very quickly, crossing (or starting below) the forecast proficiency threshold. Conversely, if the forecast loses proficiency more slowly, or the proficiency threshold requirements are lower, the forecast horizon is further into the future. Figure 11.1: The ecological forecast horizon (from Petchey et al. (2015)). 11.1 Sources and types of uncertainty Addressing question 2, “What determines prediction uncertainty?” involves exploring the sources and types of uncertainty. Dietze provides a nice classification of prediction uncertainty in his book (Dietze 2017a) and subsequent paper (Dietze 2017b) in the form of an equation (note that I’ve spread it over multiple lines): \\[ \\underbrace{Var[Y_{t+1}]}_\\text{predictive variance} \\approx \\; \\underbrace{stability*uncertainty}_\\text{initial conditions} \\; + \\\\ \\space\\\\ \\underbrace{sensitivity*uncertainty}_\\text{drivers} \\; + \\\\ \\space\\\\ \\underbrace{sensitivity*(uncertainty+variability)}_\\text{(parameters + random effects)} \\; + \\\\ \\space\\\\ \\underbrace{Var[\\epsilon]}_\\text{process error} \\; \\; \\] If we break the terms down into (something near) English, we get: The dependent variable: \\[Var[Y_{t+1}] \\approx\\] “The uncertainty in the prediction for the variable of interest (\\(Y\\)) in the next time step (\\(t+1\\)) is approximately equal to…” And now the independent variables (or terms in the model): \\[\\underbrace{stability*uncertainty}_\\text{initial conditions} \\; +\\] “The stability multiplied by the uncertainty in the initial conditions, plus” Here, the initial conditions = the state of \\(Y\\) at time \\(t\\) (our starting time point). Stability refers to whether it is a variable with stabilizing feedbacks (think of alternate stable states), versus whether it changes very quickly (with the extreme being one that tends towards chaos such as atmospheric conditions often do). Another example may be the populations of \\(r\\) (unstable) versus \\(K\\) (stable) selected species. Uncertainty is just uncertainty in the state of \\(Y\\) due to observation error. Then \\[\\underbrace{sensitivity * uncertainty}_\\text{drivers} \\; + \\] “The sensitivity to, multiplied by the uncertainty in, external drivers, plus” The external drivers are just the independent variables in the model (i.e. the covariates). So the predictability of \\(Y\\) depends on its sensitivity to each covariate (i.e. how much change would we expect in \\(Y\\) for a given change in the covariate), and the uncertainty in those covariates. Worst case scenario would be if \\(Y\\) was highly sensitive and the covariates were highly uncertain. Note that given we are forecasting through time, uncertainty in each covariate reflects how well we can forecast that covariate (e.g. future climate). If we can’t predict \\(X\\), we can’t use it to predict \\(Y\\)… That said, if \\(Y\\) is relatively insensitive to \\(X\\), then there’s less of a problem (apart from the question of why we’ve included it in the model in the first place?). Then \\[\\underbrace{sensitivity*(uncertainty+variability)}_\\text{(parameters + random effects)} + \\] “The sensitivity to, multiplied by the uncertainty and variability in, the parameters of the model, plus” Parameter uncertainty pertains to how good our estimates of the parameters are. This is usually a question of sample size - “Do we have enough data to obtain a good estimate (i.e. accurate mean, low uncertainty) of the parameters?”. It is also linked to the number of parameters in the model. The more parameters, the more data you need to obtain good parameter estimates. This is another reason to avoid overly complex models. Parameter sensitivity is similar to driver sensitivity - “How much change do we expect in \\(Y\\) for a given change in the parameter?” The overall contribution of a parameter to the predictive variance (i.e. uncertainty in the forecast) depends on its sensitivity multiplied by its uncertainty. Targeting fieldwork etc to better constrain poorly estimated parameters is one of the best ways to reduce prediction uncertainty. Parameter variability reflects factors that cause deviation (or offsets) from the mean of the parameter that may be known, but may either be poorly estimated or not included in the rest of the model. These are random effects that can be caused by factors like space, time, phylogeny, etc. Then \\[\\underbrace{Var[\\epsilon]}_\\text{process error}\\] “The process error.” This refers to errors in the model due to structural uncertainty and stochasticity. Stochasticity refers to ecological phenomena of relevance that are very difficult to predict (at least within the context of the focal model). Examples include the occurrence of fire, dispersal or mortality that are often chance events similar to a coin toss. Model structural uncertainty simply reflects that all models are simplifications of reality and none are perfect. We’ll always be missing something. That said, this could also include “user error” where the forecaster specified the wrong process model, or applied the wrong probability distribution in the data model, etc. Working with multiple models and employing model selection or averaging can help reduce structural uncertainty (in addition to thinking really hard about how to specify a better model of course…). 11.2 Propagating uncertainty There are many methods to address question 3, “How can we propagate uncertainty through our models and into our predictions?”, but it’s worth recognizing that these are actually two steps: Propagating uncertainty through the model i.e. in fitting the model, so we can include uncertainty in our parameter estimates This is typically focused on “How does the uncertainty in X affect the uncertainty in Y?” Propagating uncertainty into our forecasts i.e. exploring the implications of uncertainty in our model (parameters etc) for our confidence in the forecast when making predictions with our fitted model Here we focus on “How do we forecast Y with uncertainty?” This second step is actually the first step in data assimilation, which we’ll discuss tomorrow I’m not going to spend much time on this, but suffice to say it could be a lecture series of its own. In (very) short, there are 5 methods to address step 1, and most of these cross-walk to related methods for step 2 (see Table 11.1). Among these methods are distinctions among whether they: Return distributions (e.g. Gaussian curve) or moments (means, medians, standard deviations, etc) They have analytical solutions, or need to be approximated numerically They also have trade-offs between efficiency vs flexibility, with the most efficient having the most rigid requirements and assumptions (analytical), while the most flexible (numeric) can be very computationally taxing (or impossible given a complex enough model). Table 11.1: Methods for propagating uncertainty through models (and into forecasts) Approach Distribution Moments Analytical Variable Transform Analytical Moments (Kalman Filter) Taylor Series (Extended Kalman Filter) Numerical Monte Carlo (Particle Filter) Ensemble (Ensemble Kalman Filter) Note: It is possible to propagate uncertainty through the model and into your forecast in one step with Bayesian methods, by treating the forecast states as “missing data” values and estimating posterior distributions for them. This would essentially fit with Monte Carlo methods in Table 11.1. This approach may not suit all forecasting circumstances though. 11.3 Analyzing and reducing uncertainty Question 4, “How can we reduce prediction uncertainty?” requires: Working out where it’s coming from (by analyzing and partitioning the sources of uncertainty). Targeting sources of uncertainty that can be reduced with the best return on investment (important to note that these may not be the biggest sources of uncertainty, just the cheapest and easiest to resolve). Addressing 1 requires looking at the two ways in which things can be important for the uncertainty in predictions (largely covered in Dietze’s equation above): because they’re highly uncertain, which requires you: propagating uncertainty through the model as above partitioning uncertainty among your different drivers (covariates) and parameters because they’re highly sensitive, requiring you to perform: sensitivity analysis I assume you covered these when you did matrix models. I’m not going to go into them here, but they focus on how a change in X translates into a change in Y. The bigger the relative change in Y, the more sensitive. Addressing 2 may not be as straightforward as you’d hope. Parameters that are highly uncertain and to which your state variable (Y) are highly sensitive will cause the most uncertainty in your predictions. That said, given limited resources, they may not be the best target for reducing uncertainty for a number of reasons, e.g. they may be inherently uncertain and uncertainty may remain high even with vast sampling effort power analysis can help here (i.e. exploring how uncertainty changes with sample size) they may be hugely costly or time-consuming, trading off against resources you could focus on reducing other sources of uncertainty In fact, by this stage you should have most of the pieces of the puzzle to help you build a model to predict where your effort is best invested by exploring the relationship between sample size and variance contribution to overall model uncertainty! You can even include economic principles to estimate monetary or person-hour implications. This is called observational design. 11.4 Propagating and partitioning uncertainty in the impacts of invasive alien plants on streamflow 11.4.1 The background During the “Day Zero” drought municipalities like the City of Cape Town scrambled to access “alternative sources” of water for bulk supply. The options they explored (beyond demand management) included: Desalination Reclamation (i.e. purifying waste water) Groundwater (from two very different aquifers, the Cape Flats Sand Aquifer and the Table Mountain Group Aquifer) At that stage there was published peer-reviewed research by Le Maitre et al. (2016) indicating that as of 2008 invasive alien plants were estimated to be using around 5% of runoff (almost as much as Wemmershoek Dam or ~80 days worth of water under restrictions). There was additional research showing that the extent of alien invasions had become much worse since 2008, so these figures were likely to be an underestimate. When asked why they were not considering clearing invasive alien plants from the major mountain catchments, one of the excuses was [paraphrased] “because we don’t trust the estimates, they don’t provide any estimates of uncertainty”. While we knew this was a load of crock (they didn’t have uncertainty estimates for any of the other options either) Moncrieff, Slingsby, and Le Maitre (2021) decided that it’d be a good idea to explore this issue and: use a Bayesian framework to update and include uncertainty in the estimates of the volume and percent of streamflow lost to IAPs from the catchments explore the relative contribution of sources of uncertainty to overall uncertainty in streamflow losses to guide efforts to improve future estimates make it easy for anyone to recalculate estimates as and when updated data become available by adopting an open science approach, using only open-source software and sharing all data and code to provide a fully repeatable workflow 11.4.2 The Analysis The impacts of invasive alien plants (IAPs) on streamflow is predominantly determined from streamflow reduction curves from long-term catchment experiments, whereby the proportional reduction in streamflow is expressed as a function of plantation age and/or density. These can take 40 years to collect, and the Jonkershoek catchment study has been running for &gt;80 years (Slingsby et al. 2021)!!! Figure 11.2: Streamflow reduction curves for pine and eucalypt plantations under normal (suboptimal) or wet (optimal) conditions (from Moncrieff, Slingsby, and Le Maitre (2021)). These curves are then used to extrapolate spatially to MODIS pixels (250m) nested within catchments, informed by the naturalized runoff and IAP density. Propagating uncertainty in streamflow reduction For our analysis, we made sure that we only used inputs (rainfall, streamflow reduction curves, fire history, soil moisture, invasion density) that could be sampled with uncertainty (i.e. from probability distributions) and then propagated that uncertainty through to our streamflow reduction estimates using a Monte Carlo (MC) approach. Figure 11.3: Overview of the simulations run by Moncrieff, Slingsby, and Le Maitre (2021) to estimate the impacts of IAPs on streamflow in the catchments of the Cape Floristic Region including uncertainty (from Moncrieff, Slingsby, and Le Maitre (2021)) The MC simulation steps follow a series of nested loops: For each model run we: Assigned each species to a streamflow reduction curve (optimal or sub-optimal, Eucalypt or Pine) Sampled a map of vegetation age from the distribution of fire return time Estimated proportional streamflow reduction for every IAP species from the assigned curve (sampled from posterior distribution of curves) and age. Determined additional water usage by IAPs in riparian zones and areas where groundwater is accessible Within each run, for each catchment we sample the density of each IAP species Within each catchment, for each pixel we: Estimated pixel-level naturalized runoff by sampling precipitation and converting to runoff Corrected for bias in naturalized runoff by summing naturalized runoff across all pixels within each quaternary catchment and rescaling to match estimates from Bailey and Pitman (2015) Determined whether additional water in riparian zones or groundwater is accessible to IAPs Calculated the runoff lost to IAPs by multiplying potential runoff from each pixel by the proportional streamflow reduction for every IAP species, and summing across all species. 11.4.3 The Results Figure 11.4: Posterior probability distributions showing uncertainty in the impacts of IAPs on streamflow in the catchments feeding Cape Towns major dams (from Moncrieff, Slingsby, and Le Maitre (2021)). This process provided us with estimates of the impacts of IAPs on streamflow for all catchments in the Cape Floristic Region as posterior probability distributions - i.e. with uncertainty. The posterior mean estimated streamflow loss to IAPs in the quaternary catchments surrounding Cape Town’s major dams was 25.5 million m\\(^3\\) per annum, with lower and upper estimates of 20.3 and 43.4 million m\\(^3\\) respectively. Given the City’s target water use of 0.45 million m\\(^3\\) per day at the height of the drought, this translates to between 45 and 97 days of water supply!!! Note that this was still using the 2008 estimates of IAP invasions… Partitioning sources of uncertainty Beyond estimating the impacts of IAPs with uncertainty, Moncrieff, Slingsby, and Le Maitre (2021) performed additional analyses to partition the uncertainty among the various potential sources. For this we examined the relative contribution of a each source to overall uncertainty in streamflow reduction by setting the uncertainty in all other inputs to zero. Figure 11.5: The estimated uncertainty attributed to each of the model inputs (Moncrieff, Slingsby, and Le Maitre 2021) From this it’s clear that the data we need most is better estimates of the extent and density of invasions!!! Fortunately, this is easier to get than more 40-year catchment experiments! How did our results compare to the original estimates? Figure 11.6: Comparison of estimates of IAP impacts on streamflow with (Moncrieff, Slingsby, and Le Maitre 2021) and without (Le Maitre et al. 2016) uncertainty. By comparing our results to Le Maitre et al. (2016) we can see that our estimates are very similar, albeit slightly higher for low density invasions and lower for high density invasions. Either way, the losses are huge and likely to have been much worse during the “Day Zero” drought. Fortunately, a lot of effort has been invested in advocating for alien clearing and catchment restoration and this is starting to happen at scale through interventions like The Greater Cape Town Water Fund. While Glenn is adamant that these differences aren’t the result of Jensen’s Inequality, I thought it worth flagging Jensen’s inequality since (a) it’s likely to trip you up one day, and (b) I find it to be another good reason why we should always estimate things with uncertainty! Jensen’s Inequality takes many forms, but in this context it indicates that the mean of a nonlinear function is not equal to the function evaluated at the mean of its inputs… (i.e. If you run your model under your mean parameter set and your model contains non-linear components, what comes out is not your mean outcome…) Incidentally, this entire analysis is completely reproducible, with all data, code and a docker container on GitHub here. References "],["decisions.html", "12 Completing the forecast cycle 12.1 Data assimilation 12.2 Decision support 12.3 Final words 12.4 Revision questions", " 12 Completing the forecast cycle Here we’ll complete the forecast cycle (through data assimilation) and spend a little time discussing decision support. 12.1 Data assimilation Figure 12.1: The iterative ecological forecasting cycle in the context of the scientific method, demonstrating how we stand to learn from making iterative forecasts. From lecture on data assimilation by Michael Dietze. (Please excuse that the colours of the distributions have changed from above…). You’ll recall from the introductory lecture on models and decision making (section 3) that we outlined iterative ecological forecasting and the scientific method as closely aligned cycles or loops. You’ll also recall that I made the point in section 9 that Bayes Theorem provides an iterative probabilistic framework that makes it easier to update predictions as new data become available, mirroring the scientific method and completing the forecasting cycle. Here we’ll unpack this in a bit more detail. 12.1.1 Operational data assimilation Most modelling workflows are set up to (a) fit the available data and estimate parameters etc (we’ll call this analysis), which they then often use to (b) make predictions (which if made forward through time are typically forecasts). They usually stop there. Few workflows are set up to make forecasts repeatedly or iteratively, updating predictions as new observations are made. When making iterative forecasts, one could just refit the model and entire dataset with the new observations added, but there are a few reasons this may not be ideal: If it’s a complex model and/or a large dataset this could be very computationally taxing (e.g. Slingsby, Moncrieff, and Wilson (2020) ran the Hierarchical postfire model for the Cape Peninsula (~4000 MODIS pixels) and it took 4 days on a computing cluster with 64 cores and 1TB of RAM…). Refitting the model doesn’t make the most of learning from new observations and the forecast cycle… The alternative is to assimilate the data sequentially, through forecast cycles, imputing observations a bit at a time as they’re observed. This approach has several advantages: They can handle larger datasets, because you don’t have to assimilate all data in one go. If you start the model far in the past and update towards present day, you have the opportunity to validate your predictions, telling you how well the model does, and whether it improves with each iteration (i.e. is it learning), which gives you a good feel for how well it should do at forecasting into the future. Think of it as letting your model get a run-up like a sportsperson before jumping/bowling/throwing. Assimilating data sequentially is known as the sequential or operational data assimilation problem and occurs through two steps (the main components of the forecast cycle), the forecast step, where we project our estimates of the current state forward in time, and the analysis step, where we update our estimate of the state based on new observations. Figure 12.2: The two main components of the forecast cycle are the forecast step (stippled lines), where we project from the initial state at time 0 (t0) to the next time step (t0+1), and the analysis step, where we use the forecast and new observations to get an updated estimate of the current state at t0+1. 12.1.2 The Forecast Step While it gets a bit like the chicken and the egg dilemma, the first step has to be analysis, because you have to fit your model before you can make your first forecast (although you could argue that your first analysis requires priors, which could be viewed as your first forecast…). Either way, the forecast step is probably easier to explain first. The goals of the forecast step are to: To predict what we think may happen (or what the focal variable(s) will be) at the next time step Indicate the uncertainty in our forecast (based on the uncertainty that we have propagated through our model from various sources (data, priors, etc)) In short, we want to propagate uncertainty in our variable(s) of interest forward through time (and sometimes through space, depending on the goals). There are a number of methods for propagating uncertainty into a forecast, mostly based on the same methods one would use to propagate the uncertainty through a model (see Table 11.1). Explaining the different methods more than I did in section (11) is beyond the scope of this module, but just a reminder that (as with almost everything in statistics) there’s a trade-off between the methods whereby the most efficient (the Kalman filter in this case) also come with the most stringent assumptions (linear models and homogeneity of variance only), while the most flexible (Markov chain Monte Carlo (MCMC) in this case) are the most computationally taxing. In short, if your model isn’t too taxing, or you have cheap access to a very large computer and time to kill, MCMC is probably easiest and best… 12.1.3 The Analysis Step This step involves using Bayes Theorem to combine our prior knowledge (our forecast) with our new observations (at t0+1 in Figure 12.2) to generate an updated state for the next forecast (i.e. to t0+2). Figure 12.3: The forecast cycle chaining together applications of Bayes Theorem at each timestep (t0, t1, …). The forecast from one timestep becomes the prior for the next. Note that the forecast is directly sampled as a posterior distribution when using MCMC, but can also be propagated using other methods (see Table 11.1). This is better than just using the new data as your updated state, because: it uses our prior information and understanding it allows our model to learn and (hopefully) improve with each iteration there is likely error (noise) in the new data, so it can’t necessarily be trusted more than our prior understanding anyway Fortunately, Bayes Theorem deals with all of this very nicely, because if the forecast (prior) is uncertain and the new data precise then the data will prevail, whereas if the forecast is precise and the new data uncertain, then the posterior will retain the legacy of previous observations (Figure 12.4). Figure 12.4: The result of (A) high forecast uncertainty (the prior) and low observation error (data), and (B) low forecast uncertainty and high observation error on the posterior probability from the analysis step. Lastly, just a note that I’ve mostly dealt with single forecasts and haven’t talked about how to deal with ensemble forecasts. In short, there are data assimilation methods to deal with them, but we don’t have time to cover them. The methods, and how you apply them, depend on what kind of ensemble you are dealing with. Usually, ensembles can be divided into three kinds, but you can have mixes of all three: Where you use the same model, but vary the inputs to explore different scenarios. Where you have a set of nested models of increasing complexity (e.g. like our postfire models with and without the seasonality term). A mix of models with completely different model structures aimed at forecasting the same thing. 12.2 Decision support This is probably the hardest part of the whole ecological forecasting business - people… It is also a huge topic and not one I can cover in half a lecture. Here I just touch on a few hints and difficulties. First and foremost, the decision at hand may not be amenable to a quantitative approach. Ecological forecasting requires a clearly defined information need with a measurable (and modelable) state variable, framed within one or multiple decision alternatives (scenarios). There’s also the risk of external factors making the forecasts unreliable, especially if they are not controlled by the decision maker and/or their probability is unknown (e.g. fire, pandemics, etc). These external factors are where developing scenarios with different boundary conditions can be very useful e.g. a scenario with versus a scenario without a fire, or the different future climate states under the alternative development pathways, etc. Scenarios are often “what if” statements designed to address major sources of uncertainty that make it near-impossible to make accurate predictions with a single forecast. It’s perhaps useful to note the distinction between predictions versus projections at this point: predictions are statements about the probability of the occurrence of events or the state of variables in the future based on what we currently know projections are statements about the probability of the occurrence of events or the state of variables in the future given specific scenarios with clear boundary conditions If the prerequisites to be able to do ecological forecasting are met, then… 12.2.1 In an ideal world… You’ll be working with an organized team that is a well-oiled machine at implementing Adaptive Management and Structured Decision Making and you can naturally slot into their workflow. The advantages of Adaptive Management and Structured Decision Making are that they are founded on the concept of iterative learning cycles, which they have in common with the ecological forecasting cycle and the scientific method. Figure 12.5: Conceptual relationships between iterative ecological forecasting, adaptive decision-making, adaptive monitoring, and the scientific method cycles (Dietze et al. 2018). You’re already familiar with how the iterative ecological forecast cycle integrates with the Adaptive Management Cycle… The beauty for the forecaster in this scenario is that a lot of the work is already done. The decision alternatives (scenarios) have been well framed, the performance measures, state variables of interest and associated covariates identified. Iterations of the learning cycle may even have already begun (through the Adaptive Management Cycle) and all you need do is develop the existing qualitative model into something more quantitative as more data and understanding are accumulated. Think of the Protea example in section 4, where the demography of these species is already used for decision making using semi-quantitative “rules of thumb”. Figure 12.6: The Structured Decision Making Cycle sensu Gregory et al. (2012). Structured Decision Making, as the name suggests, is far more focused on assisting with the process of coming to a decision than the process of management, but it is particularly useful in the first iteration of the Adaptive Management Cycle. Gregory et al. (2012) provide a very nice magnum opus on Structured Decision Making in natural resource management. It is particularly valuable when there are many stakeholders with disparate interests. We can model properties and performance measures, but decisions are ultimately about values and often require evaluating trade-offs among properties with incomparable units, e.g. people housed/fed/watered vs species saved from extinction… This can rapidly become a highly emotive space. It is also a space where the values that dominate are often the ones that are shouted the loudest and other important values are marginalized. One of the major goals of Structured Decision Making is to make sure all issues and values are brought to light and considered in a transparent framework so that trade-offs can be clearly identified and considered. It also aims to ensure that all alternatives are considered, since it’s impossible to make the right choice if it isn’t even on the table. A key strength in Structured Decision Making is in attempting to directly address the social, political or cognitive biases that limit the ability to highlight all values or alternatives. - e.g. many decisions in South Africa (and globally) pit people’s immediate needs (water, housing, etc) against the environment. These are incredibly difficult decisions, and while people (especially politicians) would rather ignore the fact that choosing one is choosing against the other, if we’re not transparent about this we’re not going to learn from our decisions and improve them in the next iteration. 12.2.2 In reality… Many (if not most) decisions are made with very little information and/or within institutional structures that are not well designed for iterative learning and adaptation. In fact, you’ll often encounter situations where people aren’t interested in the information you have to offer, even if it’s exactly what they need. You may often find they don’t know what they need, so you may need to tell them, if you can get the chance… This may well be met with resistance, because you’re adding what is perceived to be unnecessary complexity to an otherwise straightforward decision scenario (e.g. most engineers prefer to ignore environmental concerns - the first step is often just making sure that the environment is even considered among the objectives and performance measures…). That said, you should also have the humility recognize that it is very possible that what you have to offer is not what the decision maker needs, despite your preconceptions. Effective engagement with decision makers requires a lot of empathy (putting yourself in their shoes) and willingness to listen, combined with a good dose of taking them to task when needed (i.e. when they’re ignoring important information out if ignorance or conveniance). You’re unlikely to know the full complexity and constraints of the institutional (and legal) frameworks within which they have to work. They may well be on your side personally, but can’t show that formally. That said, they may be overly blinkered by their institutional frameworks and norms and need a little disruption to force them to think outside the box. There’s also a tricky balance between knowing what you can offer (based on the available data and the various factors that determine whether you can develop a useful forecast) and learning what is needed (either from or by working with decision makers). On the one hand, designing an “over-the-wall” forecasting solution without engagement with stakeholders is unlikely to be well received. On the other hand, discussion of potential forecasting solutions with decision makers does need to be constrained by the reality of the limits to forecasting, otherwise you may be asked to (or worse - you may promise to) deliver on an impossible forecasting task. “Unconstrained” discussions typically lead to time-wasting and anymosity - “Why were we discussing this if it was never a possibility?!” In many ways working with decision makers is about slowly building relationships and exploring possibilities together, possibly focusing on one or a few allies who see the potential value early. If you can use their input to help develop something useful, it becomes much easier to demonstrate realistic value to the broader group, often championed by your allies. Then you’re well placed to refine and improve it. 12.3 Final words We need to move past the traditional ecologist’s view that after decades’ years worth of data collection and hypothesis testing, maybe we’ll know enough to start building a predictive model… And start adopting an engineering (or decision-makers’) view (at least one that subscribes to some iterative learning paradigm) - start building a model with no or very little data, and use that to guide what data to collect and how to improve the it. It may be completely wrong to begin with, but will rapidly improve. 12.4 Revision questions Because you asked so nicely… These would all be essay questions (30-50 marks, with how much you write depending on the mark and time allocation). Discuss the main requirements for data used in ecological forecasts and how they may (or may not) be resolved through adherence to the Data Life Cycle. Why is there a crisis in the way we do science, what can we do about it, and what are the additional benefits? What are the benefits of Bayesian approaches over other statistical frameworks in the context of ecological forecasting and how does the Bayesian framework make them possible? Discuss the ways in which one can fuse data with models and how this may be limited by your statistical approach. You have been tasked with improving the proficiency of an ecological forecasting model. Discuss the potential sources of uncertainty, how you would identify them and deploy limited resources to reduce them. Discuss the ecological forecasting cycle and how it integrates with and accelerates learning within existing management and knowledge generation frameworks. References "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

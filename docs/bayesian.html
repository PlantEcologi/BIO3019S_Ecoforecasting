<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>7 Going Bayesian | An Introduction to Ecological Forecasting</title>
  <meta name="description" content="This is a minimal introduction to Ecological Forecasting and Reproducible Research for the 3rd year undergraduate Biological Sciences BIO3019S class at the University of Cape Town." />
  <meta name="generator" content="bookdown 0.37 and GitBook 2.6.7" />

  <meta property="og:title" content="7 Going Bayesian | An Introduction to Ecological Forecasting" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a minimal introduction to Ecological Forecasting and Reproducible Research for the 3rd year undergraduate Biological Sciences BIO3019S class at the University of Cape Town." />
  <meta name="github-repo" content="jslingsby/BIO3019S_Ecoforecasting" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7 Going Bayesian | An Introduction to Ecological Forecasting" />
  
  <meta name="twitter:description" content="This is a minimal introduction to Ecological Forecasting and Reproducible Research for the 3rd year undergraduate Biological Sciences BIO3019S class at the University of Cape Town." />
  

<meta name="author" content="Jasper Slingsby" />


<meta name="date" content="2024-08-02" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="data.html"/>
<link rel="next" href="datafusion.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Introduction to Ecological Forecasting</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Overview</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#general"><i class="fa fa-check"></i><b>1.1</b> General</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#lectures-and-practicals"><i class="fa fa-check"></i><b>1.2</b> Lectures and practicals</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#preparation"><i class="fa fa-check"></i><b>1.3</b> Preparation</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#a-bit-about-me"><i class="fa fa-check"></i><b>1.4</b> A bit about me</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#acknowledgements-and-further-reading"><i class="fa fa-check"></i><b>1.5</b> Acknowledgements and further reading:</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="pracprep.html"><a href="pracprep.html"><i class="fa fa-check"></i><b>2</b> Preparation for the practical</a></li>
<li class="chapter" data-level="3" data-path="models.html"><a href="models.html"><i class="fa fa-check"></i><b>3</b> Models and decision making</a>
<ul>
<li class="chapter" data-level="3.1" data-path="models.html"><a href="models.html#the-basics-of-making-a-decision"><i class="fa fa-check"></i><b>3.1</b> The basics of making a decision</a></li>
<li class="chapter" data-level="3.2" data-path="models.html"><a href="models.html#getting-quantitative"><i class="fa fa-check"></i><b>3.2</b> Getting quantitative</a></li>
<li class="chapter" data-level="3.3" data-path="models.html"><a href="models.html#iterative-decision-making"><i class="fa fa-check"></i><b>3.3</b> Iterative decision-making</a></li>
<li class="chapter" data-level="3.4" data-path="models.html"><a href="models.html#iterative-decision-making-and-the-scientific-method"><i class="fa fa-check"></i><b>3.4</b> Iterative decision-making and the scientific method</a></li>
<li class="chapter" data-level="3.5" data-path="models.html"><a href="models.html#the-importance-of-prediction-in-ecology"><i class="fa fa-check"></i><b>3.5</b> The importance of prediction in ecology</a></li>
<li class="chapter" data-level="3.6" data-path="models.html"><a href="models.html#iterative-near-term-ecological-forecasting"><i class="fa fa-check"></i><b>3.6</b> Iterative near-term ecological forecasting</a></li>
<li class="chapter" data-level="3.7" data-path="models.html"><a href="models.html#iterative-ecological-forecasting-in-context"><i class="fa fa-check"></i><b>3.7</b> Iterative ecological forecasting in context</a></li>
<li class="chapter" data-level="3.8" data-path="models.html"><a href="models.html#reproducible-research"><i class="fa fa-check"></i><b>3.8</b> Reproducible research</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="forecasts.html"><a href="forecasts.html"><i class="fa fa-check"></i><b>4</b> Making forecasts</a>
<ul>
<li class="chapter" data-level="4.1" data-path="forecasts.html"><a href="forecasts.html#proteaceae-as-model-organisms"><i class="fa fa-check"></i><b>4.1</b> Proteaceae as model organisms</a></li>
<li class="chapter" data-level="4.2" data-path="forecasts.html"><a href="forecasts.html#proteaceae-as-management-indicators"><i class="fa fa-check"></i><b>4.2</b> Proteaceae as management indicators</a></li>
<li class="chapter" data-level="4.3" data-path="forecasts.html"><a href="forecasts.html#potential-issues-with-the-rules-of-thumb"><i class="fa fa-check"></i><b>4.3</b> Potential issues with the rules of thumb…</a></li>
<li class="chapter" data-level="4.4" data-path="forecasts.html"><a href="forecasts.html#assessing-population-viability"><i class="fa fa-check"></i><b>4.4</b> Assessing population viability</a></li>
<li class="chapter" data-level="4.5" data-path="forecasts.html"><a href="forecasts.html#climate-and-fire-driven-changes-in-demographic-rates-and-distribution"><i class="fa fa-check"></i><b>4.5</b> Climate and fire-driven changes in demographic rates and distribution</a></li>
<li class="chapter" data-level="4.6" data-path="forecasts.html"><a href="forecasts.html#near-term-iterative-ecological-forecasts"><i class="fa fa-check"></i><b>4.6</b> Near-term iterative ecological forecasts?</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="reproducibility.html"><a href="reproducibility.html"><i class="fa fa-check"></i><b>5</b> Reproducible research</a>
<ul>
<li class="chapter" data-level="5.1" data-path="reproducibility.html"><a href="reproducibility.html#the-reproducibility-crisis"><i class="fa fa-check"></i><b>5.1</b> The Reproducibility Crisis</a></li>
<li class="chapter" data-level="5.2" data-path="reproducibility.html"><a href="reproducibility.html#replication-and-the-reproducibility-spectrum"><i class="fa fa-check"></i><b>5.2</b> Replication and the Reproducibility Spectrum</a></li>
<li class="chapter" data-level="5.3" data-path="reproducibility.html"><a href="reproducibility.html#reproducible-scientific-workflows"><i class="fa fa-check"></i><b>5.3</b> Reproducible Scientific Workflows</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="reproducibility.html"><a href="reproducibility.html#file-and-folder-management"><i class="fa fa-check"></i><b>5.3.1</b> File and folder management</a></li>
<li class="chapter" data-level="5.3.2" data-path="reproducibility.html"><a href="reproducibility.html#coding-and-code-management"><i class="fa fa-check"></i><b>5.3.2</b> Coding and code management</a></li>
<li class="chapter" data-level="5.3.3" data-path="reproducibility.html"><a href="reproducibility.html#computing-environment-and-software"><i class="fa fa-check"></i><b>5.3.3</b> Computing environment and software</a></li>
<li class="chapter" data-level="5.3.4" data-path="reproducibility.html"><a href="reproducibility.html#sharing-of-the-data-code-publication-etc"><i class="fa fa-check"></i><b>5.3.4</b> Sharing of the data, code, publication etc</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="reproducibility.html"><a href="reproducibility.html#why-work-reproducibly"><i class="fa fa-check"></i><b>5.4</b> Why work reproducibly?</a></li>
<li class="chapter" data-level="5.5" data-path="reproducibility.html"><a href="reproducibility.html#barriers-to-working-reproducibly"><i class="fa fa-check"></i><b>5.5</b> Barriers to working reproducibly</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>6</b> Data Management</a>
<ul>
<li class="chapter" data-level="6.1" data-path="data.html"><a href="data.html#why-do-you-need-to-manage-your-data"><i class="fa fa-check"></i><b>6.1</b> Why do you need to manage your data?</a></li>
<li class="chapter" data-level="6.2" data-path="data.html"><a href="data.html#the-data-life-cycle"><i class="fa fa-check"></i><b>6.2</b> The Data Life Cycle</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="data.html"><a href="data.html#plan"><i class="fa fa-check"></i><b>6.2.1</b> Plan</a></li>
<li class="chapter" data-level="6.2.2" data-path="data.html"><a href="data.html#collect-and-assure"><i class="fa fa-check"></i><b>6.2.2</b> Collect and Assure</a></li>
<li class="chapter" data-level="6.2.3" data-path="data.html"><a href="data.html#describe"><i class="fa fa-check"></i><b>6.2.3</b> Describe</a></li>
<li class="chapter" data-level="6.2.4" data-path="data.html"><a href="data.html#preserve"><i class="fa fa-check"></i><b>6.2.4</b> Preserve</a></li>
<li class="chapter" data-level="6.2.5" data-path="data.html"><a href="data.html#discover"><i class="fa fa-check"></i><b>6.2.5</b> Discover</a></li>
<li class="chapter" data-level="6.2.6" data-path="data.html"><a href="data.html#integrate"><i class="fa fa-check"></i><b>6.2.6</b> Integrate</a></li>
<li class="chapter" data-level="6.2.7" data-path="data.html"><a href="data.html#analyze"><i class="fa fa-check"></i><b>6.2.7</b> Analyze</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="data.html"><a href="data.html#data-and-decisions"><i class="fa fa-check"></i><b>6.3</b> Data and decisions</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="bayesian.html"><a href="bayesian.html"><i class="fa fa-check"></i><b>7</b> Going Bayesian</a>
<ul>
<li class="chapter" data-level="7.1" data-path="bayesian.html"><a href="bayesian.html#least-squares"><i class="fa fa-check"></i><b>7.1</b> Least Squares</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="bayesian.html"><a href="bayesian.html#least-squares-doesnt-explicitly-include-a-data-model"><i class="fa fa-check"></i><b>7.1.1</b> Least Squares doesn’t explicitly include a data model</a></li>
<li class="chapter" data-level="7.1.2" data-path="bayesian.html"><a href="bayesian.html#least-squares-focuses-on-what-the-parameters-are-not-rather-than-what-they-are"><i class="fa fa-check"></i><b>7.1.2</b> Least squares focuses on what the parameters are not, rather than what they are</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="bayesian.html"><a href="bayesian.html#maximum-likelihood"><i class="fa fa-check"></i><b>7.2</b> Maximum likelihood</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="bayesian.html"><a href="bayesian.html#joint-conditional-and-marginal-probabilities"><i class="fa fa-check"></i><b>7.2.1</b> Joint, conditional and marginal probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="bayesian.html"><a href="bayesian.html#bayes-theorem"><i class="fa fa-check"></i><b>7.3</b> Bayes’ Theorem</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="bayesian.html"><a href="bayesian.html#the-beauty-of-bayes-theorem"><i class="fa fa-check"></i><b>7.3.1</b> The beauty of Bayes’ Theorem</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="datafusion.html"><a href="datafusion.html"><i class="fa fa-check"></i><b>8</b> Data Fusion</a>
<ul>
<li class="chapter" data-level="8.1" data-path="datafusion.html"><a href="datafusion.html#a-reminder-of-bayes-theorem-and-its-benefits"><i class="fa fa-check"></i><b>8.1</b> A reminder of Bayes Theorem and its benefits</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="datafusion.html"><a href="datafusion.html#the-benefits-of-bayes"><i class="fa fa-check"></i><b>8.1.1</b> The benefits of Bayes</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="datafusion.html"><a href="datafusion.html#data-fusion"><i class="fa fa-check"></i><b>8.2</b> Data fusion</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="datafusion.html"><a href="datafusion.html#model-layers"><i class="fa fa-check"></i><b>8.2.1</b> Model layers</a></li>
<li class="chapter" data-level="8.2.2" data-path="datafusion.html"><a href="datafusion.html#hierarchical-bayes-and-postfire-recovery-example"><i class="fa fa-check"></i><b>8.2.2</b> Hierarchical Bayes and postfire recovery example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="statespace.html"><a href="statespace.html"><i class="fa fa-check"></i><b>9</b> Latent variables and state-space models</a>
<ul>
<li class="chapter" data-level="9.1" data-path="statespace.html"><a href="statespace.html#latent-variables-state-space-models"><i class="fa fa-check"></i><b>9.1</b> Latent variables &amp; state-space models</a></li>
<li class="chapter" data-level="9.2" data-path="statespace.html"><a href="statespace.html#latent-variables"><i class="fa fa-check"></i><b>9.2</b> Latent variables</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="statespace.html"><a href="statespace.html#observation-error"><i class="fa fa-check"></i><b>9.2.1</b> Observation error</a></li>
<li class="chapter" data-level="9.2.2" data-path="statespace.html"><a href="statespace.html#proxy-data"><i class="fa fa-check"></i><b>9.2.2</b> Proxy data</a></li>
<li class="chapter" data-level="9.2.3" data-path="statespace.html"><a href="statespace.html#missing-data"><i class="fa fa-check"></i><b>9.2.3</b> Missing data</a></li>
<li class="chapter" data-level="9.2.4" data-path="statespace.html"><a href="statespace.html#unobserved-variables"><i class="fa fa-check"></i><b>9.2.4</b> Unobserved variables</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="statespace.html"><a href="statespace.html#state-space-models"><i class="fa fa-check"></i><b>9.3</b> State-space models</a></li>
<li class="chapter" data-level="9.4" data-path="statespace.html"><a href="statespace.html#state-space-models---illustration"><i class="fa fa-check"></i><b>9.4</b> State-space models - illustration</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="statespace.html"><a href="statespace.html#state-space-models---simple-example"><i class="fa fa-check"></i><b>9.4.1</b> State-space models - simple example</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="statespace.html"><a href="statespace.html#state-space-models---postfire-example"><i class="fa fa-check"></i><b>9.5</b> State-space models - postfire example</a></li>
<li class="chapter" data-level="9.6" data-path="statespace.html"><a href="statespace.html#state-space-model-uses"><i class="fa fa-check"></i><b>9.6</b> State-space model uses</a></li>
<li class="chapter" data-level="9.7" data-path="statespace.html"><a href="statespace.html#references"><i class="fa fa-check"></i><b>9.7</b> References</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="uncertainty.html"><a href="uncertainty.html"><i class="fa fa-check"></i><b>10</b> Uncertainty</a>
<ul>
<li class="chapter" data-level="10.1" data-path="uncertainty.html"><a href="uncertainty.html#sources-and-types-of-uncertainty"><i class="fa fa-check"></i><b>10.1</b> Sources and types of uncertainty</a></li>
<li class="chapter" data-level="10.2" data-path="uncertainty.html"><a href="uncertainty.html#propagating-uncertainty"><i class="fa fa-check"></i><b>10.2</b> Propagating uncertainty</a></li>
<li class="chapter" data-level="10.3" data-path="uncertainty.html"><a href="uncertainty.html#analyzing-and-reducing-uncertainty"><i class="fa fa-check"></i><b>10.3</b> Analyzing and reducing uncertainty</a></li>
<li class="chapter" data-level="10.4" data-path="uncertainty.html"><a href="uncertainty.html#propagating-and-partitioning-uncertainty-in-the-impacts-of-invasive-alien-plants-on-streamflow"><i class="fa fa-check"></i><b>10.4</b> Propagating and partitioning uncertainty in the impacts of invasive alien plants on streamflow</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="uncertainty.html"><a href="uncertainty.html#the-background"><i class="fa fa-check"></i><b>10.4.1</b> The background</a></li>
<li class="chapter" data-level="10.4.2" data-path="uncertainty.html"><a href="uncertainty.html#the-analysis"><i class="fa fa-check"></i><b>10.4.2</b> The Analysis</a></li>
<li class="chapter" data-level="10.4.3" data-path="uncertainty.html"><a href="uncertainty.html#the-results"><i class="fa fa-check"></i><b>10.4.3</b> The Results</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="decisions.html"><a href="decisions.html"><i class="fa fa-check"></i><b>11</b> Completing the forecast cycle</a>
<ul>
<li class="chapter" data-level="11.1" data-path="decisions.html"><a href="decisions.html#data-assimilation"><i class="fa fa-check"></i><b>11.1</b> Data assimilation</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="decisions.html"><a href="decisions.html#operational-data-assimilation"><i class="fa fa-check"></i><b>11.1.1</b> Operational data assimilation</a></li>
<li class="chapter" data-level="11.1.2" data-path="decisions.html"><a href="decisions.html#the-forecast-step"><i class="fa fa-check"></i><b>11.1.2</b> The Forecast Step</a></li>
<li class="chapter" data-level="11.1.3" data-path="decisions.html"><a href="decisions.html#the-analysis-step"><i class="fa fa-check"></i><b>11.1.3</b> The Analysis Step</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="decisions.html"><a href="decisions.html#decision-support"><i class="fa fa-check"></i><b>11.2</b> Decision support</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="decisions.html"><a href="decisions.html#in-an-ideal-world"><i class="fa fa-check"></i><b>11.2.1</b> In an ideal world…</a></li>
<li class="chapter" data-level="11.2.2" data-path="decisions.html"><a href="decisions.html#in-reality"><i class="fa fa-check"></i><b>11.2.2</b> In reality…</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="decisions.html"><a href="decisions.html#final-words"><i class="fa fa-check"></i><b>11.3</b> Final words</a></li>
<li class="chapter" data-level="11.4" data-path="decisions.html"><a href="decisions.html#revision"><i class="fa fa-check"></i><b>11.4</b> Revision questions</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="practical.html"><a href="practical.html"><i class="fa fa-check"></i><b>12</b> Practical: Pair coding with GitHub</a>
<ul>
<li class="chapter" data-level="12.1" data-path="practical.html"><a href="practical.html#objectives"><i class="fa fa-check"></i><b>12.1</b> Objectives</a></li>
<li class="chapter" data-level="12.2" data-path="practical.html"><a href="practical.html#postfire-regeneration"><i class="fa fa-check"></i><b>12.2</b> Postfire regeneration</a></li>
<li class="chapter" data-level="12.3" data-path="practical.html"><a href="practical.html#modular-design"><i class="fa fa-check"></i><b>12.3</b> Modular Design</a></li>
<li class="chapter" data-level="12.4" data-path="practical.html"><a href="practical.html#building-the-r-workflow-using-github"><i class="fa fa-check"></i><b>12.4</b> Building the R workflow using Github</a></li>
<li class="chapter" data-level="12.5" data-path="practical.html"><a href="practical.html#task-1-create-clone-repository"><i class="fa fa-check"></i><b>12.5</b> Task 1: Create &amp; Clone Repository</a>
<ul>
<li class="chapter" data-level="12.5.1" data-path="practical.html"><a href="practical.html#owner"><i class="fa fa-check"></i><b>12.5.1</b> OWNER</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="practical.html"><a href="practical.html#task-2-add-the-first-function-download.ndvi"><i class="fa fa-check"></i><b>12.6</b> Task 2: Add the first function: download.NDVI</a>
<ul>
<li class="chapter" data-level="12.6.1" data-path="practical.html"><a href="practical.html#owner-1"><i class="fa fa-check"></i><b>12.6.1</b> OWNER</a></li>
</ul></li>
<li class="chapter" data-level="12.7" data-path="practical.html"><a href="practical.html#task-3-collaborator-adds-plot.ndvi"><i class="fa fa-check"></i><b>12.7</b> Task 3: Collaborator adds plot.NDVI</a>
<ul>
<li class="chapter" data-level="12.7.1" data-path="practical.html"><a href="practical.html#collaborator"><i class="fa fa-check"></i><b>12.7.1</b> COLLABORATOR</a></li>
<li class="chapter" data-level="12.7.2" data-path="practical.html"><a href="practical.html#owner-2"><i class="fa fa-check"></i><b>12.7.2</b> OWNER</a></li>
</ul></li>
<li class="chapter" data-level="12.8" data-path="practical.html"><a href="practical.html#task-4-owner-adds-functions-for-model-fitting-using-mle"><i class="fa fa-check"></i><b>12.8</b> Task 4: Owner adds functions for model fitting using MLE</a>
<ul>
<li class="chapter" data-level="12.8.1" data-path="practical.html"><a href="practical.html#owner-3"><i class="fa fa-check"></i><b>12.8.1</b> OWNER</a></li>
</ul></li>
<li class="chapter" data-level="12.9" data-path="practical.html"><a href="practical.html#a-quick-explanation-of-the-mle-functions"><i class="fa fa-check"></i><b>12.9</b> A quick explanation of the MLE functions</a></li>
<li class="chapter" data-level="12.10" data-path="practical.html"><a href="practical.html#task-5-collaborator-adds-the-master-script-owner-runs-the-master-script"><i class="fa fa-check"></i><b>12.10</b> Task 5: Collaborator adds the master script, Owner runs the master script</a>
<ul>
<li class="chapter" data-level="12.10.1" data-path="practical.html"><a href="practical.html#collaborator-1"><i class="fa fa-check"></i><b>12.10.1</b> COLLABORATOR</a></li>
<li class="chapter" data-level="12.10.2" data-path="practical.html"><a href="practical.html#owner-4"><i class="fa fa-check"></i><b>12.10.2</b> OWNER</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="practical-output-master-script.html"><a href="practical-output-master-script.html"><i class="fa fa-check"></i><b>13</b> Practical output: Master script</a>
<ul>
<li class="chapter" data-level="13.1" data-path="practical-output-master-script.html"><a href="practical-output-master-script.html#source-functions-get-data-and-plot"><i class="fa fa-check"></i><b>13.1</b> Source functions, get data and plot</a></li>
<li class="chapter" data-level="13.2" data-path="practical-output-master-script.html"><a href="practical-output-master-script.html#fit-models-using-non-linear-least-squares-nls"><i class="fa fa-check"></i><b>13.2</b> Fit models using Non-linear Least Squares (NLS)</a></li>
<li class="chapter" data-level="13.3" data-path="practical-output-master-script.html"><a href="practical-output-master-script.html#compare-nls-models-using-anova"><i class="fa fa-check"></i><b>13.3</b> Compare NLS models using ANOVA</a></li>
<li class="chapter" data-level="13.4" data-path="practical-output-master-script.html"><a href="practical-output-master-script.html#fit-models-using-maximum-likelihood-estimation-mle"><i class="fa fa-check"></i><b>13.4</b> Fit models using Maximum Likelihood Estimation (MLE)</a></li>
<li class="chapter" data-level="13.5" data-path="practical-output-master-script.html"><a href="practical-output-master-script.html#compare-mle-models-using-akaikes-information-criterion-aic"><i class="fa fa-check"></i><b>13.5</b> Compare MLE models using Akaike’s information criterion (AIC)</a></li>
<li class="chapter" data-level="13.6" data-path="practical-output-master-script.html"><a href="practical-output-master-script.html#going-bayesian"><i class="fa fa-check"></i><b>13.6</b> Going Bayesian</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/jslingsby/BIO3019S_Ecoforecasting" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Ecological Forecasting</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bayesian" class="section level1 hasAnchor" number="7">
<h1><span class="header-section-number">7</span> Going Bayesian<a href="bayesian.html#bayesian" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>This section is also available as a slideshow. Right click or hold <code>Ctrl</code> or <code>Command</code> and <a href="presentations/7_bayesian.html" target="_blank">click this link</a> to view in full screen.</p>
<div>
<iframe width="600" height="400" marginheight="0" marginwidth="0" src="presentations/7_bayesian.html"></iframe>
</div>
<p><br></p>
<p>In this section I aim to provide a brief and (relatively) soft introduction to Bayesian statistical theory.</p>
<p>Iterative near-term ecological forecasting presents new challenges you don’t commonly encounter in the world of null hypothesis testing that has dominated ecology to date…</p>
<div class="float">
<img src="img/ecoforecastingloop.png" style="width:70.0%" alt="The iterative ecological forecasting cycle in the context of the scientific method, demonstrating how we stand to learn from making iterative forecasts." />
<div class="figcaption">The iterative ecological forecasting cycle in the context of the scientific method, demonstrating how we stand to learn from making iterative forecasts.</div>
</div>
<p><br></p>
<p>While ecological forecasting and decision support can be done with traditional statistics (often termed “frequentist statistics”) it is generally much easier to do in a Bayesian statistical framework.</p>
<p><strong>Bayesian approaches have several advantages for forecasting:</strong></p>
<blockquote>
<p><em>NOTE: there are frequentist approaches for doing much of this, but they are typically cumbersome “add-ons” that require many additional assumptions. Once you’re using Bayes you can achieve all this without much extra work.</em></p>
</blockquote>
<ol style="list-style-type: decimal">
<li>They are typically <strong>focused on estimating what properties are</strong> (i.e. the actual value of a particular parameter) and not just establishing what they are not (i.e. testing for significant difference (<em>null hypothesis testing</em>), as is usually the focus in frequentist statistics)</li>
<li>They are highly <strong>flexible</strong>, allowing one to build relatively <strong>complex models with varied data sources</strong> (and/or of varying quality), especially Hierarchical Bayesian models</li>
<li>They can easily treat all terms as probability distributions, making it <strong>easier to quantify, propagate, partition and represent uncertainties throughout the analysis probabilistically</strong>. More on this later, but it addresses two of the key components of the ecological forecasting cycle:
<ul>
<li>being able to <strong>present the uncertainty</strong> in the forecast to the decision maker</li>
<li>being able to <strong>analyze the uncertainty</strong> in the forecast, in the hope that this can guide reducing the uncertainty and improving the next forecast, e.g. through targeted data collection or altering the model.</li>
</ul></li>
<li>They provide an <strong>iterative</strong> probabilistic framework that mirrors the scientific method, allowing us to formalize learning from new evidence (i.e. new data) in the context of existing knowledge (<em>prior</em> information). This framework makes it easier to update predictions as new data become available, completing the forecasting cycle.</li>
</ol>
<p><br></p>
<p><strong>Before I can introduce Bayes</strong>, there are a few basic building blocks we need to establish first.</p>
<ol style="list-style-type: decimal">
<li>How the method of <strong>Least Squares</strong> works, and its limitations, especially it’s inflexible, implicit data model.</li>
<li>The concept of <strong>likelihood</strong> and the estimation of maximum likelihood, since this is a major component of Bayes’ Theorem. In fact, while likelihood is technically still a frequentist method, it has the advantages 1 &amp; most of 2 above, without going full Bayes.</li>
</ol>
<p><br></p>
<blockquote>
<p><em>NOTE: This really is a minimalist introduction that only provides the tidbits I need you to know to follow the rest of the module. This note is a proviso to make it clear that I am withholding important details, before anyone accuses me of lies of ommission.</em></p>
</blockquote>
<p><br></p>
<div id="least-squares" class="section level2 hasAnchor" number="7.1">
<h2><span class="header-section-number">7.1</span> Least Squares<a href="bayesian.html#least-squares" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Traditional parametric statistics like regression analysis and analysis of variance (ANOVA) rely on Ordinary Least Squares (OLS). There are a few other flavours of least squares that allow a bit more flexibility (e.g. nonlinear (NLS) that we used in the practical in section <a href="practical.html#practical">12</a>, partial least squares, etc), but I’m not going to go into these distinctions.</p>
<p>In general, least squares approaches “fit” (i.e. estimate the parameters of) models by <strong>minimizing the sums of the squared residuals</strong>. Let’s explore this by looking at an example of a linear regression.</p>
<div class="figure"><span style="display:block;" id="fig:leastsquares1"></span>
<img src="BIO3019S-Ecoforecasting_files/figure-html/leastsquares1-1.png" alt="A hypothetical example showing a linear model fit." width="672" />
<p class="caption">
Figure 7.1: A hypothetical example showing a linear model fit.
</p>
</div>
<p><br></p>
<p>Here the model (blue line) is drawn through the cloud of points so as to minimize the sum of the squared vertical (y-axis) differences (i.e. residuals) between each point and the regression line.</p>
<p>Let’s redraw this highlighting the residuals:</p>
<p><br></p>
<div class="figure"><span style="display:block;" id="fig:leastsquares2"></span>
<img src="BIO3019S-Ecoforecasting_files/figure-html/leastsquares2-1.png" alt="The linear model highlighting the residuals (lollypops) relative to the values predicted by the model (open circles along the regression line)." width="672" />
<p class="caption">
Figure 7.2: The linear model highlighting the residuals (lollypops) relative to the values predicted by the model (open circles along the regression line).
</p>
</div>
<p><br></p>
<p>So the grey lines linking each observed data point to the regression line are the residuals. Note that they are <strong>vertical and not orthogonal</strong> to the regression line, because they represent the variance in Y (<em>Reward</em> in this case) that is not explained by X (<em>Effort</em> in this case). The open black circles are the Y-values that our linear model predicts for a given X-value. There is no scatter in the predicted values, because the scatter is <strong>residual variance that the model cannot account for and predict</strong>.</p>
<p>Now let’s have a look at a histogram of the residuals:</p>
<p><br></p>
<div class="figure"><span style="display:block;" id="fig:leastsquares3"></span>
<img src="BIO3019S-Ecoforecasting_files/figure-html/leastsquares3-1.png" alt="A histogram of the residuals from the linear model above." width="672" />
<p class="caption">
Figure 7.3: A histogram of the residuals from the linear model above.
</p>
</div>
<p><br></p>
<p>In this case, the residuals approximate a <strong><em>normal distribution</em></strong> (or should, since I generated them from a normal distribution…). You’ll recall that this is <strong>one of the assumptions when using linear models or ANOVA (often termed “homoscedasticity” or “homogeneity of variance”)</strong> - i.e. it is an assumption of the Least Squares method. Least squares cannot handle residuals that are not normally distributed. If the residuals were not normally distributed, then either we are fitting the wrong model (e.g. we should consider a non-linear rather than a linear model), or our assumptions are violated and we should not be using this technique!</p>
<p>The reason Least Squares requires this assumption is that for minimizing the sums of squares to work, <strong>a unit change in the residuals should be scale invariant</strong>. In other words, the difference between 1 and 2 needs to be the same as the difference between 101 and 102. This is only the case when the residuals are normally distributed (versus log-scale for example). If the scale is variable, then <strong>minimizing the sums of squares does not work</strong>. People often try to get around the assumption of homogeneity of variance by <strong>tranforming</strong> their data (e.g. log or arcsine transform) to try to get them to an invariant scale.</p>
<p><br></p>
<p>So to look at the shortcomings of Least Squares:</p>
<p><br></p>
<div id="least-squares-doesnt-explicitly-include-a-data-model" class="section level3 hasAnchor" number="7.1.1">
<h3><span class="header-section-number">7.1.1</span> Least Squares doesn’t explicitly include a data model<a href="bayesian.html#least-squares-doesnt-explicitly-include-a-data-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>It’s useful at this stage to make a distinction between data models and process models.</p>
<ul>
<li>The <strong>process model</strong> is the bit you’ll be used to, where we describe how the model creates a prediction for a particular set of inputs or covariates (i.e. the linear model in the case above).</li>
<li>The <strong>data model</strong> describes the residuals (i.e. the mismatch between the process model and the data). It is also often called the data observation process.</li>
</ul>
<p><br></p>
<p>Least Squares analyses don’t explicitly include a data model, because the reliance on minimizing the sums of squares means that <strong>the data model in a Least Squares analysis can only ever be a normal distribution</strong> (i.e. homogeneity of variance).</p>
<p><br></p>
<p>This is a major limitation of the Least Squares method, because:</p>
<ol style="list-style-type: lower-alpha">
<li>in reality, the <strong>data model can take many forms</strong>
<ul>
<li>e.g. Binomial coin flips, Poisson counts of individuals, Exponential waiting times, etc</li>
<li>this is where Maximum Likelihood comes into it’s own
<ul>
<li>recall that in the practical we specified two separate functions for the MLE analyses. One specified the process model (<code>pred.negexp/S</code>), the other specified the likelihood function (<code>fit.negexp/S.MLE</code>), which includes the data model.</li>
</ul></li>
</ul></li>
<li>there are times when <strong>one would like to include additional information in the data model</strong>
<ul>
<li>e.g. sampling variability (e.g. different observers or instruments), measurement errors, instrument calibration, proxy data, unequal variances, missing data, etc</li>
<li>this is where Bayesian models come into their own</li>
</ul></li>
</ol>
<p><br></p>
</div>
<div id="least-squares-focuses-on-what-the-parameters-are-not-rather-than-what-they-are" class="section level3 hasAnchor" number="7.1.2">
<h3><span class="header-section-number">7.1.2</span> Least squares focuses on what the parameters are not, rather than what they are<a href="bayesian.html#least-squares-focuses-on-what-the-parameters-are-not-rather-than-what-they-are" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Least Squares focuses on <strong>significance testing</strong> - the ability to reject (or to fail to reject) the null hypothesis. In the case of a linear model, <strong>the null hypothesis is that the slope is zero (i.e. there is no effect of X on Y)</strong> and sometimes includes that the intercept should be zero too (although this is not usually required). I don’t have the time to go through the full explanation of how the null hypothesis is tested in this lecture, but it is useful to highlight that the linear model is only considered useful when you can reject the null hypothesis that the slope is zero (usually at an alpha of P &lt; 0.05). While this is a start, <strong>it doesn’t tell you anything about the probability of the parameters actually being the estimates you arrives at by minimizing the sums of squares</strong>?!</p>
<p><br></p>
</div>
</div>
<div id="maximum-likelihood" class="section level2 hasAnchor" number="7.2">
<h2><span class="header-section-number">7.2</span> Maximum likelihood<a href="bayesian.html#maximum-likelihood" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong><em>Maximum likelihood is a method used for estimating model parameters</em></strong></p>
<p><em>The likelihood principle states a that parameter value is more likely than another if it is the one for which the data are more probable.</em></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:likelihood"></span>
<img src="BIO3019S-Ecoforecasting_files/figure-html/likelihood-1.png" alt="The probability of a observed point (the dotted line) being generated under two alternative hypotheses (sets of parameter values). In this case H1 is more likely, because the probability density at $x$ = 1 is higher for H1 than H2 (roughly 0.25 vs 0.025). This makes H1 0.25/0.025 = 10 times more likely." width="75%" />
<p class="caption">
Figure 7.4: The probability of a observed point (the dotted line) being generated under two alternative hypotheses (sets of parameter values). In this case H1 is more likely, because the probability density at <span class="math inline">\(x\)</span> = 1 is higher for H1 than H2 (roughly 0.25 vs 0.025). This makes H1 0.25/0.025 = 10 times more likely.
</p>
</div>
<p><br></p>
<p><em>Maximum Likelihood Estimation (MLE) applies this principle by optimizing the parameter values such that they maximize the likelihood that the process described by the given model produced the data that were observed.</em></p>
<ul>
<li>To relate this to Figure <a href="bayesian.html#fig:likelihood">7.4</a>, the parameter values would be a continuum of hypotheses to select from, and of course there would be far more data points that you’d need to calculate the probabilities for.</li>
</ul>
<p><br></p>
<p>Viewed differently, when using MLE we assume that we have the correct model and apply MLE to choose the parameters so as to maximize the <em>conditional probability</em> of the data given those parameter estimates. The notation for this conditional probability is <span class="math inline">\(P(Data|Parameters)\)</span>.</p>
<p>This process leaves us knowing the likelihood of the best set of parameters and the conditional probability of the data given those parameters <span class="math inline">\(P(Data|Parameters)\)</span>.</p>
<p>The problem is that in the context of forecasting (and many other modelling approaches) <strong>what we really want is to know is the conditional probability of the parameters given the data</strong> <span class="math inline">\(P(Parameters|Data)\)</span>, because this allow us to <strong><em>express the uncertainty in the parameter estimates as probabilities</em></strong><a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>. To get there, we need to apply a little probability theory, which provides a somewhat surprising and very useful byproduct.</p>
<p><br></p>
<p>First, let’s brush up on our probability theory…</p>
<p><br></p>
<div id="joint-conditional-and-marginal-probabilities" class="section level3 hasAnchor" number="7.2.1">
<h3><span class="header-section-number">7.2.1</span> Joint, conditional and marginal probabilities<a href="bayesian.html#joint-conditional-and-marginal-probabilities" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Here we’ll look at the interactions between two random variables and illustrate it with the Venn diagram below.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:vennprobability"></span>
<img src="img/vennprobability.png" alt="Venn diagram illustrating ten events (points) across two sets $x$ and $y$." width="75%" />
<p class="caption">
Figure 7.5: Venn diagram illustrating ten events (points) across two sets <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>.
</p>
</div>
<p><br></p>
<p>First, we can define the <strong><em>joint probability</em></strong>, <span class="math inline">\(P(x,y)\)</span>, which is the probability of both <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> occurring simultaneously, which in the case of Figure <a href="bayesian.html#fig:vennprobability">7.5</a> is the probability of occurring within the overlap of the circles = 3/10. Needless to say <strong>the joint probability of</strong> <span class="math inline">\(P(y,x)\)</span> is identical to <span class="math inline">\(P(x,y)\)</span> (= 3/10).</p>
<p>Second, based on the joint probability we can define two <strong><em>conditional probabilities</em></strong>:</p>
<ul>
<li>the probability of <span class="math inline">\(x\)</span> given <span class="math inline">\(y\)</span>, <span class="math inline">\(P(x|y)\)</span>, and</li>
<li>the probability of <span class="math inline">\(y\)</span> given <span class="math inline">\(x\)</span>, <span class="math inline">\(P(y|x)\)</span>.</li>
</ul>
<p>In the context of the Venn diagram these would be:</p>
<ul>
<li>the probability of being in set <span class="math inline">\(x\)</span> given that we are only considering the points in set <span class="math inline">\(y\)</span> (= 3/6), and</li>
<li>the probability of being in set <span class="math inline">\(y\)</span> given that we are only considering the points in set <span class="math inline">\(x\)</span> (= 3/7).</li>
</ul>
<p>Last, we can define two <strong><em>marginal probabilities</em></strong> for <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, which are just the separate probabilities of being in set <span class="math inline">\(x\)</span> or being in set <span class="math inline">\(y\)</span> given the full set of events, i.e.:</p>
<ul>
<li><span class="math inline">\(P(x)\)</span> = 7/10 and</li>
<li><span class="math inline">\(P(y)\)</span> = 6/10.</li>
</ul>
<p>We can also show that <strong>the joint probabilities are the product of the conditional and marginal probabilities</strong>:</p>
<span class="math display">\[\begin{align*}
P(x,y) = P(x|y) P(y) = 3/6 * 6/10 = 0.3
\end{align*}\]</span>
<p>And by the same token:</p>
<span class="math display">\[\begin{align*}
P(y,x) = P(y|x) P(y) = 3/7 * 7/10 = 0.3
\end{align*}\]</span>
<p><br></p>
<p>Why I’m telling you all this is because knowing this last equation (that the joint probabilities are the product of the conditional and marginal probabilities) means that we can derive the information we want to know, <span class="math inline">\(P(Parameters|Data)\)</span>, as a function of the information maximum likelihood estimation has given us, <span class="math inline">\(P(Data|Parameters)\)</span>…</p>
<p><br></p>
</div>
</div>
<div id="bayes-theorem" class="section level2 hasAnchor" number="7.3">
<h2><span class="header-section-number">7.3</span> Bayes’ Theorem<a href="bayesian.html#bayes-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>From here I’ll refer to the parameters as <span class="math inline">\(\theta\)</span> and the data as (<span class="math inline">\(D\)</span>), because writing them out in full is a bit clunky and grownups don’t usually do that.</p>
<p>Since we are interested in the conditional probability of the parameters given the data, <span class="math inline">\(p(\theta|D)\)</span>, we need to take the equations above and solve for <span class="math inline">\(p(\theta|D)\)</span>.</p>
<p>From above, we know</p>
<span class="math display">\[\begin{align*}
p(\theta,D) = p(\theta|D)p(D)
\end{align*}\]</span>
<p>and we know that the joint probabilities are identical, so we can also write</p>
<span class="math display">\[\begin{align*}
p(\theta,D) = p(D|\theta)p(\theta)
\end{align*}\]</span>
<p>so we can rewrite this to compare the right hand side of the last two equations</p>
<span class="math display">\[\begin{align*}
p(\theta|D)p(D) = p(D|\theta)p(\theta)
\end{align*}\]</span>
<p>which if we solve for <span class="math inline">\(p(\theta|D)\)</span> is</p>
<span class="math display">\[\begin{align*}
p(\theta|D) &amp; = \frac{p(D|\theta) \; p(\theta)}{p(D)} \;\;
\end{align*}\]</span>
<p>which is known as Bayes’ Theorem.</p>
<p><br></p>
<div id="the-beauty-of-bayes-theorem" class="section level3 hasAnchor" number="7.3.1">
<h3><span class="header-section-number">7.3.1</span> The beauty of Bayes’ Theorem<a href="bayesian.html#the-beauty-of-bayes-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now let’s unwrap our birthday present and see what we got!</p>
<p>Rewriting the terms on one line allows us to label them with the names by which they are commonly known:</p>
<p><span class="math display">\[
\underbrace{p(\theta|D)}_\text{posterior} \; = \; \underbrace{p(D|\theta)}_\text{likelihood} \;\; \underbrace{p(\theta)}_\text{prior} \; / \; \underbrace{p(D)}_\text{evidence}
\]</span> First, let’s start with the last term, the <strong><em>evidence</em></strong> as this will allow us to simplify things.</p>
<p>The marginal probability of the data, <span class="math inline">\(p(D)\)</span>, is now called the <strong><em>evidence</em></strong> for the model, and represents the overall probability of the data according to the model, determined by averaging across all possible parameter values weighted by the strength of belief in those parameter values.</p>
<p>This sounds quite complicated, but never fear, all you need to know is that the implication of having <span class="math inline">\(p(D)\)</span> in the denominator is that it normalizes the numerators to ensure that the updated probabilities sum to 1 over all possible parameter values. In other words, the evidence term ensures that the posterior, <span class="math inline">\(p(\theta|D)\)</span>, is expressed as a probability distribution, and can otherwise mostly be ignored.</p>
<p>This allows us to focus on the important bits, and to express Bayes’ Rule as</p>
<p><span class="math display">\[
\underbrace{p(\theta|D)}_\text{posterior} \; \propto \; \underbrace{p(D|\theta)}_\text{likelihood} \;\; \underbrace{p(\theta)}_\text{prior} \;
\]</span></p>
<p>Which reads “The posterior is proportional to the likelihood times the prior”.</p>
<p>This leaves us with three terms.</p>
<p>First, we have the <strong><em>likelihood</em></strong>, <span class="math inline">\(p(D|\theta)\)</span>. This is unchanged and still represents the probability that the data could be generated by the model with parameter values <span class="math inline">\(\theta\)</span>, and when used in analyses still works to find the likelihood profiles of the parameters.</p>
<p>Next, what we have been calling the conditional probability of the parameters given the data, <span class="math inline">\(p(\theta|D)\)</span>, is now called the <strong><em>posterior</em></strong>, and represents the credibility of the parameter values <span class="math inline">\(\theta\)</span>, taking the data, <span class="math inline">\(D\)</span>, into account. In other words, it gives us a probability distribution for the values any parameter can take, essentially <strong>allowing us to represent uncertainty in the model and forecasts as probabilities, which is the holy grail we were after!!!</strong></p>
<p>Last, we’ll look at the marginal probability of the parameters, <span class="math inline">\(p(\theta)\)</span>, which is now called the <strong><em>prior</em></strong>. While <span class="math inline">\(p(\theta)\)</span> is almost an unexpected consequence of having solved the equation for <span class="math inline">\(p(\theta|D)\)</span>, this is where the magic lies! Firstly, we know that <span class="math inline">\(p(\theta)\)</span> is a necessary requirement for us to be able to represent the posterior, <span class="math inline">\(p(\theta|D)\)</span>, as a probability distribution. That it helps us do this is magic in itself, but what is <span class="math inline">\(p(\theta)\)</span> itself?</p>
<p><em>The prior represents the credibility of the parameter values,</em> <span class="math inline">\(\theta\)</span>, without the data, <span class="math inline">\(D\)</span>.</p>
<p>How can we know anything about the parameter values without the data you may ask? In short, the answer is because we are applying <strong>the scientific method</strong>, whereby we interrogate new evidence (the data) in the context of previous knowledge or information to update our understanding. The term <span class="math inline">\(p(\theta)\)</span> is called the prior, because it represents our <strong>prior belief</strong> of what the parameters should be, before interrogating the data. In other words, the prior is our “context of previous knowledge or information”. The nice thing is that if we don’t have much previous knowledge or information, we can specify the prior to represent that.</p>
<p>The prior is incredibly powerful, but, as always the <a href="https://en.wikipedia.org/wiki/With_great_power_comes_great_responsibility">Peter Parker Principle</a> applies - “With great power comes great responsibility!”</p>
<p><br></p>
<p><strong>The prior is incredibly powerful, because:</strong></p>
<ol style="list-style-type: decimal">
<li>It allows Bayesian analyses to be <strong>iterative</strong>. The posterior from one analysis can become the prior for the next!
<ul>
<li>This provides a formal probabilistic framework for the scientific method! New evidence must be considered in the context of previous knowledge or information, and provides us the opportunity to update our beliefs.</li>
<li>This is also ideal for iterative forecasting, because the previous forecast can be the prior for the next, (and the observed outcome of the previous forecast window can be the new data that’s fed into the likelihood function for the next forecast).</li>
</ul></li>
<li>It makes Bayesian modelling <strong>incredibly flexible</strong> by allowing us to specify complex models like hierarchical models in a completely probabilistic framework relatively intuitively.
<ul>
<li>For example, in some cases we may not have any strong direct “prior beliefs” about the parameter values, but we may know something about another parameter or process that influences the parameter value of interest. In this case we can specify a prior on our prior (usually called a <strong>hyperprior</strong>).</li>
<li>Another key component here is that we can specify separate or linked priors or hyperpriors for different parameters or priors.</li>
</ul></li>
</ol>
<p><br></p>
<p><strong>The prior also comes with great responsibility, because:</strong></p>
<blockquote>
<p>In short, it is very easy to specify an inappropriate prior and bias the outcome of your analysis!</p>
</blockquote>
<ul>
<li>First and foremost, as tempting as it may be, <strong><em>the one place the prior CANNOT come from is the data used in the analysis!!!</em></strong></li>
<li>Second, while many people favour specifying “uninformative” priors, it is often <strong>incredibly difficult to know what “uninformative”</strong> is for different models or parameters.
<ul>
<li>For example, say you were trying to build a model to solve a murder mystery and predict who the likely culprit is. One of the parameters may be the gender of the culprit. If you’re lazy, you may say “we set an uninformative prior that there was a 50/50 chance of the culprit being male or female”. In truth, this “uninformative” prior may well bias your result for a number of reasons:
<ul>
<li>Firstly, sex ratios are rarely 50/50 (e.g. in South Africa a quick Google search suggests the M/F ratio is supposedly 1:1.03). In this case your prior would be unfairly biasing the model towards predicting that that the culprit is a woman.</li>
<li>Acknowledging this, you may then set your sex ratio as the observed ratio for the population in the region where the murder took place, but there are further complexities that may bias your results. e.g. Observed data suggest that far more men commit murder than women, but going with the observed sex ratio for the population makes the assumption that men and women are equally likely to commit murder.</li>
<li>Next, your prior has not taken into account the profile of the victim - man, woman, child, etc - but the sex ratios of murderers varies hugely whether you’re considering androcide, femicide or infanticide.</li>
<li>Lastly, what about gender identity and sexual orientation (LGBTQI+)? Have your sources of prior information taken this into account and would this affect the outcome?</li>
</ul></li>
</ul></li>
</ul>
<p><a href="https://www.theguardian.com/world/2021/apr/18/obscure-maths-bayes-theorem-reliability-covid-lateral-flow-tests-probability">Here’s a great Guardian article</a> on why considering priors are important for Covid testing and criminal cases. It should help you get your head around this.</p>
<p>We’ll get stuck into examples showing the value of Bayes in ecological studies in the next lecture.</p>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="3">
<li id="fn3"><p>Note that we have the <em>likelihood</em> of the parameter estimates, but <span class="math inline">\(likelihood \neq probability\)</span>. I don’t want to spend time on the distinction in the lecture, but it is a very important concept. The area under a probability distribution sums to 1, but the area under a likelihood profile curve does not. The reason for this is that probability relate to the set of possible results, which are exhaustive and mutually exclusive, whereas likelihood relates to the possible hypotheses, which are neither exhaustive nor mutually exclusive (i.e. there’s no limit to the number of potential hypotheses, and hypotheses can be nested or overlapping).<a href="bayesian.html#fnref3" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="data.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="datafusion.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/jslingsby/BIO3019S_Ecoforecasting/edit/master/07-going_bayesian.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["BIO3019S Ecoforecasting.pdf", "BIO3019S Ecoforecasting.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
